{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Copyright 2025 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
      ],
      "metadata": {
        "id": "LROI8y1LL8kL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WS5t185lhfMa"
      },
      "outputs": [],
      "source": [
        "# @title Install dependency\n",
        "\n",
        "!pip install mediapy thop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8FfWGdoIq4B"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "\n",
        "import functools\n",
        "import math\n",
        "\n",
        "import einops\n",
        "import numpy as np\n",
        "import torch\n",
        "import jax\n",
        "import jax.numpy as jnp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VideoMAE from Github"
      ],
      "metadata": {
        "id": "g8mBkfCKM1qO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download VideoMAE code\n",
        "\n",
        "%cd /content\n",
        "!git clone https://github.com/MCG-NJU/VideoMAE.git"
      ],
      "metadata": {
        "id": "YOQwHpOpM4Hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define VideoMAE model\n",
        "\n",
        "%cd /content/VideoMAE\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint as checkpoint\n",
        "from functools import partial\n",
        "from timm.models.layers import drop_path, to_2tuple, trunc_normal_\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "  def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "    super().__init__()\n",
        "    out_features = out_features or in_features\n",
        "    hidden_features = hidden_features or in_features\n",
        "    self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "    self.act = act_layer()\n",
        "    self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "    self.drop = nn.Dropout(drop)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc1(x)\n",
        "    x = self.act(x)\n",
        "    # x = self.drop(x)\n",
        "    # commit this for the orignal BERT implement\n",
        "    x = self.fc2(x)\n",
        "    x = self.drop(x)\n",
        "    return x\n",
        "\n",
        "class Attention(nn.Module):\n",
        "  def __init__(\n",
        "      self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.,\n",
        "      proj_drop=0., attn_head_dim=None):\n",
        "    super().__init__()\n",
        "    self.num_heads = num_heads\n",
        "    head_dim = dim // num_heads\n",
        "    if attn_head_dim is not None:\n",
        "      head_dim = attn_head_dim\n",
        "    all_head_dim = head_dim * self.num_heads\n",
        "    self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "    self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n",
        "    if qkv_bias:\n",
        "      self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
        "      self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
        "    else:\n",
        "      self.q_bias = None\n",
        "      self.v_bias = None\n",
        "\n",
        "    self.attn_drop = nn.Dropout(attn_drop)\n",
        "    self.proj = nn.Linear(all_head_dim, dim)\n",
        "    self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, N, C = x.shape\n",
        "    qkv_bias = None\n",
        "    if self.q_bias is not None:\n",
        "      qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n",
        "    # qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "    qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n",
        "    qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
        "    q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
        "\n",
        "    q = q * self.scale\n",
        "    attn = (q @ k.transpose(-2, -1))\n",
        "\n",
        "\n",
        "    attn = attn.softmax(dim=-1)\n",
        "    attn = self.attn_drop(attn)\n",
        "\n",
        "    x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n",
        "    x = self.proj(x)\n",
        "    x = self.proj_drop(x)\n",
        "    return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "  def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "               drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n",
        "               attn_head_dim=None):\n",
        "    super().__init__()\n",
        "    self.norm1 = norm_layer(dim)\n",
        "    self.attn = Attention(\n",
        "      dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "      attn_drop=attn_drop, proj_drop=drop, attn_head_dim=attn_head_dim)\n",
        "    # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
        "    self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "    self.norm2 = norm_layer(dim)\n",
        "    mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "    self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    if init_values > 0:\n",
        "      self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
        "      self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
        "    else:\n",
        "      self.gamma_1, self.gamma_2 = None, None\n",
        "\n",
        "  def forward(self, x):\n",
        "    if self.gamma_1 is None:\n",
        "      x = x + self.drop_path(self.attn(self.norm1(x)))\n",
        "      x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "    else:\n",
        "      x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x)))\n",
        "      x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n",
        "    return x\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "  \"\"\" Image to Patch Embedding\n",
        "  \"\"\"\n",
        "  def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, num_frames=16, tubelet_size=2):\n",
        "    super().__init__()\n",
        "    img_size = to_2tuple(img_size)\n",
        "    patch_size = to_2tuple(patch_size)\n",
        "    self.tubelet_size = int(tubelet_size)\n",
        "    num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0]) * (num_frames // self.tubelet_size)\n",
        "    self.img_size = img_size\n",
        "    self.patch_size = patch_size\n",
        "    self.num_patches = num_patches\n",
        "    self.proj = nn.Conv3d(in_channels=in_chans, out_channels=embed_dim,\n",
        "                          kernel_size = (self.tubelet_size,  patch_size[0],patch_size[1]),\n",
        "                          stride=(self.tubelet_size,  patch_size[0],  patch_size[1]))\n",
        "\n",
        "  def forward(self, x, **kwargs):\n",
        "    B, C, T, H, W = x.shape\n",
        "    # FIXME look at relaxing size constraints\n",
        "    # assert H == self.img_size[0] and W == self.img_size[1], \\\n",
        "    #     f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
        "    x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "    return x\n",
        "\n",
        "# sin-cos position encoding\n",
        "# https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/Models.py#L31\n",
        "def get_sinusoid_encoding_table(n_position, d_hid):\n",
        "  ''' Sinusoid position encoding table '''\n",
        "  # TODO: make it with torch instead of numpy\n",
        "  def get_position_angle_vec(position):\n",
        "    return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]\n",
        "\n",
        "  sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n",
        "  sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2]) # dim 2i\n",
        "  sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2]) # dim 2i+1\n",
        "\n",
        "  return  torch.tensor(sinusoid_table,dtype=torch.float, requires_grad=False).unsqueeze(0)\n",
        "\n",
        "class PretrainVisionTransformerEncoder(nn.Module):\n",
        "  \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
        "  \"\"\"\n",
        "  def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=0, embed_dim=768, depth=12,\n",
        "               num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
        "               drop_path_rate=0., norm_layer=nn.LayerNorm, init_values=None, tubelet_size=2, use_checkpoint=False,\n",
        "               use_learnable_pos_emb=False):\n",
        "    super().__init__()\n",
        "    self.num_classes = num_classes\n",
        "    self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
        "    self.patch_embed = PatchEmbed(\n",
        "      img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim, tubelet_size=tubelet_size)\n",
        "    num_patches = self.patch_embed.num_patches\n",
        "    self.embedding_shape = (8, 14, 14)  # Manually added for resizing position embedding\n",
        "    self.use_checkpoint = use_checkpoint\n",
        "\n",
        "    # TODO: Add the cls token\n",
        "    if use_learnable_pos_emb:\n",
        "      self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "    else:\n",
        "      # sine-cosine positional embeddings\n",
        "      self.pos_embed = get_sinusoid_encoding_table(num_patches, embed_dim)\n",
        "\n",
        "    dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "    self.blocks = nn.ModuleList([\n",
        "      Block(\n",
        "        dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "        drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n",
        "        init_values=init_values)\n",
        "      for i in range(depth)])\n",
        "    self.norm = norm_layer(embed_dim)\n",
        "    self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "  def interpolate_pos_encoding(self, x, h, w):\n",
        "    x = x.reshape(self.embedding_shape + (-1,))\n",
        "    dim = x.shape[-1]\n",
        "    x = F.interpolate(\n",
        "      x.permute(0, 3, 1, 2),\n",
        "      scale_factor=(h / self.embedding_shape[-2], w / self.embedding_shape[-1]),\n",
        "      mode=\"bicubic\",\n",
        "    )\n",
        "    x = x.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "    return x\n",
        "\n",
        "  def forward_features(self, x):\n",
        "    _, _, T, h, w = x.shape\n",
        "    h = h // self.patch_embed.patch_size[0]\n",
        "    w = w // self.patch_embed.patch_size[1]\n",
        "    x = self.patch_embed(x)\n",
        "\n",
        "    # x = x + self.pos_embed.type_as(x).to(x.device).clone().detach()\n",
        "    pos_embed = self.pos_embed.to(x.device)\n",
        "    x = x + self.interpolate_pos_encoding(pos_embed, h, w)\n",
        "\n",
        "    B, _, C = x.shape\n",
        "    x_vis = x.reshape(B, -1, C) # ~mask means visible\n",
        "\n",
        "    if self.use_checkpoint:\n",
        "      for blk in self.blocks:\n",
        "        x_vis = checkpoint.checkpoint(blk, x_vis)\n",
        "    else:\n",
        "      for blk in self.blocks:\n",
        "        x_vis = blk(x_vis)\n",
        "\n",
        "    x_vis = self.norm(x_vis)\n",
        "    return x_vis\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.forward_features(x)\n",
        "    # x = self.head(x)\n",
        "    return x\n",
        "\n",
        "class PretrainVisionTransformerDecoder(nn.Module):\n",
        "  \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
        "  \"\"\"\n",
        "  def __init__(self, patch_size=16, num_classes=768, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.,\n",
        "               qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.,\n",
        "               norm_layer=nn.LayerNorm, init_values=None, num_patches=196, tubelet_size=2, use_checkpoint=False\n",
        "               ):\n",
        "    super().__init__()\n",
        "    self.num_classes = num_classes\n",
        "    assert num_classes == 3 * tubelet_size * patch_size ** 2\n",
        "    self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
        "    self.patch_size = patch_size\n",
        "    self.use_checkpoint = use_checkpoint\n",
        "\n",
        "    dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "    self.blocks = nn.ModuleList([\n",
        "      Block(\n",
        "        dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "        drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n",
        "        init_values=init_values)\n",
        "      for i in range(depth)])\n",
        "    self.norm = norm_layer(embed_dim)\n",
        "    self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  def _init_weights(self, m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "      nn.init.xavier_uniform_(m.weight)\n",
        "      if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "    elif isinstance(m, nn.LayerNorm):\n",
        "      nn.init.constant_(m.bias, 0)\n",
        "      nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "  def get_num_layers(self):\n",
        "    return len(self.blocks)\n",
        "\n",
        "  @torch.jit.ignore\n",
        "  def no_weight_decay(self):\n",
        "    return {'pos_embed', 'cls_token'}\n",
        "\n",
        "  def get_classifier(self):\n",
        "    return self.head\n",
        "\n",
        "  def reset_classifier(self, num_classes, global_pool=''):\n",
        "    self.num_classes = num_classes\n",
        "    self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "  def forward(self, x, return_token_num):\n",
        "    if self.use_checkpoint:\n",
        "      for blk in self.blocks:\n",
        "        x = checkpoint.checkpoint(blk, x)\n",
        "    else:\n",
        "      for blk in self.blocks:\n",
        "        x = blk(x)\n",
        "\n",
        "    if return_token_num > 0:\n",
        "      x = self.head(self.norm(x[:, -return_token_num:])) # only return the mask tokens predict pixels\n",
        "    else:\n",
        "      x = self.head(self.norm(x))\n",
        "\n",
        "    return x\n",
        "\n",
        "class PretrainVisionTransformer(nn.Module):\n",
        "  \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               img_size=224,\n",
        "               patch_size=16,\n",
        "               encoder_in_chans=3,\n",
        "               encoder_num_classes=0,\n",
        "               encoder_embed_dim=768,\n",
        "               encoder_depth=12,\n",
        "               encoder_num_heads=12,\n",
        "               decoder_num_classes=1536, #  decoder_num_classes=768,\n",
        "               decoder_embed_dim=512,\n",
        "               decoder_depth=8,\n",
        "               decoder_num_heads=8,\n",
        "               mlp_ratio=4.,\n",
        "               qkv_bias=False,\n",
        "               qk_scale=None,\n",
        "               drop_rate=0.,\n",
        "               attn_drop_rate=0.,\n",
        "               drop_path_rate=0.,\n",
        "               norm_layer=nn.LayerNorm,\n",
        "               init_values=0.,\n",
        "               use_learnable_pos_emb=False,\n",
        "               use_checkpoint=False,\n",
        "               tubelet_size=2,\n",
        "               num_classes=0, # avoid the error from create_fn in timm\n",
        "               in_chans=0, # avoid the error from create_fn in timm\n",
        "               ):\n",
        "    super().__init__()\n",
        "    self.encoder = PretrainVisionTransformerEncoder(\n",
        "      img_size=img_size,\n",
        "      patch_size=patch_size,\n",
        "      in_chans=encoder_in_chans,\n",
        "      num_classes=encoder_num_classes,\n",
        "      embed_dim=encoder_embed_dim,\n",
        "      depth=encoder_depth,\n",
        "      num_heads=encoder_num_heads,\n",
        "      mlp_ratio=mlp_ratio,\n",
        "      qkv_bias=qkv_bias,\n",
        "      qk_scale=qk_scale,\n",
        "      drop_rate=drop_rate,\n",
        "      attn_drop_rate=attn_drop_rate,\n",
        "      drop_path_rate=drop_path_rate,\n",
        "      norm_layer=norm_layer,\n",
        "      init_values=init_values,\n",
        "      tubelet_size=tubelet_size,\n",
        "      use_checkpoint=use_checkpoint,\n",
        "      use_learnable_pos_emb=use_learnable_pos_emb)\n",
        "\n",
        "    # self.decoder = PretrainVisionTransformerDecoder(\n",
        "    #     patch_size=patch_size,\n",
        "    #     num_patches=self.encoder.patch_embed.num_patches,\n",
        "    #     num_classes=decoder_num_classes,\n",
        "    #     embed_dim=decoder_embed_dim,\n",
        "    #     depth=decoder_depth,\n",
        "    #     num_heads=decoder_num_heads,\n",
        "    #     mlp_ratio=mlp_ratio,\n",
        "    #     qkv_bias=qkv_bias,\n",
        "    #     qk_scale=qk_scale,\n",
        "    #     drop_rate=drop_rate,\n",
        "    #     attn_drop_rate=attn_drop_rate,\n",
        "    #     drop_path_rate=drop_path_rate,\n",
        "    #     norm_layer=norm_layer,\n",
        "    #     init_values=init_values,\n",
        "    #     tubelet_size=tubelet_size,\n",
        "    #     use_checkpoint=use_checkpoint)\n",
        "\n",
        "    # self.encoder_to_decoder = nn.Linear(encoder_embed_dim, decoder_embed_dim, bias=False)\n",
        "\n",
        "    self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
        "\n",
        "    self.pos_embed = get_sinusoid_encoding_table(self.encoder.patch_embed.num_patches, decoder_embed_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    _, _, T, _, _ = x.shape\n",
        "    x_vis = self.encoder(x) # [B, N_vis, C_e]\n",
        "    # x_vis = self.encoder_to_decoder(x_vis) # [B, N_vis, C_d]\n",
        "    # B, N, C = x_vis.shape\n",
        "    # we don't unshuffle the correct visible token order,\n",
        "    # but shuffle the pos embedding accorddingly.\n",
        "    # expand_pos_embed = self.pos_embed.expand(B, -1, -1).type_as(x).to(x.device).clone().detach()\n",
        "    # pos_emd_vis = expand_pos_embed[~mask].reshape(B, -1, C)\n",
        "    # pos_emd_mask = expand_pos_embed[mask].reshape(B, -1, C)\n",
        "    # x_full = torch.cat([x_vis + pos_emd_vis, self.mask_token + pos_emd_mask], dim=1) # [B, N, C_d]\n",
        "    # x = self.decoder(x_full, pos_emd_mask.shape[1]) # [B, N_mask, 3 * 16 * 16]\n",
        "\n",
        "    # return x\n",
        "    return x_vis\n",
        "\n",
        "def pretrain_videomae_small_patch16_224():\n",
        "  model = PretrainVisionTransformer(\n",
        "      img_size=224,\n",
        "      patch_size=16,\n",
        "      encoder_embed_dim=384,\n",
        "      encoder_depth=12,\n",
        "      encoder_num_heads=6,\n",
        "      encoder_num_classes=0,\n",
        "      decoder_num_classes=1536,\n",
        "      decoder_embed_dim=192,\n",
        "      decoder_num_heads=3,\n",
        "      mlp_ratio=4,\n",
        "      qkv_bias=True,\n",
        "      norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
        "  return model\n",
        "\n",
        "def pretrain_videomae_base_patch16_224():\n",
        "  model = PretrainVisionTransformer(\n",
        "      img_size=224,\n",
        "      patch_size=16,\n",
        "      encoder_embed_dim=768,\n",
        "      encoder_depth=12,\n",
        "      encoder_num_heads=12,\n",
        "      encoder_num_classes=0,\n",
        "      decoder_num_classes=1536,\n",
        "      decoder_embed_dim=384,\n",
        "      decoder_num_heads=6,\n",
        "      mlp_ratio=4,\n",
        "      qkv_bias=True,\n",
        "      norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
        "  return model\n",
        "\n",
        "def pretrain_videomae_large_patch16_224():\n",
        "  model = PretrainVisionTransformer(\n",
        "      img_size=224,\n",
        "      patch_size=16,\n",
        "      encoder_embed_dim=1024,\n",
        "      encoder_depth=24,\n",
        "      encoder_num_heads=16,\n",
        "      encoder_num_classes=0,\n",
        "      decoder_num_classes=1536,\n",
        "      decoder_embed_dim=512,\n",
        "      decoder_num_heads=8,\n",
        "      mlp_ratio=4,\n",
        "      qkv_bias=True,\n",
        "      norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
        "  return model\n",
        "\n",
        "def pretrain_videomae_huge_patch16_224():\n",
        "  model = PretrainVisionTransformer(\n",
        "      img_size=224,\n",
        "      patch_size=16,\n",
        "      encoder_embed_dim=1280,\n",
        "      encoder_depth=32,\n",
        "      encoder_num_heads=16,\n",
        "      encoder_num_classes=0,\n",
        "      decoder_num_classes=1536,\n",
        "      decoder_embed_dim=640,\n",
        "      decoder_num_heads=8,\n",
        "      mlp_ratio=4,\n",
        "      qkv_bias=True,\n",
        "      norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
        "  return model"
      ],
      "metadata": {
        "id": "ytanbpA6QtVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Compute Memory and Latency {form-width: \"20%\"}\n",
        "\n",
        "import time\n",
        "\n",
        "model = pretrain_videomae_large_patch16_224()\n",
        "\n",
        "for num_frames in [16, 32, 64, 128, 256, 512, 1024]:\n",
        "  inputs = torch.randn(1, 3, num_frames, 256, 256).cuda()\n",
        "\n",
        "  # Measure latency\n",
        "  n_runs = 1\n",
        "  start = time.time()\n",
        "\n",
        "  for _ in range(n_runs):\n",
        "    _ = model(inputs)\n",
        "\n",
        "  torch.cuda.synchronize()  # Wait again before stopping timer\n",
        "  end = time.time()\n",
        "\n",
        "  avg_latency = (end - start) / n_runs  # seconds per inference\n",
        "\n",
        "  print(f\"Average Latency: {avg_latency*1000:.2f} ms\")"
      ],
      "metadata": {
        "id": "0E88Qj5OdSkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VideoMAE v2"
      ],
      "metadata": {
        "id": "U028FGpOT74K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define VideoMAE v2 model\n",
        "\n",
        "import transformers\n",
        "\n",
        "def get_sinusoid_encoding_table(n_position, d_hid):\n",
        "  def get_angle(pos):\n",
        "    return [pos / np.power(10000, 2 * (i // 2) / d_hid) for i in range(d_hid)]\n",
        "  table = np.array([get_angle(i) for i in range(n_position)])\n",
        "  table[:, 0::2], table[:, 1::2] = np.sin(table[:, 0::2]), np.cos(table[:, 1::2])\n",
        "  return torch.FloatTensor(table).unsqueeze(0)\n",
        "\n",
        "class VideoMAEv2Config(transformers.configuration_utils.PretrainedConfig):\n",
        "  model_type = 'VideoMAEv2_Base'\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "  def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU):\n",
        "    super().__init__()\n",
        "    out_features = out_features or in_features\n",
        "    hidden_features = hidden_features or in_features\n",
        "    self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "    self.act = act_layer()\n",
        "    self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.fc2(self.act(self.fc1(x)))\n",
        "\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., attn_head_dim=None):\n",
        "    super().__init__()\n",
        "    self.num_heads = num_heads\n",
        "    head_dim = attn_head_dim or dim // num_heads\n",
        "    all_head_dim = head_dim * self.num_heads\n",
        "    self.scale = qk_scale or head_dim**-0.5\n",
        "    self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n",
        "    if qkv_bias:\n",
        "      self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
        "      self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
        "    else:\n",
        "      self.q_bias = self.v_bias = None\n",
        "    self.attn_drop = nn.Dropout(attn_drop)\n",
        "    self.proj = nn.Linear(all_head_dim, dim)\n",
        "    self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, N, C = x.shape\n",
        "    qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias)) if self.q_bias is not None else None\n",
        "    qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias).reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
        "    q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "    attn = (q * self.scale) @ k.transpose(-2, -1)\n",
        "    attn = self.attn_drop(attn.softmax(dim=-1))\n",
        "    x = self.proj((attn @ v).transpose(1, 2).reshape(B, N, -1))\n",
        "    return self.proj_drop(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0., drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm, attn_head_dim=None, cos_attn=False):\n",
        "    super().__init__()\n",
        "    self.norm1, self.norm2 = norm_layer(dim), norm_layer(dim)\n",
        "    self.attn = Attention(dim, num_heads, qkv_bias, qk_scale, attn_drop, drop, attn_head_dim)\n",
        "    self.drop_path = nn.Identity()\n",
        "    self.mlp = Mlp(dim, int(dim * mlp_ratio), act_layer=act_layer)\n",
        "    if init_values > 0:\n",
        "      self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n",
        "      self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n",
        "    else:\n",
        "      self.gamma_1 = self.gamma_2 = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    if self.gamma_1 is None:\n",
        "      x = x + self.drop_path(self.attn(self.norm1(x)))\n",
        "      x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "    else:\n",
        "      x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x)))\n",
        "      x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n",
        "    return x\n",
        "\n",
        "def to_2tuple(x): return (x, x) if not isinstance(x, tuple) else x\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "  def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, num_frames=16, tubelet_size=2):\n",
        "    super().__init__()\n",
        "    img_size, patch_size = to_2tuple(img_size), to_2tuple(patch_size)\n",
        "    num_spatial = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])\n",
        "    num_patches = num_spatial * (num_frames // tubelet_size)\n",
        "    self.img_size, self.patch_size, self.num_patches, self.tubelet_size = img_size, patch_size, num_patches, tubelet_size\n",
        "    self.proj = nn.Conv3d(in_chans, embed_dim, (tubelet_size, patch_size[0], patch_size[1]), (tubelet_size, patch_size[0], patch_size[1]))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, C, T, H, W = x.shape\n",
        "    return self.proj(x).flatten(2).transpose(1, 2)\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "  def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0., head_drop_rate=0., norm_layer=nn.LayerNorm, layer_norm_eps=1e-12, init_values=0., use_learnable_pos_emb=False, init_scale=0., num_frames=16, tubelet_size=2, use_mean_pooling=True, with_cp=False, cos_attn=False):\n",
        "    super().__init__()\n",
        "    self.num_classes, self.num_features, self.embed_dim, self.tubelet_size = num_classes, embed_dim, embed_dim, tubelet_size\n",
        "    self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim, num_frames, tubelet_size)\n",
        "    self.patch_size = patch_size\n",
        "    num_patches = self.patch_embed.num_patches\n",
        "    self.embedding_shape = (8, 14, 14)\n",
        "    self.with_cp = with_cp\n",
        "    norm_layer = functools.partial(eval(norm_layer), eps=layer_norm_eps)\n",
        "    self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim)) if use_learnable_pos_emb else get_sinusoid_encoding_table(num_patches, embed_dim)\n",
        "    self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "    dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
        "    self.blocks = nn.ModuleList([Block(embed_dim, num_heads, mlp_ratio, qkv_bias, qk_scale, drop_rate, attn_drop_rate, dpr[i], norm_layer=norm_layer, init_values=init_values, cos_attn=cos_attn) for i in range(depth)])\n",
        "    self.norm = nn.Identity() if use_mean_pooling else norm_layer(embed_dim)\n",
        "    self.fc_norm = norm_layer(embed_dim) if use_mean_pooling else None\n",
        "    self.head_dropout = nn.Dropout(head_drop_rate)\n",
        "    self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "    if use_learnable_pos_emb:\n",
        "      nn.init.trunc_normal_(self.pos_embed, std=.02)\n",
        "\n",
        "  def interpolate_pos_encoding(self, x, h, w):\n",
        "    x = x.reshape(self.embedding_shape + (-1,))\n",
        "    dim = x.shape[-1]\n",
        "    x = F.interpolate(\n",
        "        x.permute(0, 3, 1, 2),\n",
        "        scale_factor=(h / self.embedding_shape[-2], w / self.embedding_shape[-1]),\n",
        "        mode=\"bicubic\",\n",
        "    )\n",
        "    x = x.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "    return x\n",
        "\n",
        "  def forward(self, x):\n",
        "    B = x.size(0)\n",
        "    _, _, _, h, w = x.shape\n",
        "    x = self.patch_embed(x)\n",
        "    # x = x + self.pos_embed.expand(B, -1, -1).type_as(x).to(x.device).clone().detach()\n",
        "    pos_embed = self.pos_embed.type_as(x).to(x.device).clone().detach()\n",
        "    h = h // self.patch_size\n",
        "    w = w // self.patch_size\n",
        "    x = x + self.interpolate_pos_encoding(pos_embed, h, w)\n",
        "    x = self.pos_drop(x)\n",
        "    for blk in self.blocks:\n",
        "      x = blk(x)\n",
        "    return self.fc_norm(x)\n",
        "\n",
        "\n",
        "class VideoMAEv2(transformers.PreTrainedModel):\n",
        "  config_class = VideoMAEv2Config\n",
        "  def __init__(self, config=None):\n",
        "    super().__init__(config=config)\n",
        "    self.model_config = config.model_config\n",
        "    self.model = VisionTransformer(**self.model_config)\n",
        "\n",
        "  def forward(self, video):\n",
        "    return self.model(video)"
      ],
      "metadata": {
        "id": "0w9FX2enT90p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Compute Memory and Latency {form-width: \"20%\"}\n",
        "\n",
        "import time\n",
        "\n",
        "model = VideoMAEv2.from_pretrained('OpenGVLab/VideoMAEv2-Large')\n",
        "\n",
        "for num_frames in [16, 32, 64, 128, 256, 512, 1024]:\n",
        "  inputs = torch.randn(1, 3, num_frames, 256, 256).cuda()\n",
        "\n",
        "  # Measure latency\n",
        "  n_runs = 1\n",
        "  start = time.time()\n",
        "\n",
        "  for _ in range(n_runs):\n",
        "    _ = model(inputs)\n",
        "\n",
        "  torch.cuda.synchronize()  # Wait again before stopping timer\n",
        "  end = time.time()\n",
        "\n",
        "  avg_latency = (end - start) / n_runs  # seconds per inference\n",
        "\n",
        "  print(f\"Average Latency: {avg_latency*1000:.2f} ms\")"
      ],
      "metadata": {
        "id": "kcPI_o84ehDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73nrIX6XphZp"
      },
      "source": [
        "### V-JEPA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjycvMnExgRX"
      },
      "outputs": [],
      "source": [
        "# @title Download V-JEPA code\n",
        "%cd /content\n",
        "!git clone https://github.com/facebookresearch/jepa.git\n",
        "# !pip install -e jepa\n",
        "# !pip install timm einops torchcodec torchvision torch==2.6.0\n",
        "%cd /content/jepa"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Compute Memory and Latency {form-width: \"20%\"}\n",
        "\n",
        "%cd /content/jepa\n",
        "\n",
        "import src.models.vision_transformer as vit\n",
        "import time\n",
        "\n",
        "model = vit.VisionTransformer(\n",
        "    patch_size=16, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4,\n",
        "    qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), img_size=224, num_frames=16, tubelet_size=2,\n",
        "    uniform_power=False, use_sdpa=False, use_SiLU=False, tight_SiLU=True)\n",
        "model = model.cuda()\n",
        "model = model.eval()\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "for num_frames in [16, 32, 64, 128, 256, 512, 1024]:\n",
        "  inputs = torch.randn(1, 3, num_frames, 256, 256).cuda()\n",
        "\n",
        "  # Measure latency\n",
        "  n_runs = 1\n",
        "  start = time.time()\n",
        "\n",
        "  for _ in range(n_runs):\n",
        "    _ = model(inputs)\n",
        "\n",
        "  torch.cuda.synchronize()  # Wait again before stopping timer\n",
        "  end = time.time()\n",
        "\n",
        "  avg_latency = (end - start) / n_runs  # seconds per inference\n",
        "\n",
        "  print(f\"Average Latency: {avg_latency*1000:.2f} ms\")"
      ],
      "metadata": {
        "id": "23Ez0M85kSLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFUEDNhSKiIt"
      },
      "source": [
        "### V-JEPA 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5k91TyxoA6i"
      },
      "outputs": [],
      "source": [
        "# @title Define V-JEPA 2 model\n",
        "\n",
        "import transformers\n",
        "\n",
        "class VJEPA2Model(transformers.VJEPA2PreTrainedModel):\n",
        "  def __init__(self, config):\n",
        "    super().__init__(config)\n",
        "    self.config = config\n",
        "    self.encoder = transformers.models.vjepa2.modeling_vjepa2.VJEPA2Encoder(config)\n",
        "    self.predictor = transformers.models.vjepa2.modeling_vjepa2.VJEPA2Predictor(config)\n",
        "\n",
        "  def forward(self, video):\n",
        "    encoder_outputs = self.encoder(\n",
        "        pixel_values_videos=video,\n",
        "        head_mask=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "    )\n",
        "    sequence_output = encoder_outputs.last_hidden_state\n",
        "\n",
        "    return sequence_output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Compute Memory and Latency {form-width: \"20%\"}\n",
        "\n",
        "import time\n",
        "\n",
        "model = VJEPA2Model.from_pretrained(\"facebook/vjepa2-vitl-fpc64-256\")\n",
        "model = model.cuda()\n",
        "model = model.eval()\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "for num_frames in [16, 32, 64, 128, 256, 512, 1024]:\n",
        "  inputs = torch.randn(1, num_frames, 3, 256, 256).cuda()\n",
        "\n",
        "  # Measure latency\n",
        "  n_runs = 1\n",
        "  start = time.time()\n",
        "\n",
        "  for _ in range(n_runs):\n",
        "    _ = model(inputs)\n",
        "\n",
        "  torch.cuda.synchronize()  # Wait again before stopping timer\n",
        "  end = time.time()\n",
        "\n",
        "  avg_latency = (end - start) / n_runs  # seconds per inference\n",
        "\n",
        "  print(f\"Average Latency: {avg_latency*1000:.2f} ms\")"
      ],
      "metadata": {
        "id": "yjsCEfXzgd3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlE0TwAkCpbF"
      },
      "source": [
        "### DINO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qO0Hi1buCpbF"
      },
      "outputs": [],
      "source": [
        "# @title Download DINO code\n",
        "\n",
        "%cd /content\n",
        "!git clone https://github.com/facebookresearch/dino.git\n",
        "%cd /content/dino"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Compute Memory and Latency {form-width: \"20%\"}\n",
        "\n",
        "%cd /content/dino\n",
        "\n",
        "import vision_transformer as vits\n",
        "import time\n",
        "\n",
        "model = vits.__dict__['vit_large'](patch_size=16, num_classes=0)\n",
        "model = model.cuda()\n",
        "model = model.eval()\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "for num_frames in [16, 32, 64, 128, 256, 512, 1024]:\n",
        "  inputs = torch.randn(1, num_frames, 3, 256, 256).cuda()\n",
        "\n",
        "  # Measure latency\n",
        "  n_runs = 1\n",
        "  start = time.time()\n",
        "\n",
        "  for _ in range(n_runs):\n",
        "    for t in range(num_frames):\n",
        "      _ = model(inputs[:, t])\n",
        "\n",
        "  torch.cuda.synchronize()  # Wait again before stopping timer\n",
        "  end = time.time()\n",
        "\n",
        "  avg_latency = (end - start) / n_runs  # seconds per inference\n",
        "\n",
        "  print(f\"Average Latency: {avg_latency*1000:.2f} ms\")"
      ],
      "metadata": {
        "id": "IE__98kqhJzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DINO v2"
      ],
      "metadata": {
        "id": "jqXOdbfFBvLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define DINO v2 model\n",
        "\n",
        "import transformers\n",
        "from transformers.models.dinov2.modeling_dinov2 import Dinov2Embeddings, Dinov2Encoder\n",
        "\n",
        "class Dinov2Model(transformers.Dinov2PreTrainedModel):\n",
        "  def __init__(self, config):\n",
        "    super().__init__(config)\n",
        "    self.config = config\n",
        "    self.embeddings = Dinov2Embeddings(config)\n",
        "    self.encoder = Dinov2Encoder(config)\n",
        "    self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "  def forward(self, image):\n",
        "    embedding_output = self.embeddings(image, bool_masked_pos=None)\n",
        "    encoder_outputs = self.encoder(\n",
        "        embedding_output,\n",
        "        head_mask=None,\n",
        "    )\n",
        "    sequence_output = self.layernorm(encoder_outputs[0])\n",
        "    return sequence_output"
      ],
      "metadata": {
        "id": "XWBInTp9B1Qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Compute Memory and Latency {form-width: \"20%\"}\n",
        "\n",
        "import time\n",
        "\n",
        "model = Dinov2Model.from_pretrained(\"facebook/dinov2-large\")\n",
        "model = model.cuda()\n",
        "model = model.eval()\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "for num_frames in [16, 32, 64, 128, 256, 512, 1024]:\n",
        "  inputs = torch.randn(1, num_frames, 3, 256, 256).cuda()\n",
        "\n",
        "  # Measure latency\n",
        "  n_runs = 1\n",
        "  start = time.time()\n",
        "\n",
        "  for _ in range(n_runs):\n",
        "    for t in range(num_frames):\n",
        "      _ = model(inputs[:, t])\n",
        "\n",
        "  torch.cuda.synchronize()  # Wait again before stopping timer\n",
        "  end = time.time()\n",
        "\n",
        "  avg_latency = (end - start) / n_runs  # seconds per inference\n",
        "\n",
        "  print(f\"Average Latency: {avg_latency*1000:.2f} ms\")"
      ],
      "metadata": {
        "id": "4EbfiBF2hWiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAh5ArYeTSbV"
      },
      "source": [
        "### CropMAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YI93dVCDTRja"
      },
      "outputs": [],
      "source": [
        "# @title Download CropMAE code\n",
        "\n",
        "%cd /content\n",
        "!git clone https://github.com/alexandre-eymael/CropMAE.git\n",
        "%cd /content/CropMAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uq9a4eORW04p"
      },
      "outputs": [],
      "source": [
        "# @title Load CropMAE model\n",
        "\n",
        "%cd /content/CropMAE\n",
        "\n",
        "from timm.models.vision_transformer import Block\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "  \"\"\" Image to Patch Embedding\"\"\"\n",
        "  def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "    super().__init__()\n",
        "    num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "    self.img_size = img_size\n",
        "    self.patch_size = patch_size\n",
        "    self.num_patches = num_patches\n",
        "    self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "    return x\n",
        "\n",
        "class MaskedAutoencoderViT(nn.Module):\n",
        "  \"\"\" Masked Autoencoder with VisionTransformer backbone\"\"\"\n",
        "  def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4., norm_layer=nn.LayerNorm, ckpt_path=None):\n",
        "    super().__init__()\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # MAE encoder specifics\n",
        "    self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
        "    num_patches = self.patch_embed.num_patches\n",
        "\n",
        "    self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "    self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
        "\n",
        "    self.blocks = nn.ModuleList([Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer) for i in range(depth)])\n",
        "    self.norm = norm_layer(embed_dim)\n",
        "\n",
        "  def interpolate_pos_encoding(self, x, w, h):\n",
        "    npatch = x.shape[1] - 1\n",
        "    N = self.pos_embed.shape[1] - 1\n",
        "    if npatch == N and w == h:\n",
        "      return self.pos_embed\n",
        "    class_pos_embed = self.pos_embed[:, 0]\n",
        "    patch_pos_embed = self.pos_embed[:, 1:]\n",
        "    dim = x.shape[-1]\n",
        "    w0 = w // self.patch_embed.patch_size\n",
        "    h0 = h // self.patch_embed.patch_size\n",
        "    # we add a small number to avoid floating point error in the interpolation\n",
        "    # see discussion at https://github.com/facebookresearch/dino/issues/8\n",
        "    w0, h0 = w0 + 0.1, h0 + 0.1\n",
        "    patch_pos_embed = nn.functional.interpolate(\n",
        "        patch_pos_embed.reshape(\n",
        "            1, int(math.sqrt(N)), int(math.sqrt(N)), dim\n",
        "        ).permute(0, 3, 1, 2),\n",
        "        scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n",
        "        mode=\"bicubic\",\n",
        "    )\n",
        "    assert int(w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]\n",
        "    patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "    return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, nc, w, h = x.shape\n",
        "    x = self.patch_embed(x)  # patch linear embedding\n",
        "\n",
        "    # add the [CLS] token to the embed patch tokens\n",
        "    cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "    x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "    # add positional encoding to each token\n",
        "    x = x + self.interpolate_pos_encoding(x, w, h)\n",
        "\n",
        "    for blk in self.blocks:\n",
        "      x = blk(x)\n",
        "    x = self.norm(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Compute Memory and Latency {form-width: \"20%\"}\n",
        "\n",
        "import time\n",
        "\n",
        "model = MaskedAutoencoderViT(\n",
        "    patch_size=16, embed_dim=1024, depth=24, num_heads=16,\n",
        "    mlp_ratio=4, norm_layer=functools.partial(nn.LayerNorm, eps=1e-6))\n",
        "model = model.cuda()\n",
        "model = model.eval()\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "for num_frames in [16, 32, 64, 128, 256, 512, 1024]:\n",
        "  inputs = torch.randn(1, num_frames, 3, 256, 256).cuda()\n",
        "\n",
        "  # Measure latency\n",
        "  n_runs = 1\n",
        "  start = time.time()\n",
        "\n",
        "  for _ in range(n_runs):\n",
        "    for t in range(num_frames):\n",
        "      _ = model(inputs[:, t])\n",
        "\n",
        "  torch.cuda.synchronize()  # Wait again before stopping timer\n",
        "  end = time.time()\n",
        "\n",
        "  avg_latency = (end - start) / n_runs  # seconds per inference\n",
        "\n",
        "  print(f\"Average Latency: {avg_latency*1000:.2f} ms\")"
      ],
      "metadata": {
        "id": "UZkuVtS8hmUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RVM"
      ],
      "metadata": {
        "id": "d7Qiqzqb_k-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download checkpoint\n",
        "\n",
        "%mkdir /content/rvm\n",
        "%cd /content/rvm\n",
        "\n",
        "!wget https://storage.googleapis.com/dm-tapnet/tmp/pretrain_rvm_large16_256_xid175558463_wid1.npz"
      ],
      "metadata": {
        "id": "4EgDXCb2_o_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define model {form-width: \"20%\"}\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import einops\n",
        "import re\n",
        "import dataclasses\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "  def __init__(self, patch_size=(16, 16), num_features=1024):\n",
        "    super().__init__()\n",
        "    self.patch_size = patch_size\n",
        "    self.num_features = num_features\n",
        "    self.Conv_0 = nn.Conv2d(in_channels=3, out_channels=num_features, kernel_size=patch_size, stride=patch_size, padding=0)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.permute(0, 3, 1, 2)\n",
        "    return self.Conv_0(x).permute(0, 2, 3, 1)\n",
        "\n",
        "def get_mae_sinusoid_encoding_table(n_position, d_hid, dtype=torch.float32):\n",
        "  \"\"\"Sinusoid positional encoding table for MAE.\"\"\"\n",
        "  def get_position_angle_vec(position):\n",
        "    return [position / math.pow(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]\n",
        "\n",
        "  sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n",
        "  sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
        "  sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
        "\n",
        "  return torch.tensor(sinusoid_table, dtype=dtype)[None, ...]\n",
        "\n",
        "class SincosPosEmb(nn.Module):\n",
        "  \"\"\"Returns sinusoidal positional embedding given the shape of the tokens.\"\"\"\n",
        "  def __init__(self, base_token_shape=None, use_jax_interpolation=False):\n",
        "    super().__init__()\n",
        "    self.base_token_shape = base_token_shape\n",
        "    self.use_jax_interpolation = use_jax_interpolation\n",
        "\n",
        "  def forward(self, tokens_shape):\n",
        "    d = tokens_shape[-1]\n",
        "    if self.base_token_shape is not None:\n",
        "      h, w = self.base_token_shape\n",
        "    else:\n",
        "      h, w = tokens_shape[-3], tokens_shape[-2]\n",
        "\n",
        "    posenc = get_mae_sinusoid_encoding_table(h * w, d)  # [1, h*w, d]\n",
        "    posenc = posenc.view(1, h, w, d)  # [1, h, w, d]\n",
        "\n",
        "    *b, tokens_h, tokens_w, _ = tokens_shape\n",
        "    for _ in range(len(b)-1):\n",
        "      posenc = posenc.expand(*b, -1, -1, -1)\n",
        "\n",
        "    if tokens_h != h or tokens_w != w:\n",
        "      if self.use_jax_interpolation:\n",
        "        posenc = jnp.array(posenc.numpy())\n",
        "        posenc = jax.image.resize(posenc, (*b, tokens_h, tokens_w, d), method='bicubic')\n",
        "        posenc = torch.from_numpy(np.array(posenc))\n",
        "      else:\n",
        "        posenc = posenc.view(-1, h, w, d)\n",
        "        posenc = F.interpolate(\n",
        "          posenc.permute(0, 3, 1, 2),  # [B, D, H, W]\n",
        "          size=(tokens_h, tokens_w),\n",
        "          mode='bicubic',\n",
        "          align_corners=False\n",
        "        ).permute(0, 2, 3, 1)  # [B, H, W, D]\n",
        "        posenc = posenc.view(*b, tokens_h, tokens_w, d)\n",
        "\n",
        "    return posenc.cuda()\n",
        "\n",
        "class Tokenizer(nn.Module):\n",
        "  def __init__(self, patch_embedding, posenc):\n",
        "    super().__init__()\n",
        "    self.patch_embedding = patch_embedding\n",
        "    self.posenc = posenc\n",
        "\n",
        "  def forward(self, x):\n",
        "    tokens = self.patch_embedding(x)\n",
        "    # posenc = self.posenc(tokens.shape)\n",
        "    # tokens += posenc\n",
        "    return tokens\n",
        "\n",
        "class TransformerMLP(nn.Module):\n",
        "  \"\"\"Simple MLP with a single hidden layer for use in Transformer blocks.\"\"\"\n",
        "  def __init__(self, input_dim, hidden_size=None):\n",
        "    super().__init__()\n",
        "    self.hidden_size = 4 * input_dim if hidden_size is None else hidden_size\n",
        "    self.dense_in = nn.Linear(input_dim, self.hidden_size)\n",
        "    self.dense_out = nn.Linear(self.hidden_size, input_dim)\n",
        "    nn.init.xavier_uniform_(self.dense_in.weight)\n",
        "    nn.init.zeros_(self.dense_in.bias)\n",
        "    nn.init.xavier_uniform_(self.dense_out.weight)\n",
        "    nn.init.zeros_(self.dense_out.bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    h = F.gelu(self.dense_in(x))\n",
        "    return self.dense_out(h)\n",
        "\n",
        "def dot_product_attention_weights(query, key):\n",
        "  query = query / math.sqrt(query.size(-1))\n",
        "  attn_weights = torch.einsum('bqhd,bkhd->bhqk', query, key)\n",
        "  attn_weights = F.softmax(attn_weights, dim=-1)\n",
        "  return attn_weights\n",
        "\n",
        "class ImprovedMultiHeadDotProductAttention(nn.Module):\n",
        "  def __init__(self, embed_dim, num_heads, qk_features=None, v_features=None, out_features=None):\n",
        "    super().__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.qk_features = qk_features or embed_dim\n",
        "    self.v_features = v_features or self.qk_features\n",
        "    self.out_features = out_features or embed_dim\n",
        "\n",
        "    # Head dimensions\n",
        "    self.head_dim_qk = self.qk_features // self.num_heads\n",
        "    self.head_dim_v = self.v_features // self.num_heads\n",
        "\n",
        "    # Linear projections\n",
        "    self.query = nn.Linear(embed_dim, self.qk_features)\n",
        "    self.key  = nn.Linear(embed_dim, self.qk_features)\n",
        "    self.value = nn.Linear(embed_dim, self.v_features)\n",
        "\n",
        "    # Output projection\n",
        "    self.out = nn.Linear(self.v_features, self.out_features)\n",
        "\n",
        "  def forward(self, inputs_q, inputs_k=None, inputs_v=None, mask=None):\n",
        "    batch_size, seq_len_q, _ = inputs_q.shape\n",
        "    if inputs_k is None:\n",
        "      inputs_k = inputs_q\n",
        "    if inputs_v is None:\n",
        "      inputs_v = inputs_k\n",
        "\n",
        "    seq_len_k = inputs_k.shape[1]\n",
        "\n",
        "    # Linear projections and reshape to (batch, seq_len, num_heads, head_dim)\n",
        "    query = self.query(inputs_q).view(batch_size, seq_len_q, self.num_heads, self.head_dim_qk)\n",
        "    key   = self.key(inputs_k).view(batch_size, seq_len_k, self.num_heads, self.head_dim_qk)\n",
        "    value = self.value(inputs_v).view(batch_size, seq_len_k, self.num_heads, self.head_dim_v)\n",
        "\n",
        "    # Scaled dot-product attention\n",
        "    query_scaled = query / math.sqrt(self.head_dim_qk)\n",
        "    attn_weights = torch.einsum('bqhd,bkhd->bhqk', query_scaled, key)\n",
        "    if mask is not None:\n",
        "      attn_weights = attn_weights.masked_fill(mask == 0, float('-inf'))\n",
        "    attn_weights = F.softmax(attn_weights, dim=-1)\n",
        "\n",
        "    # Weighted sum over values\n",
        "    x = torch.einsum('bhqk,bkhd->bqhd', attn_weights, value)\n",
        "    x = x.reshape(batch_size, seq_len_q, self.num_heads * self.head_dim_v)\n",
        "\n",
        "    # Output projection\n",
        "    out = self.out(x)\n",
        "    return out\n",
        "\n",
        "class PreNormBlock(nn.Module):\n",
        "  def __init__(self, attention_norm, mlp_norm, attention, mlp):\n",
        "    super().__init__()\n",
        "    self.attention_norm = attention_norm\n",
        "    self.mlp_norm = mlp_norm\n",
        "    self.attention = attention\n",
        "    self.mlp = mlp\n",
        "\n",
        "  def forward(self, x):\n",
        "    norm_x = self.attention_norm(x)\n",
        "    x = x + self.attention(norm_x)\n",
        "    norm_x = self.mlp_norm(x)\n",
        "    x = x + self.mlp(norm_x)\n",
        "    return x\n",
        "\n",
        "VIT_SIZES = {\n",
        "    'mu': (32, 1, 128, 2),\n",
        "    'Ti': (192, 12, 768, 3),\n",
        "    'S': (384, 12, 1536, 6),\n",
        "    'M': (512, 12, 2048, 8),\n",
        "    'B': (768, 12, 3072, 12),\n",
        "    'L': (1024, 24, 4096, 16),\n",
        "    'H': (1280, 32, 5120, 16),\n",
        "    'g': (1408, 40, 6144, 16),\n",
        "    'G': (1664, 48, 8192, 16),\n",
        "    'e': (1792, 56, 15360, 16),\n",
        "}\n",
        "\n",
        "@dataclasses.dataclass(frozen=True)\n",
        "class ViTSpec:\n",
        "  hidden_size: int\n",
        "  num_layers: int\n",
        "  mlp_size: int\n",
        "  num_heads: int\n",
        "  patch_size: int = None\n",
        "\n",
        "  @classmethod\n",
        "  def from_variant_string(cls, variant_str: str):\n",
        "    r = re.match(r'^([Vv][Ii][Tt][-_])?(?P<name>[a-zA-Z]{1,2})(/(?P<patch>\\d+))?$', variant_str)\n",
        "    if r is None:\n",
        "      raise ValueError(f'Invalid variant string: {variant_str!r}.')\n",
        "    name = r.groupdict()['name']\n",
        "    spec = cls(*VIT_SIZES[name])\n",
        "    patch_size = r.groupdict()['patch']\n",
        "    if patch_size is not None:\n",
        "      spec = dataclasses.replace(spec, patch_size=int(patch_size))\n",
        "    return spec\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, num_layers, hidden_size, num_heads, mlp_size, qk_features=None, v_features=None):\n",
        "    super().__init__()\n",
        "    self.layers = nn.ModuleList([\n",
        "        PreNormBlock(\n",
        "            attention_norm=nn.LayerNorm(hidden_size, eps=1e-06, dtype=torch.float32),\n",
        "            mlp_norm=nn.LayerNorm(hidden_size, eps=1e-06, dtype=torch.float32),\n",
        "            attention=ImprovedMultiHeadDotProductAttention(\n",
        "                embed_dim=hidden_size,\n",
        "                num_heads=num_heads,\n",
        "                qk_features=qk_features or hidden_size,\n",
        "                v_features=v_features or hidden_size,\n",
        "            ),\n",
        "            mlp=TransformerMLP(input_dim=hidden_size, hidden_size=mlp_size),\n",
        "        )\n",
        "        for _ in range(num_layers)\n",
        "    ])\n",
        "    self.LayerNorm_0 = nn.LayerNorm(hidden_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    return self.LayerNorm_0(x)\n",
        "\n",
        "  @classmethod\n",
        "  def from_variant_str(cls, variant_str: str, **kwargs):\n",
        "    spec = ViTSpec.from_variant_string(variant_str)\n",
        "    all_kwargs = dict(\n",
        "      num_layers=spec.num_layers,\n",
        "      hidden_size=spec.hidden_size,\n",
        "      mlp_size=spec.mlp_size,\n",
        "      num_heads=spec.num_heads,\n",
        "    )\n",
        "    all_kwargs.update(kwargs)\n",
        "    return cls(**all_kwargs)\n",
        "\n",
        "class CrossAttentionBlock(nn.Module):\n",
        "  def __init__(self, num_heads, num_feats, mlp_dim, dtype=torch.float32):\n",
        "    super().__init__()\n",
        "    self.attention_norm = nn.LayerNorm(num_feats, eps=1e-6, dtype=dtype)\n",
        "    self.mlp_norm = nn.LayerNorm(num_feats, eps=1e-6, dtype=dtype)\n",
        "    self.ca_attention_norm = nn.LayerNorm(num_feats, eps=1e-6, dtype=dtype)\n",
        "\n",
        "    self.attention = ImprovedMultiHeadDotProductAttention(\n",
        "      embed_dim=num_feats, num_heads=num_heads\n",
        "    )\n",
        "    self.ca_attention = ImprovedMultiHeadDotProductAttention(\n",
        "      embed_dim=num_feats, num_heads=num_heads\n",
        "    )\n",
        "    self.mlp = TransformerMLP(input_dim=num_feats, hidden_size=mlp_dim)\n",
        "\n",
        "  def forward(self, x, x_kv):\n",
        "    residual = x\n",
        "    x = x + self.ca_attention(inputs_q=self.ca_attention_norm(x), inputs_k=x_kv, inputs_v=x_kv)\n",
        "    x = x + self.mlp(self.mlp_norm(x))\n",
        "    x = x + self.attention(self.attention_norm(x))\n",
        "    return x\n",
        "\n",
        "class CrossAttentionTransformer(nn.Module):\n",
        "  def __init__(self, num_layers, num_heads, num_feats, mlp_dim, dtype=torch.float32):\n",
        "    super().__init__()\n",
        "    self.xa_blocks = nn.ModuleList([\n",
        "      CrossAttentionBlock(num_heads, num_feats, mlp_dim, dtype=dtype)\n",
        "      for _ in range(num_layers)\n",
        "    ])\n",
        "    self.output_norm = nn.LayerNorm(num_feats, eps=1e-6, dtype=dtype)\n",
        "\n",
        "  def forward(self, inputs, inputs_kv):\n",
        "    x = inputs\n",
        "    for block in self.xa_blocks:\n",
        "      x = block(x, inputs_kv)\n",
        "    return self.output_norm(x)\n",
        "\n",
        "class RandomStateInit(nn.Module):\n",
        "  \"\"\"Random, non-learnable state initialization.\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, inputs, batch_shape):\n",
        "    shape = inputs.shape[-2:]\n",
        "    state = 0 * torch.randn(batch_shape + shape, dtype=inputs.dtype, device=inputs.device)\n",
        "    return state\n",
        "\n",
        "class GatedTransformerCore(nn.Module):\n",
        "  def __init__(self, transformer, initializer, token_dim, state_layer_norm):\n",
        "    super().__init__()\n",
        "    self.transformer = transformer\n",
        "    self.initializer = initializer\n",
        "    self.token_dim = token_dim\n",
        "    self.state_layer_norm = state_layer_norm\n",
        "\n",
        "    self.input_update = nn.Linear(token_dim, token_dim, bias=False)\n",
        "    self.input_reset = nn.Linear(token_dim, token_dim, bias=False)\n",
        "    self.state_update = nn.Linear(token_dim, token_dim, bias=False)\n",
        "    self.state_reset = nn.Linear(token_dim, token_dim, bias=False)\n",
        "\n",
        "  def forward(self, inputs, state):\n",
        "    update_gate = F.sigmoid(self.input_update(inputs) + self.state_update(state))\n",
        "    reset_gate = F.sigmoid(self.input_reset(inputs) + self.state_reset(state))\n",
        "    h = self.transformer(inputs, inputs_kv=reset_gate * self.state_layer_norm(state))\n",
        "    output = (1 - update_gate) * state + update_gate * h\n",
        "    state = output\n",
        "    return output, state\n",
        "\n",
        "class VideoSiamMAE(nn.Module):\n",
        "  \"\"\"Video Siamese masked autoencoder model.\"\"\"\n",
        "\n",
        "  def __init__(self, tokenizer, encoder, rnn_core, latent_emb_dim=384):\n",
        "    super().__init__()\n",
        "    self.tokenizer = tokenizer\n",
        "    self.encoder = encoder\n",
        "    self.rnn_core = rnn_core\n",
        "    self.latent_emb_dim = latent_emb_dim\n",
        "\n",
        "    # cls_token is a learnable parameter\n",
        "    self.cls_token = nn.Parameter(torch.randn(1, 1, latent_emb_dim) * 0.02)\n",
        "\n",
        "  def forward(self, frame, state=None):\n",
        "    # Tokenize input frame\n",
        "    frame_tokens = self.tokenizer(frame)  # shape [..., h, w, D] expected\n",
        "    frame_tokens = einops.rearrange(frame_tokens, '... h w d -> ... (h w) d')\n",
        "\n",
        "    *b, _, _ = frame_tokens.shape\n",
        "    # Broadcast cls_token across batch\n",
        "    cls_token = self.cls_token.expand(*b, -1, -1)  # shape [..., 1, D]\n",
        "\n",
        "    # Concatenate CLS with patch tokens\n",
        "    frame_tokens = torch.cat([cls_token, frame_tokens], dim=-2)\n",
        "\n",
        "    # Encode with transformer encoder\n",
        "    encoded_frame_tokens = self.encoder(frame_tokens)\n",
        "\n",
        "    # Initialize state if first step\n",
        "    if state is None:\n",
        "        # Expect initializer to accept (inputs, batch_shape)\n",
        "        state = self.rnn_core.initializer(encoded_frame_tokens, batch_shape=(1,))\n",
        "\n",
        "    # Recurrent core update\n",
        "    features, state = self.rnn_core(encoded_frame_tokens, state)\n",
        "\n",
        "    return features, state\n",
        "\n",
        "model = VideoSiamMAE(\n",
        "    tokenizer=Tokenizer(\n",
        "        patch_embedding=PatchEmbedding(patch_size=[16, 16], num_features=1024),\n",
        "        posenc=SincosPosEmb(base_token_shape=[16, 16]),\n",
        "    ),\n",
        "    encoder=Transformer.from_variant_str(variant_str='L'),\n",
        "    rnn_core=GatedTransformerCore(\n",
        "        transformer=CrossAttentionTransformer(\n",
        "            num_layers=4,\n",
        "            num_heads=16,\n",
        "            num_feats=1024,\n",
        "            mlp_dim=4096,\n",
        "            dtype=torch.float32,\n",
        "        ),\n",
        "        initializer=RandomStateInit(),\n",
        "        token_dim=1024,\n",
        "        state_layer_norm=nn.LayerNorm(1024, eps=0.0001, bias=False),\n",
        "    ),\n",
        "    latent_emb_dim=1024,\n",
        ")\n",
        "model = model.cuda()\n",
        "model = model.eval()\n",
        "torch.set_grad_enabled(False)"
      ],
      "metadata": {
        "id": "nDchT_aY_wCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load checkpoint\n",
        "\n",
        "%cd /content/rvm\n",
        "\n",
        "def recover_tree(flat_dict):\n",
        "  tree = {}\n",
        "  for k, v in flat_dict.items():\n",
        "    parts = k.split(\"/\")\n",
        "    node = tree\n",
        "    for part in parts[:-1]:\n",
        "      if part not in node:\n",
        "        node[part] = {}\n",
        "      node = node[part]\n",
        "    node[parts[-1]] = v\n",
        "  return tree\n",
        "\n",
        "def flatten_flax_params(params, parent_key=\"\"):\n",
        "  \"\"\"\n",
        "  Flatten nested Flax params dict into {'a.b.c': subdict}.\n",
        "  \"\"\"\n",
        "  items = {}\n",
        "  for k, v in params.items():\n",
        "    new_key = f\"{parent_key}.{k}\" if parent_key else k\n",
        "    if isinstance(v, dict):\n",
        "      items.update(flatten_flax_params(v, new_key))\n",
        "    else:\n",
        "      items[new_key] = v\n",
        "  return items\n",
        "\n",
        "def flax_to_torch(flat_flax, torch_model):\n",
        "  for name, param in torch_model.named_parameters():\n",
        "    # Normalize naming\n",
        "    name_fixed = name.replace('layers.', 'layers_')\n",
        "    name_fixed = name_fixed.replace('blocks.', 'blocks_')\n",
        "\n",
        "    flax_key = None\n",
        "\n",
        "    if name == \"cls_token\":\n",
        "      flax_key = \"cls_token\"\n",
        "\n",
        "    elif name.endswith(\"weight\"):\n",
        "      # Try Linear/Conv kernels\n",
        "      flax_key = name_fixed.replace(\"weight\", \"kernel\")\n",
        "      if flax_key not in flat_flax:\n",
        "        # Try LayerNorm scale\n",
        "        flax_key = name_fixed.replace(\"weight\", \"scale\")\n",
        "\n",
        "    elif name.endswith(\"bias\"):\n",
        "      flax_key = name_fixed  # bias names usually match directly\n",
        "\n",
        "    if flax_key is None or flax_key not in flat_flax:\n",
        "      print(f\"[WARN] Missing weights for {name} (flax_key={flax_key})\")\n",
        "      continue\n",
        "\n",
        "    # Load array\n",
        "    array = np.array(flat_flax[flax_key])\n",
        "    tensor = torch.tensor(array)\n",
        "\n",
        "    # Handle Conv2d kernel\n",
        "    if param.ndim == 4:\n",
        "      # Flax: [H, W, in, out]  Torch: [out, in, H, W]\n",
        "      if tensor.ndim == 5 and tensor.shape[0] == 1:  # Sometimes an extra batch dim\n",
        "        tensor = tensor[0]\n",
        "      tensor = tensor.permute(3, 2, 0, 1)\n",
        "\n",
        "    # Handle Linear kernels\n",
        "    elif param.ndim == 2:\n",
        "      if tensor.ndim == 2:\n",
        "        # Dense: [in, out]  [out, in]\n",
        "        tensor = tensor.T\n",
        "      elif tensor.ndim == 3:\n",
        "        # DenseGeneral\n",
        "        if param.shape[0] == tensor.shape[-1] * tensor.shape[-2]:  # Q/K/V projection\n",
        "          tensor = tensor.reshape(tensor.shape[0], -1).T\n",
        "        else:  # Output projection\n",
        "          tensor = tensor.reshape(-1, tensor.shape[-1]).T\n",
        "      else:\n",
        "        raise ValueError(f\"Unexpected kernel shape {tensor.shape} for {name}\")\n",
        "\n",
        "    # Reshape if needed (bias, cls_token, norm, etc.)\n",
        "    tensor = tensor.reshape(param.shape)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      param.copy_(tensor)\n",
        "\n",
        "    print(f\"Loaded {name} from {flax_key}\")\n",
        "\n",
        "restored_params = recover_tree(np.load(\"pretrain_rvm_large16_256_xid175558463_wid1.npz\", allow_pickle=False))\n",
        "\n",
        "flat_flax = flatten_flax_params(restored_params)\n",
        "flax_to_torch(flat_flax, model)"
      ],
      "metadata": {
        "id": "77tSO8PnIjNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Compute Memory and Latency {form-width: \"20%\"}\n",
        "\n",
        "import time\n",
        "\n",
        "for num_frames in [16, 32, 64, 128, 256, 512, 1024]:\n",
        "  inputs = torch.randn(1, num_frames, 256, 256, 3).cuda()\n",
        "\n",
        "  # Measure latency\n",
        "  n_runs = 1\n",
        "  start = time.time()\n",
        "\n",
        "  for _ in range(n_runs):\n",
        "    state = None\n",
        "    for t in range(num_frames):\n",
        "      output, state = model(inputs[:, t], state)\n",
        "\n",
        "  torch.cuda.synchronize()  # Wait again before stopping timer\n",
        "  end = time.time()\n",
        "\n",
        "  avg_latency = (end - start) / n_runs  # seconds per inference\n",
        "\n",
        "  print(f\"Average Latency: {avg_latency*1000:.2f} ms\")"
      ],
      "metadata": {
        "id": "7NIgdC2L_zro"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "last_runtime": {
        "build_target": "//learning/pytorch/colab:kernel",
        "kind": "private"
      },
      "machine_shape": "hm",
      "provenance": [
        {
          "file_id": "1u5---Un_ja0zDuNHYw7CyT5aZ7vv5OPo",
          "timestamp": 1765586494499
        },
        {
          "file_id": "1BA8StfouHvQChfUHN264qkkKWyQ4LWQa",
          "timestamp": 1758208667573
        },
        {
          "file_id": "1VX4rxrjP05POLVzRqjOo1KCXHbBCg_9c",
          "timestamp": 1755600525138
        },
        {
          "file_id": "1davdOLPoQ4KiWkKc6HKWRb48IJTqUET3",
          "timestamp": 1753193893350
        },
        {
          "file_id": "1gNb5SWr16OHEOcHIkRI783Cz-B5he_sK",
          "timestamp": 1752155778633
        },
        {
          "file_id": "1Sg7f1ibn8r6YaAz6by1no0zXn4K9kw-W",
          "timestamp": 1747303081093
        },
        {
          "file_id": "1Dla4d31nDY1skIkEKRCeSSW2l9RLCFH6",
          "timestamp": 1742835424224
        },
        {
          "file_id": "1NiZzcShUkJIyI3OBi4rHat0WXQhVVqN2",
          "timestamp": 1741194275175
        },
        {
          "file_id": "1Cmj_lBDUFDPp5A_YDU3xZUjgfDAd8L1P",
          "timestamp": 1741046636775
        },
        {
          "file_id": "1em9gkinQ6-F3TnBtwL5TdYZmvAkD7XRA",
          "timestamp": 1741034974907
        },
        {
          "file_id": "1npI3DYCA629BZXYbEUt8fHJGT_A-_9CN",
          "timestamp": 1741004612797
        }
      ],
      "toc_visible": true,
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
