{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WS5t185lhfMa"
      },
      "outputs": [],
      "source": [
        "# @title Install dependency\n",
        "\n",
        "!pip install mediapy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8FfWGdoIq4B"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "\n",
        "import functools\n",
        "import glob\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "\n",
        "import cv2\n",
        "import einops\n",
        "from PIL import Image\n",
        "import scipy.io as sio\n",
        "import mediapy as media\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import queue\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import tqdm\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uq7e4u4pjN5m"
      },
      "source": [
        "### Prepare videos for qualitative visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmODMFlwnD3X"
      },
      "outputs": [],
      "source": [
        "# @title Download DAVIS videos\n",
        "\n",
        "video_names = [\n",
        "    'goat',\n",
        "    # 'india',\n",
        "    # 'soapbox',\n",
        "]\n",
        "\n",
        "for video_name in video_names:\n",
        "  !wget https://storage.googleapis.com/dm-tapnet/davis2017/{video_name}.mp4\n",
        "\n",
        "davis_videos = []\n",
        "for video_name in video_names:\n",
        "  video = media.read_video(f\"{video_name}.mp4\")\n",
        "  video = media.resize_video(video, (480, 880))\n",
        "  davis_videos.append(video)\n",
        "  media.show_video(video, fps=16, height=128, codec='gif')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mgqCSVRj0_Q"
      },
      "outputs": [],
      "source": [
        "# @title Download MoCA videos\n",
        "\n",
        "video_names = [\n",
        "    'scorpionfish_0',\n",
        "    # 'snow_leopard_8',\n",
        "    # 'white_tailed_ptarmigan',\n",
        "    # 'grasshopper_1',\n",
        "    # 'plaice',\n",
        "    # 'pygmy_seahorse_0',\n",
        "    # 'stick_insect_0',\n",
        "    # 'crab',\n",
        "    # 'crab_1',\n",
        "    # 'copperhead_snake',\n",
        "]\n",
        "\n",
        "for video_name in video_names:\n",
        "  !wget https://storage.googleapis.com/dm-tapnet/MoCA/{video_name}.mp4\n",
        "\n",
        "moca_videos = []\n",
        "for video_name in video_names:\n",
        "  video = media.read_video(f\"{video_name}.mp4\")\n",
        "  video = media.resize_video(video, (360, 640))\n",
        "  moca_videos.append(video)\n",
        "  media.show_video(video, fps=16, width=256, codec='gif')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download Dalmatian videos\n",
        "\n",
        "!wget https://storage.googleapis.com/dm-tapnet/tmp/dalmatian_illusion.mp4\n",
        "\n",
        "dalmatian_video = media.read_video(\"dalmatian_illusion.mp4\")\n",
        "dalmatian_video = media.resize_video(dalmatian_video, (360, 480))\n",
        "media.show_video(dalmatian_video, fps=16, width=256, codec='gif')"
      ],
      "metadata": {
        "id": "17S-pK1rrpWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUWdU_F1jTwi"
      },
      "outputs": [],
      "source": [
        "# @title Create noise video\n",
        "\n",
        "def create_noise_movie(T, h, w):\n",
        "  background = np.random.rand(h, w, 3)\n",
        "  square = np.random.rand(h // 2, w // 2, 3)\n",
        "  movie = []\n",
        "  for t in range(T):\n",
        "    frame = background.copy()\n",
        "    # Calculate the position of the square\n",
        "    x = (t * (w - w // 2)) // T  # Move across the width\n",
        "    y = (h - h // 2) // 2  # Centered vertically\n",
        "    frame[y:y + h // 2, x:x + w // 2] = square\n",
        "    movie.append(frame)\n",
        "  return (np.array(movie) * 255.0).astype(np.uint8)\n",
        "\n",
        "# noise_video = create_noise_movie(32, 256, 256)\n",
        "noise_video = create_noise_movie(64, 512, 512)\n",
        "media.show_video(noise_video, fps=16, height=128, codec='gif')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RV1tHjRfSzu0"
      },
      "outputs": [],
      "source": [
        "# @title Combine qualitative videos\n",
        "\n",
        "videos = davis_videos + moca_videos + [noise_video]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare datasets and metrics for quantitative evaluation"
      ],
      "metadata": {
        "id": "_3i1vtpV-x_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Download datasets"
      ],
      "metadata": {
        "id": "Z8PW3Ka-_aL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download DAVIS-2017 dataset\n",
        "\n",
        "%cd /content\n",
        "!git clone https://github.com/davisvideochallenge/davis-2017\n",
        "%cd /content/davis-2017\n",
        "!./data/get_davis.sh"
      ],
      "metadata": {
        "id": "VZ9woeHz-35i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download JHMDB dataset\n",
        "\n",
        "%cd /content\n",
        "!wget https://storage.googleapis.com/dm-tapnet/jhmdb.zip\n",
        "!unzip jhmdb.zip"
      ],
      "metadata": {
        "id": "GMVfjvCJ-74A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download VIP dataset\n",
        "\n",
        "%cd /content\n",
        "!wget https://storage.googleapis.com/dm-tapnet/VIP.zip\n",
        "!unzip VIP.zip"
      ],
      "metadata": {
        "id": "75gFWGar_AKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS4AhhImUdBk"
      },
      "source": [
        "#### Main Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2c5kZa_fUdBl"
      },
      "outputs": [],
      "source": [
        "# @title Dataset functions\n",
        "\n",
        "def create_davis_dataset(root_path):\n",
        "  \"\"\"DAVIS dataset, including fields required for video segmentation evaluation.\"\"\"\n",
        "  for video_id in tf.io.gfile.GFile(os.path.join(root_path, 'ImageSets/2017/val.txt'), 'r'):\n",
        "    video_id = video_id.strip()\n",
        "    frame_files = sorted(tf.io.gfile.glob(os.path.join(root_path, 'JPEGImages/480p', video_id, '*.jpg')))\n",
        "    frames = np.stack([np.array(Image.open(tf.io.gfile.GFile(f, 'rb')).convert('RGB')) for f in frame_files])\n",
        "    label_files = sorted(tf.io.gfile.glob(os.path.join(root_path, 'Annotations/480p', video_id, '*.png')))\n",
        "    labels = np.stack([np.array(Image.open(tf.io.gfile.GFile(f, 'rb'))) for f in label_files])\n",
        "    yield {'video': frames, 'mask': labels, 'video_id': video_id}\n",
        "\n",
        "def create_jhmdb_dataset(jhmdb_path):\n",
        "  \"\"\"JHMDB dataset, including fields required for PCK evaluation.\"\"\"\n",
        "  video_ids = []\n",
        "  for file in os.listdir(os.path.join(jhmdb_path, 'splits')):\n",
        "    if file.endswith('split1.txt'):\n",
        "      video_folder = '_'.join(file.split('_')[:-2])\n",
        "      lines = open(os.path.join(jhmdb_path, 'splits', file), 'r').readlines()\n",
        "      for line in lines:\n",
        "        video_name, traintest = line.split()\n",
        "        if int(traintest) == 2:\n",
        "          video_id = os.path.join(video_folder, video_name.split('.')[0])\n",
        "          video_ids.append(video_id)\n",
        "\n",
        "  random.shuffle(video_ids)\n",
        "  for video_id in video_ids:\n",
        "    joints = os.path.join(jhmdb_path, 'joint_positions', video_id, 'joint_positions.mat')\n",
        "    pose = sio.loadmat(joints)['pos_img'] - 1  # matlab -> python\n",
        "    pose = pose.astype(np.float32).transpose(2, 1, 0)  # (num_frames, num_points, 2)\n",
        "    frame_files = sorted(glob.glob(os.path.join(jhmdb_path, 'Rename_Images', video_id, '*.png')))\n",
        "    frames = np.stack([np.array(Image.open(open(f, 'rb')).convert('RGB')) for f in frame_files])\n",
        "    yield {'video': frames, 'pose': pose, 'video_id': video_id}\n",
        "\n",
        "def create_vip_dataset(root_path):\n",
        "  \"\"\"VIP dataset, including fields required for video segmentation evaluation.\"\"\"\n",
        "  for video_id in tf.io.gfile.GFile(os.path.join(root_path, 'lists/val_videos.txt'), 'r'):\n",
        "    video_id = video_id.strip()\n",
        "    frame_files = sorted(tf.io.gfile.glob(os.path.join(root_path, 'Images', video_id, '*.jpg')))\n",
        "    frames = np.stack([np.array(Image.open(tf.io.gfile.GFile(f, 'rb')).convert('RGB')) for f in frame_files])\n",
        "    label_files = sorted(tf.io.gfile.glob(os.path.join(root_path, 'Annotations/Category_ids', video_id, '*.png')))\n",
        "    labels = np.stack([np.array(Image.open(tf.io.gfile.GFile(f, 'rb'))) for f in label_files])\n",
        "    yield {'video': frames, 'mask': labels, 'video_id': video_id}\n",
        "\n",
        "def create_tapvid_davis_dataset(pickle_path):\n",
        "  \"\"\"TAPVid-DAVIS dataset, including fields required for point tracking evaluation.\"\"\"\n",
        "  with tf.io.gfile.GFile(pickle_path, 'rb') as f:\n",
        "    point_tracks = pickle.load(f)\n",
        "\n",
        "  for video_id in point_tracks.keys():\n",
        "    frames = point_tracks[video_id]['video']\n",
        "    points = point_tracks[video_id]['points']\n",
        "    occluded = point_tracks[video_id]['occluded']\n",
        "\n",
        "    points = points * np.array([frames.shape[2], frames.shape[1]])\n",
        "    points = points.astype(np.float32).transpose(1, 0, 2)  # (num_frames, num_points, 2)\n",
        "    visible = np.logical_not(occluded)\n",
        "    visible = visible.transpose(1, 0)  # (num_frames, num_points)\n",
        "\n",
        "    valid = visible[0]\n",
        "    points = points[:, valid]\n",
        "    visible = visible[:, valid]\n",
        "\n",
        "    yield {'video': frames, 'points': points, 'visible': visible, 'video_id': video_id}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AL_yz16bVOzq"
      },
      "outputs": [],
      "source": [
        "# @title Label propagation functions\n",
        "\n",
        "def draw_labelmap_np(img, pt, sigma=0.5):\n",
        "  # Draw a 2D gaussian\n",
        "  # Adopted from https://github.com/anewell/pose-hg-train/blob/master/src/pypose/draw.py\n",
        "\n",
        "  # Check that any part of the gaussian is in-bounds\n",
        "  ul = [int(pt[0] - 3 * sigma), int(pt[1] - 3 * sigma)]\n",
        "  br = [int(pt[0] + 3 * sigma + 1), int(pt[1] + 3 * sigma + 1)]\n",
        "  if (ul[0] >= img.shape[1] or ul[1] >= img.shape[0] or br[0] < 0 or br[1] < 0):\n",
        "    # If not, just return the image as is\n",
        "    return img\n",
        "\n",
        "  # Generate gaussian\n",
        "  size = 6 * sigma + 1\n",
        "  x = np.arange(0, size, 1, float)\n",
        "  y = x[:, np.newaxis]\n",
        "  x0 = y0 = size // 2\n",
        "  g = np.exp(- ((x - x0) ** 2 + (y - y0) ** 2) / (2 * sigma ** 2))\n",
        "\n",
        "  # Usable gaussian range\n",
        "  g_x = max(0, -ul[0]), min(br[0], img.shape[1]) - ul[0]\n",
        "  g_y = max(0, -ul[1]), min(br[1], img.shape[0]) - ul[1]\n",
        "  # Image range\n",
        "  img_x = max(0, ul[0]), min(br[0], img.shape[1])\n",
        "  img_y = max(0, ul[1]), min(br[1], img.shape[0])\n",
        "\n",
        "  img[img_y[0]:img_y[1], img_x[0]:img_x[1]] = g[g_y[0]:g_y[1], g_x[0]:g_x[1]]\n",
        "  return img\n",
        "\n",
        "def mask2heatmap(mask, h, w, height, width):\n",
        "  \"\"\"Convert segmentation mask to heatmap (resize and one-hot encode)\"\"\"\n",
        "  label_map = np.unique(mask)\n",
        "  mask = cv2.resize(mask, (width, height), interpolation=cv2.INTER_NEAREST)\n",
        "  heatmap = np.stack([mask == l for l in label_map], axis=-1).astype(np.float32)\n",
        "  heatmap = cv2.resize(heatmap, (w, h), interpolation=cv2.INTER_LINEAR)\n",
        "  return heatmap, label_map\n",
        "\n",
        "def pose2heatmap(pose, h, w, height, width):\n",
        "  \"\"\"Convert pose to heatmap (resize and one-hot encode)\"\"\"\n",
        "  n_class = pose.shape[0]\n",
        "  coord = pose * np.array([w / width, h / height])\n",
        "  heatmap = np.zeros((h, w, n_class + 1), dtype=np.float32)\n",
        "  for i in range(n_class):\n",
        "    heatmap[..., i + 1] = draw_labelmap_np(np.zeros((h, w)), coord[i])\n",
        "  heatmap[..., 0] = heatmap.sum(axis=-1) == 0\n",
        "  return heatmap\n",
        "\n",
        "def process_pose(preds, h, w, ori_height, ori_width, topk=5):\n",
        "  current_coords, jnt_visibles = [], []\n",
        "  for t in range(preds.shape[0]):\n",
        "    pred = preds[t][..., 1:]\n",
        "    flatlbls = pred.flatten(0, 1)\n",
        "    vals, ids = torch.topk(flatlbls, k=topk, dim=0)\n",
        "    vals /= vals.sum(0)[None]\n",
        "    xx, yy = ids % pred.shape[1], ids // pred.shape[1]\n",
        "    current_coord = torch.stack([(xx * vals).sum(0), (yy * vals).sum(0)], dim=0)\n",
        "    current_coord[0, :] = current_coord[0, :] / w * ori_width\n",
        "    current_coord[1, :] = current_coord[1, :] / h * ori_height\n",
        "    current_coord[:, flatlbls.sum(0) == 0] = -1\n",
        "    current_coord = current_coord.cpu().numpy().transpose(1, 0)\n",
        "    jnt_visible = (current_coord[:, 0] >= 0).astype(float)  # (n_class)\n",
        "    current_coords.append(current_coord)\n",
        "    jnt_visibles.append(jnt_visible)\n",
        "  current_coords = np.stack(current_coords, axis=0)\n",
        "  jnt_visibles = np.stack(jnt_visibles, axis=0)\n",
        "  return current_coords, jnt_visibles\n",
        "\n",
        "def process_points(preds, h, w, ori_height, ori_width, topk=5):\n",
        "  points = []\n",
        "  for t in range(preds.shape[0]):\n",
        "    pred = preds[t][..., 1:]\n",
        "    flatlbls = pred.flatten(0, 1)\n",
        "    vals, ids = torch.topk(flatlbls, k=topk, dim=0)\n",
        "    vals /= vals.sum(0)[None]\n",
        "    xx, yy = ids % pred.shape[1], ids // pred.shape[1]\n",
        "    point = torch.stack([(xx * vals).sum(0), (yy * vals).sum(0)], dim=0)\n",
        "    point[0, :] = point[0, :] / w * ori_width\n",
        "    point[1, :] = point[1, :] / h * ori_height\n",
        "    point[:, flatlbls.sum(0) == 0] = -1\n",
        "    point = point.cpu().numpy().transpose(1, 0)\n",
        "    points.append(point)\n",
        "  points = np.stack(points, axis=0)\n",
        "  return points\n",
        "\n",
        "def process_segmentation(preds, lbl_map, height, width, ori_height, ori_width):\n",
        "  pred_lbls = []\n",
        "  for t in range(preds.shape[0]):\n",
        "    pred = preds[t]\n",
        "    pred = pred.cpu().numpy()\n",
        "    # Upsample predicted soft label maps\n",
        "    pred_dist = cv2.resize(pred, (width, height))[:]\n",
        "    # Argmax to get the hard label for index\n",
        "    pred_lbl = np.argmax(pred_dist, axis=-1)\n",
        "    pred_lbl = np.array(lbl_map, dtype=np.int32)[pred_lbl]\n",
        "    pred_lbl = cv2.resize(pred_lbl, (ori_width, ori_height), interpolation=cv2.INTER_NEAREST_EXACT)\n",
        "    pred_lbls.append(pred_lbl)\n",
        "  pred_lbls = np.stack(pred_lbls, axis=0)\n",
        "  return pred_lbls\n",
        "\n",
        "def label_propagation(feats, heatmap, n_context=20, temperature=0.7, topk=7, radius=20, restrict_neighborhood=True, norm_mask=False):\n",
        "  \"\"\"Propagation of the heatmap based on feature similarity.\"\"\"\n",
        "\n",
        "  # Creates a mask indicating valid neighbors for each grid element.\n",
        "  h, w = feats.shape[1], feats.shape[2]\n",
        "  gx, gy = torch.meshgrid(torch.arange(0, h), torch.arange(0, w), indexing=\"ij\")  # (h, w)\n",
        "  D = ((gx[None, None, :, :] - gx[:, :, None, None])**2 + (gy[None, None, :, :] - gy[:, :, None, None])**2).float() ** 0.5\n",
        "  D = (D < radius).float().to('cuda')\n",
        "  D[D == 0] = -1e10\n",
        "  D[D == 1] = 0\n",
        "  D = D.permute(2, 3, 0, 1)  # (h, w, h, w)\n",
        "\n",
        "  # The queue stores the context frames\n",
        "  que = queue.Queue(n_context)\n",
        "  for _ in range(n_context):\n",
        "    que.put([feats[0], heatmap])\n",
        "\n",
        "  preds = []\n",
        "  for t in range(feats.shape[0]):\n",
        "    # Use first and previous frames as context\n",
        "    ctx_feats = torch.stack([feats[0]] + [pair[0] for pair in que.queue])\n",
        "    ctx_lbls = torch.stack([heatmap] + [pair[1] for pair in que.queue])\n",
        "\n",
        "    aff = torch.einsum('hwc, tmnc -> hwtmn', feats[t], ctx_feats) / temperature  # (h, w, n_context+1, h, w)\n",
        "    if restrict_neighborhood:\n",
        "      aff[:, :, 1:] += D[:, :, None]  # (h, w, n_context+1, h, w)\n",
        "    aff = aff.view(aff.shape[0], aff.shape[1], -1)  # (h, w, n_context+1 * h * w)\n",
        "\n",
        "    weights, ids = torch.topk(aff, topk, dim=-1)  # (h, w, topk), (h, w, topk)\n",
        "    weights = F.softmax(weights, dim=-1)  # (h, w, topk)\n",
        "    ctx_lbls = ctx_lbls.view(-1, ctx_lbls.shape[-1])  # (n_context+1 * h * w, n_class)\n",
        "    pred = torch.einsum('hwlk, hwl -> hwk', ctx_lbls[ids], weights) # (h, w, n_class)\n",
        "\n",
        "    if que.qsize() == n_context:\n",
        "      que.get()\n",
        "    que.put([feats[t], pred])\n",
        "\n",
        "    if norm_mask:\n",
        "      pred -= pred.min(-1)[0][..., None]\n",
        "      pred /= pred.max(-1)[0][..., None]\n",
        "\n",
        "    preds.append(pred)\n",
        "  preds = torch.stack(preds)\n",
        "  return preds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Visualization functions\n",
        "\n",
        "def visualize_pca(features, video):\n",
        "  pca = sklearn.decomposition.PCA(n_components=3, whiten=True)\n",
        "  pca_data = pca.fit_transform(einops.rearrange(features, 't n m c -> (t n m) c'))\n",
        "  pca_data = pca_data.reshape(features.shape[:-1] + (3,))\n",
        "  pca_video = (pca_data - pca_data.min()) / (pca_data.max() - pca_data.min())\n",
        "  pca_video = jax.image.resize(pca_video, video.shape, method='nearest')\n",
        "  return pca_video\n",
        "\n",
        "def segmentations_to_video(masks):\n",
        "  num_objects = np.max(masks)  # assume consecutive numbering\n",
        "  # palette = [(0, 0, 0)] + sns.color_palette(n_colors=num_objects)\n",
        "  palette = sns.color_palette(n_colors=num_objects + 1)\n",
        "  video = np.zeros((masks.shape[0], masks.shape[1], masks.shape[2], 3))\n",
        "  for i in range(num_objects + 1):\n",
        "    video[masks == i] = palette[i]\n",
        "  return video\n",
        "\n",
        "def visualize_kmeans(features, video, n_clusters=5):\n",
        "  kmeans = sklearn.cluster.KMeans(n_clusters, init='k-means++')\n",
        "  result = kmeans.fit(einops.rearrange(features, 't n m c -> (t n m) c'))\n",
        "  kmeans_labels = jnp.reshape(result.labels_, features.shape[:-1])\n",
        "  kmeans_video = segmentations_to_video(kmeans_labels)\n",
        "  kmeans_video = jax.image.resize(kmeans_video, video.shape, method='nearest')\n",
        "  return kmeans_video"
      ],
      "metadata": {
        "id": "cisN-wkIJAPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl0G1du2UPDV"
      },
      "source": [
        "#### Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dw97pfvUPDV"
      },
      "outputs": [],
      "source": [
        "# @title DAVIS video segmentation evaluation metric\n",
        "# https://github.com/davisvideochallenge/davis2017-evaluation/blob/master/davis2017/metrics.py\n",
        "\n",
        "import math\n",
        "import cv2\n",
        "\n",
        "def db_eval_iou(annotation, segmentation, void_pixels=None):\n",
        "  \"\"\" Compute region similarity as the Jaccard Index.\n",
        "  Arguments:\n",
        "      annotation   (ndarray): binary annotation   map.\n",
        "      segmentation (ndarray): binary segmentation map.\n",
        "      void_pixels  (ndarray): optional mask with void pixels\n",
        "\n",
        "  Return:\n",
        "      jaccard (float): region similarity\n",
        "  \"\"\"\n",
        "  assert annotation.shape == segmentation.shape, \\\n",
        "      f'Annotation({annotation.shape}) and segmentation:{segmentation.shape} dimensions do not match.'\n",
        "  annotation = annotation.astype(bool)\n",
        "  segmentation = segmentation.astype(bool)\n",
        "\n",
        "  if void_pixels is not None:\n",
        "    assert annotation.shape == void_pixels.shape, \\\n",
        "        f'Annotation({annotation.shape}) and void pixels:{void_pixels.shape} dimensions do not match.'\n",
        "    void_pixels = void_pixels.astype(bool)\n",
        "  else:\n",
        "    void_pixels = np.zeros_like(segmentation)\n",
        "\n",
        "  # Intersection between all sets\n",
        "  inters = np.sum((segmentation & annotation) & np.logical_not(void_pixels), axis=(-2, -1))\n",
        "  union = np.sum((segmentation | annotation) & np.logical_not(void_pixels), axis=(-2, -1))\n",
        "\n",
        "  j = inters / union\n",
        "  if j.ndim == 0:\n",
        "    j = 1 if np.isclose(union, 0) else j\n",
        "  else:\n",
        "    j[np.isclose(union, 0)] = 1\n",
        "  return j\n",
        "\n",
        "\n",
        "def db_eval_boundary(annotation, segmentation, void_pixels=None, bound_th=0.008):\n",
        "  assert annotation.shape == segmentation.shape\n",
        "  if void_pixels is not None:\n",
        "    assert annotation.shape == void_pixels.shape\n",
        "  if annotation.ndim == 3:\n",
        "    n_frames = annotation.shape[0]\n",
        "    f_res = np.zeros(n_frames)\n",
        "    for frame_id in range(n_frames):\n",
        "      void_pixels_frame = None if void_pixels is None else void_pixels[frame_id, :, :, ]\n",
        "      f_res[frame_id] = f_measure(segmentation[frame_id, :, :, ], annotation[frame_id, :, :], void_pixels_frame, bound_th=bound_th)\n",
        "  elif annotation.ndim == 2:\n",
        "    f_res = f_measure(segmentation, annotation, void_pixels, bound_th=bound_th)\n",
        "  else:\n",
        "    raise ValueError(f'db_eval_boundary does not support tensors with {annotation.ndim} dimensions')\n",
        "  return f_res\n",
        "\n",
        "\n",
        "def f_measure(foreground_mask, gt_mask, void_pixels=None, bound_th=0.008):\n",
        "  \"\"\"\n",
        "  Compute mean,recall and decay from per-frame evaluation.\n",
        "  Calculates precision/recall for boundaries between foreground_mask and\n",
        "  gt_mask using morphological operators to speed it up.\n",
        "\n",
        "  Arguments:\n",
        "      foreground_mask (ndarray): binary segmentation image.\n",
        "      gt_mask         (ndarray): binary annotated image.\n",
        "      void_pixels     (ndarray): optional mask with void pixels\n",
        "\n",
        "  Returns:\n",
        "      F (float): boundaries F-measure\n",
        "  \"\"\"\n",
        "  assert np.atleast_3d(foreground_mask).shape[2] == 1\n",
        "  if void_pixels is not None:\n",
        "    void_pixels = void_pixels.astype(bool)\n",
        "  else:\n",
        "    void_pixels = np.zeros_like(foreground_mask).astype(bool)\n",
        "\n",
        "  bound_pix = bound_th if bound_th >= 1 else \\\n",
        "      np.ceil(bound_th * np.linalg.norm(foreground_mask.shape))\n",
        "\n",
        "  # Get the pixel boundaries of both masks\n",
        "  fg_boundary = _seg2bmap(foreground_mask * np.logical_not(void_pixels))\n",
        "  gt_boundary = _seg2bmap(gt_mask * np.logical_not(void_pixels))\n",
        "\n",
        "  from skimage.morphology import disk\n",
        "\n",
        "  # fg_dil = binary_dilation(fg_boundary, disk(bound_pix))\n",
        "  fg_dil = cv2.dilate(fg_boundary.astype(np.uint8), disk(bound_pix).astype(np.uint8))\n",
        "  # gt_dil = binary_dilation(gt_boundary, disk(bound_pix))\n",
        "  gt_dil = cv2.dilate(gt_boundary.astype(np.uint8), disk(bound_pix).astype(np.uint8))\n",
        "\n",
        "  # Get the intersection\n",
        "  gt_match = gt_boundary * fg_dil\n",
        "  fg_match = fg_boundary * gt_dil\n",
        "\n",
        "  # Area of the intersection\n",
        "  n_fg = np.sum(fg_boundary)\n",
        "  n_gt = np.sum(gt_boundary)\n",
        "\n",
        "  # % Compute precision and recall\n",
        "  if n_fg == 0 and n_gt > 0:\n",
        "    precision = 1\n",
        "    recall = 0\n",
        "  elif n_fg > 0 and n_gt == 0:\n",
        "    precision = 0\n",
        "    recall = 1\n",
        "  elif n_fg == 0 and n_gt == 0:\n",
        "    precision = 1\n",
        "    recall = 1\n",
        "  else:\n",
        "    precision = np.sum(fg_match) / float(n_fg)\n",
        "    recall = np.sum(gt_match) / float(n_gt)\n",
        "\n",
        "  # Compute F measure\n",
        "  if precision + recall == 0:\n",
        "    F = 0\n",
        "  else:\n",
        "    F = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "  return F\n",
        "\n",
        "\n",
        "def _seg2bmap(seg, width=None, height=None):\n",
        "  \"\"\"\n",
        "  From a segmentation, compute a binary boundary map with 1 pixel wide\n",
        "  boundaries.  The boundary pixels are offset by 1/2 pixel towards the\n",
        "  origin from the actual segment boundary.\n",
        "  Arguments:\n",
        "      seg     : Segments labeled from 1..k.\n",
        "      width   : Width of desired bmap  <= seg.shape[1]\n",
        "      height  : Height of desired bmap <= seg.shape[0]\n",
        "  Returns:\n",
        "      bmap (ndarray): Binary boundary map.\n",
        "   David Martin <dmartin@eecs.berkeley.edu>\n",
        "   January 2003\n",
        "  \"\"\"\n",
        "\n",
        "  seg = seg.astype(bool)\n",
        "  seg[seg > 0] = 1\n",
        "\n",
        "  assert np.atleast_3d(seg).shape[2] == 1\n",
        "\n",
        "  width = seg.shape[1] if width is None else width\n",
        "  height = seg.shape[0] if height is None else height\n",
        "\n",
        "  h, w = seg.shape[:2]\n",
        "\n",
        "  ar1 = float(width) / float(height)\n",
        "  ar2 = float(w) / float(h)\n",
        "\n",
        "  assert not (\n",
        "      width > w | height > h | abs(ar1 - ar2) > 0.01\n",
        "  ), \"Can't convert %dx%d seg to %dx%d bmap.\" % (w, h, width, height)\n",
        "\n",
        "  e = np.zeros_like(seg)\n",
        "  s = np.zeros_like(seg)\n",
        "  se = np.zeros_like(seg)\n",
        "\n",
        "  e[:, :-1] = seg[:, 1:]\n",
        "  s[:-1, :] = seg[1:, :]\n",
        "  se[:-1, :-1] = seg[1:, 1:]\n",
        "\n",
        "  b = seg ^ e | seg ^ s | seg ^ se\n",
        "  b[-1, :] = seg[-1, :] ^ e[-1, :]\n",
        "  b[:, -1] = seg[:, -1] ^ s[:, -1]\n",
        "  b[-1, -1] = 0\n",
        "\n",
        "  if w == width and h == height:\n",
        "    bmap = b\n",
        "  else:\n",
        "    bmap = np.zeros((height, width))\n",
        "    for x in range(w):\n",
        "      for y in range(h):\n",
        "        if b[y, x]:\n",
        "          j = 1 + math.floor((y - 1) + height / h)\n",
        "          i = 1 + math.floor((x - 1) + width / h)\n",
        "          bmap[j, i] = 1\n",
        "\n",
        "  return bmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJ3ZNja3UPDW"
      },
      "outputs": [],
      "source": [
        "# @title JHMDB pck evaluation metric\n",
        "# https://github.com/Liusifei/UVC/blob/jhmdb/eval_pck.py\n",
        "\n",
        "def compute_human_boxes(gts, jnt_visible_set):\n",
        "  human_boxes = []\n",
        "  for nowgt, jnt_visible in zip(gts, jnt_visible_set):\n",
        "    now_boxes = np.zeros(nowgt.shape[0])\n",
        "    for t in range(nowgt.shape[0]):\n",
        "      visible_pts = nowgt[t, jnt_visible[t] == 1]\n",
        "      if visible_pts.size:\n",
        "        min_pt, max_pt = visible_pts.min(axis=0), visible_pts.max(axis=0)\n",
        "        now_boxes[t] = 0.6 * np.linalg.norm(max_pt - min_pt)\n",
        "    human_boxes.append(now_boxes)\n",
        "  return human_boxes\n",
        "\n",
        "def compute_distances(gts, preds, human_boxes, jnt_visible_set):\n",
        "  distAll = {pidx: np.array([]) for pidx in range(15)}\n",
        "  for nowgt, predres, now_boxes, jnt_visible in zip(gts, preds, human_boxes, jnt_visible_set):\n",
        "    for i in range(nowgt.shape[1]):\n",
        "      for t in range(1, nowgt.shape[0]):\n",
        "        if jnt_visible[t, i]:\n",
        "          distAll[i] = np.append(distAll[i], np.linalg.norm(predres[t, i] - nowgt[t, i]) / now_boxes[t])\n",
        "  return distAll\n",
        "\n",
        "def computePCK(distAll, distThresh):\n",
        "  pckAll = np.zeros(len(distAll))\n",
        "  for pidx, distances in distAll.items():\n",
        "    pckAll[pidx] = 100.0 * np.sum(distances <= distThresh) / len(distances)\n",
        "  pck = np.mean(pckAll)\n",
        "  return pck, pckAll"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEcrf4hfUPDW"
      },
      "outputs": [],
      "source": [
        "# @title VIP video segmentation evaluation metric\n",
        "\n",
        "def fast_hist(a, b, n):\n",
        "  k = (a >= 0) & (a < n)\n",
        "  return np.bincount(n * a[k].astype(int) + b[k], minlength=n**2).reshape(n, n)\n",
        "\n",
        "def compute_iou(hist):\n",
        "  classes = ['background', 'hat', 'hair', 'sun-glasses', 'upper-clothes', 'dress',\n",
        "           'coat', 'socks', 'pants', 'gloves', 'scarf', 'skirt', 'torso-skin',\n",
        "           'face', 'right-arm', 'left-arm', 'right-leg', 'left-leg', 'right-shoe', 'left-shoe']\n",
        "\n",
        "  num_cor_pix = np.diag(hist)  # num of correct pixels\n",
        "  num_gt_pix = hist.sum(1)  # num of gt pixels\n",
        "  union = num_gt_pix + hist.sum(0) - num_cor_pix\n",
        "  iou_per_class = {}\n",
        "  for i in range(len(classes)):\n",
        "    iou_per_class[classes[i]] = num_cor_pix[i] / union[i]\n",
        "  iou = num_cor_pix / (num_gt_pix + hist.sum(0) - num_cor_pix)\n",
        "  iou = np.nanmean(iou)\n",
        "  return iou, iou_per_class"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluation Functions"
      ],
      "metadata": {
        "id": "QHuLdvs-LUCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title DAVIS evaluation function\n",
        "\n",
        "def evaluate_davis(model, extract_feature_function, dataset_path='/content/davis-2017/DAVIS/', height=480, width=880, patch_size=16, params=None):\n",
        "  davis_dataset = create_davis_dataset(dataset_path)\n",
        "  h, w = height // patch_size, width // patch_size  # feature resolution\n",
        "\n",
        "  n_max_class = 10\n",
        "  j_metrics, f_metrics, fj_metrics, counters = [], [], [], []\n",
        "  for sample in tqdm.tqdm(davis_dataset):\n",
        "    ori_height, ori_width = sample['video'].shape[1], sample['video'].shape[2]\n",
        "\n",
        "    # Extract features\n",
        "    video = media.resize_video(sample['video'], (height, width))\n",
        "    if params is None:\n",
        "      feats = extract_feature_function(model, video)  # PyTorch models\n",
        "    else:\n",
        "      feats = extract_feature_function(model, params, video)  # Jax models\n",
        "    if not isinstance(feats, torch.Tensor):\n",
        "      feats = torch.tensor(feats).cuda()\n",
        "    feats = torch.nn.functional.normalize(feats, dim=-1)\n",
        "\n",
        "    # Prepare downscaled first frame segmentation (resize and one-hot encode)\n",
        "    lbls_small, lbl_map = mask2heatmap(sample['mask'][0], h, w, height, width)\n",
        "    lbls_small = torch.tensor(lbls_small).cuda()\n",
        "\n",
        "    pred_lbls = label_propagation(feats, lbls_small, n_context=20, temperature=0.7, topk=7, radius=20, restrict_neighborhood=True, norm_mask=False)\n",
        "\n",
        "    pred_lbls = process_segmentation(pred_lbls, lbl_map, height, width, ori_height, ori_width)\n",
        "\n",
        "    n_class = int(sample['mask'].max())  # Get the number of objects in the segmentation map\n",
        "\n",
        "    masks = sample['mask'][1:-1]\n",
        "    all_gt_masks = np.eye(n_class + 1)[masks]\n",
        "    all_gt_masks = all_gt_masks[..., 1:]  # Remove background class\n",
        "\n",
        "    masks = pred_lbls[1:-1]\n",
        "    all_res_masks = np.eye(n_class + 1)[masks]\n",
        "    all_res_masks = all_res_masks[..., 1:]  # Remove background class\n",
        "\n",
        "    num_frames = all_gt_masks.shape[0]\n",
        "    j_metrics_res = np.zeros((n_class, num_frames))\n",
        "    f_metrics_res = np.zeros((n_class, num_frames))\n",
        "    for ii in range(n_class):\n",
        "      j_metrics_res[ii, :] = db_eval_iou(all_gt_masks[..., ii], all_res_masks[..., ii], None)\n",
        "      f_metrics_res[ii, :] = db_eval_boundary(all_gt_masks[..., ii], all_res_masks[..., ii], None)\n",
        "\n",
        "    JM, FM, FJM = np.zeros((n_max_class,)), np.zeros((n_max_class,)), np.zeros((n_max_class,))\n",
        "    for ii in range(n_class):\n",
        "      JM[ii] = np.nanmean(j_metrics_res[ii])\n",
        "      FM[ii] = np.nanmean(f_metrics_res[ii])\n",
        "      FJM[ii] = (JM[ii] + FM[ii]) / 2.\n",
        "    counter = np.zeros((n_max_class,))\n",
        "    counter[:n_class] = 1\n",
        "\n",
        "    j_metrics.append(JM)\n",
        "    f_metrics.append(FM)\n",
        "    fj_metrics.append(FJM)\n",
        "    counters.append(counter)\n",
        "\n",
        "  j_metrics = np.array(j_metrics)\n",
        "  f_metrics = np.array(f_metrics)\n",
        "  fj_metrics = np.array(fj_metrics)\n",
        "  counters = np.array(counters)\n",
        "\n",
        "  fj_metric = (fj_metrics * counters).sum() / counters.sum()\n",
        "  j_metric = (j_metrics * counters).sum() / counters.sum()\n",
        "  f_metric = (f_metrics * counters).sum() / counters.sum()\n",
        "\n",
        "  print('')\n",
        "  print('J&F-Mean',fj_metric)\n",
        "  print('J-Mean', j_metric)\n",
        "  print('F-Mean', f_metric)"
      ],
      "metadata": {
        "id": "8nJYZ_rtLSOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title JHMDB evaluation function\n",
        "\n",
        "def evaluate_jhmdb(model, extract_feature_function, dataset_path='/content/jhmdb/', height=320, width=320, patch_size=16, params=None):\n",
        "  jhmdb_dataset = create_jhmdb_dataset(jhmdb_path=dataset_path)\n",
        "  h, w = height // patch_size, width // patch_size  # feature resolution\n",
        "\n",
        "  gts, preds, jnt_visible_set = [], [], []\n",
        "  for sample in tqdm.tqdm(jhmdb_dataset):\n",
        "    ori_height, ori_width = sample['video'].shape[1], sample['video'].shape[2]\n",
        "\n",
        "    # Extract features\n",
        "    video = media.resize_video(sample['video'], (height, width))\n",
        "    if params is None:\n",
        "      feats = extract_feature_function(model, video)  # PyTorch models\n",
        "    else:\n",
        "      feats = extract_feature_function(model, params, video)  # Jax models\n",
        "    if not isinstance(feats, torch.Tensor):\n",
        "      feats = torch.tensor(feats).cuda()\n",
        "    feats = torch.nn.functional.normalize(feats, dim=-1)\n",
        "\n",
        "    # Prepare downscaled first frame heatmap (resize and one-hot encode)\n",
        "    lbls_small = pose2heatmap(sample['pose'][0], h, w, ori_height, ori_width)\n",
        "    lbls_small = torch.tensor(lbls_small).cuda()\n",
        "\n",
        "    pred_lbls = label_propagation(feats, lbls_small, n_context=20, temperature=0.7, topk=7, radius=20, restrict_neighborhood=True, norm_mask=False)\n",
        "\n",
        "    pred_lbls, jnt_visible = process_pose(pred_lbls, h, w, ori_height, ori_width)\n",
        "    preds.append(pred_lbls)\n",
        "    jnt_visible_set.append(jnt_visible)\n",
        "    gts.append(sample['pose'])\n",
        "\n",
        "  human_boxes = compute_human_boxes(gts, jnt_visible_set)\n",
        "  distAll = compute_distances(gts, preds, human_boxes, jnt_visible_set)\n",
        "\n",
        "  for threshold in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
        "    print(f\"PCK@{threshold}: {computePCK(distAll, threshold)[0]}\")"
      ],
      "metadata": {
        "id": "ewb62UaHfDTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title VIP evaluation function\n",
        "\n",
        "def evaluate_vip(model, extract_feature_function, dataset_path='/content/VIP/', height=448, width=880, patch_size=16, params=None):\n",
        "  vip_dataset = create_vip_dataset(root_path=dataset_path)\n",
        "  h, w = height // patch_size, width // patch_size  # feature resolution\n",
        "\n",
        "  n_class = 20\n",
        "  hist = np.zeros((n_class, n_class))\n",
        "  for sample in tqdm.tqdm(vip_dataset):\n",
        "    ori_height, ori_width = sample['video'].shape[1], sample['video'].shape[2]\n",
        "\n",
        "    # Extract features\n",
        "    video = media.resize_video(sample['video'], (height, width))\n",
        "    if params is None:\n",
        "      feats = extract_feature_function(model, video)  # PyTorch models\n",
        "    else:\n",
        "      feats = extract_feature_function(model, params, video)  # Jax models\n",
        "    if not isinstance(feats, torch.Tensor):\n",
        "      feats = torch.tensor(feats).cuda()\n",
        "    feats = torch.nn.functional.normalize(feats, dim=-1)\n",
        "\n",
        "    # Prepare downscaled first frame segmentation (resize and one-hot encode)\n",
        "    lbls_small, lbl_map = mask2heatmap(sample['mask'][0], h, w, height, width)\n",
        "    lbls_small = torch.tensor(lbls_small).cuda()\n",
        "\n",
        "    pred_lbls = label_propagation(feats, lbls_small, n_context=20, temperature=0.7, topk=10, radius=20, restrict_neighborhood=True, norm_mask=False)\n",
        "\n",
        "    pred_lbls = process_segmentation(pred_lbls, lbl_map, height, width, ori_height, ori_width)\n",
        "\n",
        "    for t in range(pred_lbls.shape[0]):\n",
        "      hist += fast_hist(sample['mask'][t], pred_lbls[t], n_class)\n",
        "\n",
        "  iou, iou_per_class = compute_iou(hist)\n",
        "\n",
        "  print('')\n",
        "  for key in iou_per_class:\n",
        "    print('%-15s: %f' % (key, iou_per_class[key]))\n",
        "  print ('mean IoU', iou)"
      ],
      "metadata": {
        "id": "plVS6zBnek0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckyTA9d_hwH3"
      },
      "source": [
        "### VideoMAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkBNYZBPiFvh"
      },
      "outputs": [],
      "source": [
        "# @title Load VideoMAE model\n",
        "\n",
        "import transformers\n",
        "\n",
        "def get_sinusoid_encoding_table(n_position, d_hid):\n",
        "  \"\"\"Sinusoid position encoding table\"\"\"\n",
        "\n",
        "  # TODO: make it with torch instead of numpy\n",
        "  def get_position_angle_vec(position):\n",
        "    return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]\n",
        "\n",
        "  sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n",
        "  sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
        "  sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
        "\n",
        "  return torch.FloatTensor(sinusoid_table).unsqueeze(0)\n",
        "\n",
        "class VideoMAEPatchEmbeddings(nn.Module):\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    image_size = config.image_size\n",
        "    patch_size = config.patch_size\n",
        "    num_channels = config.num_channels\n",
        "    hidden_size = config.hidden_size\n",
        "    num_frames = config.num_frames\n",
        "    tubelet_size = config.tubelet_size\n",
        "\n",
        "    image_size = image_size if isinstance(image_size, (tuple, list)) else (image_size, image_size)\n",
        "    patch_size = patch_size if isinstance(patch_size, (tuple, list)) else (patch_size, patch_size)\n",
        "    self.image_size = image_size\n",
        "    self.patch_size = patch_size\n",
        "    self.tubelet_size = int(tubelet_size)\n",
        "    self.num_channels = num_channels\n",
        "    self.num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0]) * (num_frames // self.tubelet_size)\n",
        "    self.projection = nn.Conv3d(\n",
        "      in_channels=num_channels,\n",
        "      out_channels=hidden_size,\n",
        "      kernel_size=(self.tubelet_size, patch_size[0], patch_size[1]),\n",
        "      stride=(self.tubelet_size, patch_size[0], patch_size[1]),\n",
        "    )\n",
        "\n",
        "  def forward(self, pixel_values):\n",
        "    # permute to (batch_size, num_channels, num_frames, height, width)\n",
        "    pixel_values = pixel_values.permute(0, 2, 1, 3, 4)\n",
        "    embeddings = self.projection(pixel_values).flatten(2).transpose(1, 2)\n",
        "    return embeddings\n",
        "\n",
        "class VideoMAEEmbeddings(nn.Module):\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.patch_embeddings = VideoMAEPatchEmbeddings(config)\n",
        "    self.num_patches = self.patch_embeddings.num_patches\n",
        "    self.embedding_shape = (8, 14, 14)  # Manually added for resizing position embedding\n",
        "    # fixed sin-cos embedding\n",
        "    self.position_embeddings = get_sinusoid_encoding_table(self.num_patches, config.hidden_size)\n",
        "    self.config = config\n",
        "\n",
        "  # def forward(self, pixel_values):\n",
        "  #     embeddings = self.patch_embeddings(pixel_values)\n",
        "  #     embeddings = embeddings + self.position_embeddings.detach().type_as(embeddings).to(device=embeddings.device, copy=True)\n",
        "  #     return embeddings\n",
        "\n",
        "  def interpolate_pos_encoding(self, x, h, w):\n",
        "    x = x.reshape(self.embedding_shape + (-1,))\n",
        "    dim = x.shape[-1]\n",
        "    x = F.interpolate(\n",
        "      x.permute(0, 3, 1, 2),\n",
        "      scale_factor=(h / self.embedding_shape[-2], w / self.embedding_shape[-1]),\n",
        "      mode=\"bicubic\",\n",
        "    )\n",
        "    x = x.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "    return x\n",
        "\n",
        "  def forward(self, pixel_values):\n",
        "    embeddings = self.patch_embeddings(pixel_values)\n",
        "    position_embeddings = self.position_embeddings.to(embeddings.device)\n",
        "    _, _, _, h, w = pixel_values.shape\n",
        "    h = h // self.patch_embeddings.patch_size[0]\n",
        "    w = w // self.patch_embeddings.patch_size[1]\n",
        "    embeddings = embeddings + self.interpolate_pos_encoding(position_embeddings, h, w)\n",
        "    return embeddings\n",
        "\n",
        "class VideoMAEModel(transformers.VideoMAEPreTrainedModel):\n",
        "  def __init__(self, config):\n",
        "    super().__init__(config)\n",
        "    self.config = config\n",
        "    self.embeddings = VideoMAEEmbeddings(config)\n",
        "    self.encoder = transformers.models.videomae.modeling_videomae.VideoMAEEncoder(config)\n",
        "\n",
        "    if config.use_mean_pooling:\n",
        "      self.layernorm = None\n",
        "    else:\n",
        "      self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "    self.post_init()\n",
        "\n",
        "  def forward(self, pixel_values):\n",
        "    embedding_output = self.embeddings(pixel_values)\n",
        "\n",
        "    encoder_outputs = self.encoder(\n",
        "      embedding_output,\n",
        "      head_mask=None,\n",
        "    )\n",
        "    sequence_output = encoder_outputs[0]\n",
        "    if self.layernorm is not None:\n",
        "      sequence_output = self.layernorm(sequence_output)\n",
        "\n",
        "    return sequence_output\n",
        "\n",
        "model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-large\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.eval()\n",
        "torch.set_grad_enabled(False)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ok4JNJUVIvV"
      },
      "outputs": [],
      "source": [
        "# @title Feature extraction function\n",
        "\n",
        "WINDOW_LENGTH = 16\n",
        "PATCH_SIZE = 16\n",
        "\n",
        "def extract_features(model, video):\n",
        "  video = video.astype(np.float32) / 255.0\n",
        "  video = (video - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
        "  h, w = video.shape[1] // PATCH_SIZE, video.shape[2] // PATCH_SIZE  # feature resolution\n",
        "\n",
        "  num_frames = video.shape[0]\n",
        "  if num_frames % WINDOW_LENGTH: # VideoMAE requires 16 frames per clip\n",
        "    video = np.pad(video, ((0, WINDOW_LENGTH - num_frames % WINDOW_LENGTH), (0, 0), (0, 0), (0, 0)))\n",
        "\n",
        "  torch.cuda.empty_cache()\n",
        "  video = torch.tensor(video).permute(0, 3, 1, 2).float().cuda()  # (T, 3, th, tw)\n",
        "  features = []\n",
        "  for t in range(0, video.shape[0], WINDOW_LENGTH):\n",
        "    feature = model(video[t : t + WINDOW_LENGTH][None])[0]  # (h * w + 1, c)\n",
        "    feature = feature.view(WINDOW_LENGTH // 2, h, w, -1)  # (h, w, c)\n",
        "    feature = F.interpolate(feature.permute(3, 0, 1, 2)[None], size=(WINDOW_LENGTH, h, w), mode='trilinear')\n",
        "    features.append(feature[0].permute(1, 2, 3, 0))\n",
        "  features = torch.concatenate(features, dim=0)  # (T, h, w, c)\n",
        "  features = features[:num_frames]\n",
        "  return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2S4mKA1H-oxk"
      },
      "outputs": [],
      "source": [
        "# @title PCA visualization and k-means clustering {form-width: \"20%\"}\n",
        "\n",
        "for video in videos:\n",
        "  features = extract_features(model, video)\n",
        "  features = features.cpu().numpy()\n",
        "  print(features.shape)\n",
        "\n",
        "  pca_video = visualize_pca(features, video)\n",
        "  kmeans_video = visualize_kmeans(features, video)\n",
        "  mixed_video = 0.5 * video / 255.0 + 0.5 * kmeans_video\n",
        "  video_titles = ['pca', 'kmeans', 'mixed']\n",
        "  media.show_videos([pca_video, kmeans_video, mixed_video], titles=video_titles, height=128, codec='gif', fps=16, columns=3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title DAVIS evaluation\n",
        "\n",
        "evaluate_davis(model, extract_features, dataset_path='/content/davis-2017/DAVIS/', height=480, width=880, patch_size=16)"
      ],
      "metadata": {
        "id": "Y0BolI1nAno4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title JHMDB evaluation\n",
        "\n",
        "evaluate_jhmdb(model, extract_features, dataset_path='/content/jhmdb/', height=320, width=320, patch_size=16)"
      ],
      "metadata": {
        "id": "BPpBE4FCArBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title VIP evaluation\n",
        "\n",
        "evaluate_vip(model, extract_features, dataset_path='/content/VIP/', height=448, width=880, patch_size=16)"
      ],
      "metadata": {
        "id": "uGz5pSBqAwb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VideoMAE from Github"
      ],
      "metadata": {
        "id": "g8mBkfCKM1qO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download VideoMAE code\n",
        "\n",
        "%cd /content\n",
        "!git clone https://github.com/MCG-NJU/VideoMAE.git"
      ],
      "metadata": {
        "id": "YOQwHpOpM4Hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download VideoMAE checkpoint\n",
        "\n",
        "%cd /content/VideoMAE\n",
        "\n",
        "import gdown\n",
        "\n",
        "# file_id = \"1AJQR1Rsi2N1pDn9tLyJ8DQrUREiBA1bO\"\n",
        "# gdown.download(f\"https://drive.google.com/uc?id={file_id}\", \"pretrain_videomae_huge_patch16_224.pth\", quiet=False)\n",
        "file_id = \"1qLOXWb_MGEvaI7tvuAe94CV7S2HXRwT3\"\n",
        "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", \"pretrain_videomae_large_patch16_224.pth\", quiet=False)"
      ],
      "metadata": {
        "id": "kXCTTIY1M8_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load VideoMAE model\n",
        "\n",
        "%cd /content/VideoMAE\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint as checkpoint\n",
        "from functools import partial\n",
        "from timm.models.layers import drop_path, to_2tuple, trunc_normal_\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "  def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "    super().__init__()\n",
        "    out_features = out_features or in_features\n",
        "    hidden_features = hidden_features or in_features\n",
        "    self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "    self.act = act_layer()\n",
        "    self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "    self.drop = nn.Dropout(drop)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc1(x)\n",
        "    x = self.act(x)\n",
        "    # x = self.drop(x)\n",
        "    # commit this for the orignal BERT implement\n",
        "    x = self.fc2(x)\n",
        "    x = self.drop(x)\n",
        "    return x\n",
        "\n",
        "class Attention(nn.Module):\n",
        "  def __init__(\n",
        "      self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.,\n",
        "      proj_drop=0., attn_head_dim=None):\n",
        "    super().__init__()\n",
        "    self.num_heads = num_heads\n",
        "    head_dim = dim // num_heads\n",
        "    if attn_head_dim is not None:\n",
        "      head_dim = attn_head_dim\n",
        "    all_head_dim = head_dim * self.num_heads\n",
        "    self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "    self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n",
        "    if qkv_bias:\n",
        "      self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
        "      self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
        "    else:\n",
        "      self.q_bias = None\n",
        "      self.v_bias = None\n",
        "\n",
        "    self.attn_drop = nn.Dropout(attn_drop)\n",
        "    self.proj = nn.Linear(all_head_dim, dim)\n",
        "    self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, N, C = x.shape\n",
        "    qkv_bias = None\n",
        "    if self.q_bias is not None:\n",
        "      qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n",
        "    # qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "    qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n",
        "    qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
        "    q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
        "\n",
        "    q = q * self.scale\n",
        "    attn = (q @ k.transpose(-2, -1))\n",
        "\n",
        "\n",
        "    attn = attn.softmax(dim=-1)\n",
        "    attn = self.attn_drop(attn)\n",
        "\n",
        "    x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n",
        "    x = self.proj(x)\n",
        "    x = self.proj_drop(x)\n",
        "    return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "  def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "               drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n",
        "               attn_head_dim=None):\n",
        "    super().__init__()\n",
        "    self.norm1 = norm_layer(dim)\n",
        "    self.attn = Attention(\n",
        "      dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "      attn_drop=attn_drop, proj_drop=drop, attn_head_dim=attn_head_dim)\n",
        "    # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
        "    self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "    self.norm2 = norm_layer(dim)\n",
        "    mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "    self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    if init_values > 0:\n",
        "      self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
        "      self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
        "    else:\n",
        "      self.gamma_1, self.gamma_2 = None, None\n",
        "\n",
        "  def forward(self, x):\n",
        "    if self.gamma_1 is None:\n",
        "      x = x + self.drop_path(self.attn(self.norm1(x)))\n",
        "      x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "    else:\n",
        "      x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x)))\n",
        "      x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n",
        "    return x\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "  \"\"\" Image to Patch Embedding\n",
        "  \"\"\"\n",
        "  def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, num_frames=16, tubelet_size=2):\n",
        "    super().__init__()\n",
        "    img_size = to_2tuple(img_size)\n",
        "    patch_size = to_2tuple(patch_size)\n",
        "    self.tubelet_size = int(tubelet_size)\n",
        "    num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0]) * (num_frames // self.tubelet_size)\n",
        "    self.img_size = img_size\n",
        "    self.patch_size = patch_size\n",
        "    self.num_patches = num_patches\n",
        "    self.proj = nn.Conv3d(in_channels=in_chans, out_channels=embed_dim,\n",
        "                          kernel_size = (self.tubelet_size,  patch_size[0],patch_size[1]),\n",
        "                          stride=(self.tubelet_size,  patch_size[0],  patch_size[1]))\n",
        "\n",
        "  def forward(self, x, **kwargs):\n",
        "    B, C, T, H, W = x.shape\n",
        "    # FIXME look at relaxing size constraints\n",
        "    # assert H == self.img_size[0] and W == self.img_size[1], \\\n",
        "    #     f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
        "    x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "    return x\n",
        "\n",
        "# sin-cos position encoding\n",
        "# https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/Models.py#L31\n",
        "def get_sinusoid_encoding_table(n_position, d_hid):\n",
        "  ''' Sinusoid position encoding table '''\n",
        "  # TODO: make it with torch instead of numpy\n",
        "  def get_position_angle_vec(position):\n",
        "    return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]\n",
        "\n",
        "  sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n",
        "  sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2]) # dim 2i\n",
        "  sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2]) # dim 2i+1\n",
        "\n",
        "  return  torch.tensor(sinusoid_table,dtype=torch.float, requires_grad=False).unsqueeze(0)\n",
        "\n",
        "class PretrainVisionTransformerEncoder(nn.Module):\n",
        "  \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
        "  \"\"\"\n",
        "  def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=0, embed_dim=768, depth=12,\n",
        "               num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
        "               drop_path_rate=0., norm_layer=nn.LayerNorm, init_values=None, tubelet_size=2, use_checkpoint=False,\n",
        "               use_learnable_pos_emb=False):\n",
        "    super().__init__()\n",
        "    self.num_classes = num_classes\n",
        "    self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
        "    self.patch_embed = PatchEmbed(\n",
        "      img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim, tubelet_size=tubelet_size)\n",
        "    num_patches = self.patch_embed.num_patches\n",
        "    self.embedding_shape = (8, 14, 14)  # Manually added for resizing position embedding\n",
        "    self.use_checkpoint = use_checkpoint\n",
        "\n",
        "    # TODO: Add the cls token\n",
        "    if use_learnable_pos_emb:\n",
        "      self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "    else:\n",
        "      # sine-cosine positional embeddings\n",
        "      self.pos_embed = get_sinusoid_encoding_table(num_patches, embed_dim)\n",
        "\n",
        "    dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "    self.blocks = nn.ModuleList([\n",
        "      Block(\n",
        "        dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "        drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n",
        "        init_values=init_values)\n",
        "      for i in range(depth)])\n",
        "    self.norm = norm_layer(embed_dim)\n",
        "    self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "  def interpolate_pos_encoding(self, x, h, w):\n",
        "    x = x.reshape(self.embedding_shape + (-1,))\n",
        "    dim = x.shape[-1]\n",
        "    x = F.interpolate(\n",
        "      x.permute(0, 3, 1, 2),\n",
        "      scale_factor=(h / self.embedding_shape[-2], w / self.embedding_shape[-1]),\n",
        "      mode=\"bicubic\",\n",
        "    )\n",
        "    x = x.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "    return x\n",
        "\n",
        "  def forward_features(self, x):\n",
        "    _, _, T, h, w = x.shape\n",
        "    h = h // self.patch_embed.patch_size[0]\n",
        "    w = w // self.patch_embed.patch_size[1]\n",
        "    x = self.patch_embed(x)\n",
        "\n",
        "    # x = x + self.pos_embed.type_as(x).to(x.device).clone().detach()\n",
        "    pos_embed = self.pos_embed.to(x.device)\n",
        "    x = x + self.interpolate_pos_encoding(pos_embed, h, w)\n",
        "\n",
        "    B, _, C = x.shape\n",
        "    x_vis = x.reshape(B, -1, C) # ~mask means visible\n",
        "\n",
        "    if self.use_checkpoint:\n",
        "      for blk in self.blocks:\n",
        "        x_vis = checkpoint.checkpoint(blk, x_vis)\n",
        "    else:\n",
        "      for blk in self.blocks:\n",
        "        x_vis = blk(x_vis)\n",
        "\n",
        "    x_vis = self.norm(x_vis)\n",
        "    return x_vis\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.forward_features(x)\n",
        "    # x = self.head(x)\n",
        "    return x\n",
        "\n",
        "class PretrainVisionTransformerDecoder(nn.Module):\n",
        "  \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
        "  \"\"\"\n",
        "  def __init__(self, patch_size=16, num_classes=768, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.,\n",
        "               qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.,\n",
        "               norm_layer=nn.LayerNorm, init_values=None, num_patches=196, tubelet_size=2, use_checkpoint=False\n",
        "               ):\n",
        "    super().__init__()\n",
        "    self.num_classes = num_classes\n",
        "    assert num_classes == 3 * tubelet_size * patch_size ** 2\n",
        "    self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
        "    self.patch_size = patch_size\n",
        "    self.use_checkpoint = use_checkpoint\n",
        "\n",
        "    dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "    self.blocks = nn.ModuleList([\n",
        "      Block(\n",
        "        dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "        drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n",
        "        init_values=init_values)\n",
        "      for i in range(depth)])\n",
        "    self.norm = norm_layer(embed_dim)\n",
        "    self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  def _init_weights(self, m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "      nn.init.xavier_uniform_(m.weight)\n",
        "      if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "    elif isinstance(m, nn.LayerNorm):\n",
        "      nn.init.constant_(m.bias, 0)\n",
        "      nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "  def get_num_layers(self):\n",
        "    return len(self.blocks)\n",
        "\n",
        "  @torch.jit.ignore\n",
        "  def no_weight_decay(self):\n",
        "    return {'pos_embed', 'cls_token'}\n",
        "\n",
        "  def get_classifier(self):\n",
        "    return self.head\n",
        "\n",
        "  def reset_classifier(self, num_classes, global_pool=''):\n",
        "    self.num_classes = num_classes\n",
        "    self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "  def forward(self, x, return_token_num):\n",
        "    if self.use_checkpoint:\n",
        "      for blk in self.blocks:\n",
        "        x = checkpoint.checkpoint(blk, x)\n",
        "    else:\n",
        "      for blk in self.blocks:\n",
        "        x = blk(x)\n",
        "\n",
        "    if return_token_num > 0:\n",
        "      x = self.head(self.norm(x[:, -return_token_num:])) # only return the mask tokens predict pixels\n",
        "    else:\n",
        "      x = self.head(self.norm(x))\n",
        "\n",
        "    return x\n",
        "\n",
        "class PretrainVisionTransformer(nn.Module):\n",
        "  \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               img_size=224,\n",
        "               patch_size=16,\n",
        "               encoder_in_chans=3,\n",
        "               encoder_num_classes=0,\n",
        "               encoder_embed_dim=768,\n",
        "               encoder_depth=12,\n",
        "               encoder_num_heads=12,\n",
        "               decoder_num_classes=1536, #  decoder_num_classes=768,\n",
        "               decoder_embed_dim=512,\n",
        "               decoder_depth=8,\n",
        "               decoder_num_heads=8,\n",
        "               mlp_ratio=4.,\n",
        "               qkv_bias=False,\n",
        "               qk_scale=None,\n",
        "               drop_rate=0.,\n",
        "               attn_drop_rate=0.,\n",
        "               drop_path_rate=0.,\n",
        "               norm_layer=nn.LayerNorm,\n",
        "               init_values=0.,\n",
        "               use_learnable_pos_emb=False,\n",
        "               use_checkpoint=False,\n",
        "               tubelet_size=2,\n",
        "               num_classes=0, # avoid the error from create_fn in timm\n",
        "               in_chans=0, # avoid the error from create_fn in timm\n",
        "               ):\n",
        "    super().__init__()\n",
        "    self.encoder = PretrainVisionTransformerEncoder(\n",
        "      img_size=img_size,\n",
        "      patch_size=patch_size,\n",
        "      in_chans=encoder_in_chans,\n",
        "      num_classes=encoder_num_classes,\n",
        "      embed_dim=encoder_embed_dim,\n",
        "      depth=encoder_depth,\n",
        "      num_heads=encoder_num_heads,\n",
        "      mlp_ratio=mlp_ratio,\n",
        "      qkv_bias=qkv_bias,\n",
        "      qk_scale=qk_scale,\n",
        "      drop_rate=drop_rate,\n",
        "      attn_drop_rate=attn_drop_rate,\n",
        "      drop_path_rate=drop_path_rate,\n",
        "      norm_layer=norm_layer,\n",
        "      init_values=init_values,\n",
        "      tubelet_size=tubelet_size,\n",
        "      use_checkpoint=use_checkpoint,\n",
        "      use_learnable_pos_emb=use_learnable_pos_emb)\n",
        "\n",
        "    # self.decoder = PretrainVisionTransformerDecoder(\n",
        "    #     patch_size=patch_size,\n",
        "    #     num_patches=self.encoder.patch_embed.num_patches,\n",
        "    #     num_classes=decoder_num_classes,\n",
        "    #     embed_dim=decoder_embed_dim,\n",
        "    #     depth=decoder_depth,\n",
        "    #     num_heads=decoder_num_heads,\n",
        "    #     mlp_ratio=mlp_ratio,\n",
        "    #     qkv_bias=qkv_bias,\n",
        "    #     qk_scale=qk_scale,\n",
        "    #     drop_rate=drop_rate,\n",
        "    #     attn_drop_rate=attn_drop_rate,\n",
        "    #     drop_path_rate=drop_path_rate,\n",
        "    #     norm_layer=norm_layer,\n",
        "    #     init_values=init_values,\n",
        "    #     tubelet_size=tubelet_size,\n",
        "    #     use_checkpoint=use_checkpoint)\n",
        "\n",
        "    # self.encoder_to_decoder = nn.Linear(encoder_embed_dim, decoder_embed_dim, bias=False)\n",
        "\n",
        "    self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
        "\n",
        "    self.pos_embed = get_sinusoid_encoding_table(self.encoder.patch_embed.num_patches, decoder_embed_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    _, _, T, _, _ = x.shape\n",
        "    x_vis = self.encoder(x) # [B, N_vis, C_e]\n",
        "    # x_vis = self.encoder_to_decoder(x_vis) # [B, N_vis, C_d]\n",
        "    # B, N, C = x_vis.shape\n",
        "    # we don't unshuffle the correct visible token order,\n",
        "    # but shuffle the pos embedding accorddingly.\n",
        "    # expand_pos_embed = self.pos_embed.expand(B, -1, -1).type_as(x).to(x.device).clone().detach()\n",
        "    # pos_emd_vis = expand_pos_embed[~mask].reshape(B, -1, C)\n",
        "    # pos_emd_mask = expand_pos_embed[mask].reshape(B, -1, C)\n",
        "    # x_full = torch.cat([x_vis + pos_emd_vis, self.mask_token + pos_emd_mask], dim=1) # [B, N, C_d]\n",
        "    # x = self.decoder(x_full, pos_emd_mask.shape[1]) # [B, N_mask, 3 * 16 * 16]\n",
        "\n",
        "    # return x\n",
        "    return x_vis\n",
        "\n",
        "def pretrain_videomae_small_patch16_224():\n",
        "  model = PretrainVisionTransformer(\n",
        "      img_size=224,\n",
        "      patch_size=16,\n",
        "      encoder_embed_dim=384,\n",
        "      encoder_depth=12,\n",
        "      encoder_num_heads=6,\n",
        "      encoder_num_classes=0,\n",
        "      decoder_num_classes=1536,\n",
        "      decoder_embed_dim=192,\n",
        "      decoder_num_heads=3,\n",
        "      mlp_ratio=4,\n",
        "      qkv_bias=True,\n",
        "      norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
        "  return model\n",
        "\n",
        "def pretrain_videomae_base_patch16_224():\n",
        "  model = PretrainVisionTransformer(\n",
        "      img_size=224,\n",
        "      patch_size=16,\n",
        "      encoder_embed_dim=768,\n",
        "      encoder_depth=12,\n",
        "      encoder_num_heads=12,\n",
        "      encoder_num_classes=0,\n",
        "      decoder_num_classes=1536,\n",
        "      decoder_embed_dim=384,\n",
        "      decoder_num_heads=6,\n",
        "      mlp_ratio=4,\n",
        "      qkv_bias=True,\n",
        "      norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
        "  return model\n",
        "\n",
        "def pretrain_videomae_large_patch16_224():\n",
        "  model = PretrainVisionTransformer(\n",
        "      img_size=224,\n",
        "      patch_size=16,\n",
        "      encoder_embed_dim=1024,\n",
        "      encoder_depth=24,\n",
        "      encoder_num_heads=16,\n",
        "      encoder_num_classes=0,\n",
        "      decoder_num_classes=1536,\n",
        "      decoder_embed_dim=512,\n",
        "      decoder_num_heads=8,\n",
        "      mlp_ratio=4,\n",
        "      qkv_bias=True,\n",
        "      norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
        "  return model\n",
        "\n",
        "def pretrain_videomae_huge_patch16_224():\n",
        "  model = PretrainVisionTransformer(\n",
        "      img_size=224,\n",
        "      patch_size=16,\n",
        "      encoder_embed_dim=1280,\n",
        "      encoder_depth=32,\n",
        "      encoder_num_heads=16,\n",
        "      encoder_num_classes=0,\n",
        "      decoder_num_classes=1536,\n",
        "      decoder_embed_dim=640,\n",
        "      decoder_num_heads=8,\n",
        "      mlp_ratio=4,\n",
        "      qkv_bias=True,\n",
        "      norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
        "  return model\n",
        "\n",
        "model = pretrain_videomae_large_patch16_224()\n",
        "checkpoint = torch.load('pretrain_videomae_large_patch16_224.pth', map_location='cpu')\n",
        "model.load_state_dict(checkpoint['model'], strict=False)\n",
        "# model = pretrain_videomae_huge_patch16_224()\n",
        "# checkpoint = torch.load('pretrain_videomae_huge_patch16_224.pth', map_location='cpu')\n",
        "# model.load_state_dict(checkpoint['model'])\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.eval()\n",
        "torch.set_grad_enabled(False)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "ytanbpA6QtVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Feature extraction function\n",
        "\n",
        "WINDOW_LENGTH = 16\n",
        "PATCH_SIZE = 16\n",
        "\n",
        "def extract_features(model, video):\n",
        "  video = video.astype(np.float32) / 255.0\n",
        "  video = (video - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
        "  h, w = video.shape[1] // PATCH_SIZE, video.shape[2] // PATCH_SIZE  # feature resolution\n",
        "\n",
        "  num_frames = video.shape[0]\n",
        "  if num_frames % WINDOW_LENGTH: # VideoMAE requires 16 frames per clip\n",
        "    video = np.pad(video, ((0, WINDOW_LENGTH - num_frames % WINDOW_LENGTH), (0, 0), (0, 0), (0, 0)))\n",
        "\n",
        "  torch.cuda.empty_cache()\n",
        "  video = torch.tensor(video).permute(3, 0, 1, 2).float().cuda()  # (T, 3, th, tw)\n",
        "  features = []\n",
        "  for t in range(0, video.shape[1], WINDOW_LENGTH):\n",
        "    feature = model(video[:, t : t + WINDOW_LENGTH][None])[0]  # (h * w + 1, c)\n",
        "    feature = feature.view(WINDOW_LENGTH // 2, h, w, -1)  # (h, w, c)\n",
        "    feature = F.interpolate(feature.permute(3, 0, 1, 2)[None], size=(WINDOW_LENGTH, h, w), mode='trilinear')\n",
        "    features.append(feature[0].permute(1, 2, 3, 0))\n",
        "  features = torch.concatenate(features, dim=0)  # (T, h, w, c)\n",
        "  features = features[:num_frames]\n",
        "  return features"
      ],
      "metadata": {
        "id": "l3kn5wUTQ4US"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title PCA visualization and k-means clustering {form-width: \"20%\"}\n",
        "\n",
        "for video in videos:\n",
        "  features = extract_features(model, video)\n",
        "  features = features.cpu().numpy()\n",
        "  print(features.shape)\n",
        "\n",
        "  pca_video = visualize_pca(features, video)\n",
        "  kmeans_video = visualize_kmeans(features, video)\n",
        "  mixed_video = 0.5 * video / 255.0 + 0.5 * kmeans_video\n",
        "  video_titles = ['pca', 'kmeans', 'mixed']\n",
        "  media.show_videos([pca_video, kmeans_video, mixed_video], titles=video_titles, height=128, codec='gif', fps=16, columns=3)"
      ],
      "metadata": {
        "id": "EY_2Qz_FS2Qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title DAVIS evaluation\n",
        "\n",
        "evaluate_davis(model, extract_features, dataset_path='/content/davis-2017/DAVIS/', height=480, width=880, patch_size=16)"
      ],
      "metadata": {
        "id": "lg6_vwyGZ1Jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title JHMDB evaluation\n",
        "\n",
        "evaluate_jhmdb(model, extract_features, dataset_path='/content/jhmdb/', height=320, width=320, patch_size=16)"
      ],
      "metadata": {
        "id": "FO_Mut3kaGeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title VIP evaluation\n",
        "\n",
        "evaluate_vip(model, extract_features, dataset_path='/content/VIP/', height=448, width=880, patch_size=16)"
      ],
      "metadata": {
        "id": "Ob1xojRAaJLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VideoMAE v2"
      ],
      "metadata": {
        "id": "U028FGpOT74K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load VideoMAE v2 model\n",
        "\n",
        "import transformers\n",
        "\n",
        "def get_sinusoid_encoding_table(n_position, d_hid):\n",
        "  def get_angle(pos):\n",
        "    return [pos / np.power(10000, 2 * (i // 2) / d_hid) for i in range(d_hid)]\n",
        "  table = np.array([get_angle(i) for i in range(n_position)])\n",
        "  table[:, 0::2], table[:, 1::2] = np.sin(table[:, 0::2]), np.cos(table[:, 1::2])\n",
        "  return torch.FloatTensor(table).unsqueeze(0)\n",
        "\n",
        "class VideoMAEv2Config(transformers.configuration_utils.PretrainedConfig):\n",
        "  model_type = 'VideoMAEv2_Base'\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "  def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU):\n",
        "    super().__init__()\n",
        "    out_features = out_features or in_features\n",
        "    hidden_features = hidden_features or in_features\n",
        "    self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "    self.act = act_layer()\n",
        "    self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.fc2(self.act(self.fc1(x)))\n",
        "\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., attn_head_dim=None):\n",
        "    super().__init__()\n",
        "    self.num_heads = num_heads\n",
        "    head_dim = attn_head_dim or dim // num_heads\n",
        "    all_head_dim = head_dim * self.num_heads\n",
        "    self.scale = qk_scale or head_dim**-0.5\n",
        "    self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n",
        "    if qkv_bias:\n",
        "      self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
        "      self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
        "    else:\n",
        "      self.q_bias = self.v_bias = None\n",
        "    self.attn_drop = nn.Dropout(attn_drop)\n",
        "    self.proj = nn.Linear(all_head_dim, dim)\n",
        "    self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, N, C = x.shape\n",
        "    qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias)) if self.q_bias is not None else None\n",
        "    qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias).reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
        "    q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "    attn = (q * self.scale) @ k.transpose(-2, -1)\n",
        "    attn = self.attn_drop(attn.softmax(dim=-1))\n",
        "    x = self.proj((attn @ v).transpose(1, 2).reshape(B, N, -1))\n",
        "    return self.proj_drop(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0., drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm, attn_head_dim=None, cos_attn=False):\n",
        "    super().__init__()\n",
        "    self.norm1, self.norm2 = norm_layer(dim), norm_layer(dim)\n",
        "    self.attn = Attention(dim, num_heads, qkv_bias, qk_scale, attn_drop, drop, attn_head_dim)\n",
        "    self.drop_path = nn.Identity()\n",
        "    self.mlp = Mlp(dim, int(dim * mlp_ratio), act_layer=act_layer)\n",
        "    if init_values > 0:\n",
        "      self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n",
        "      self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n",
        "    else:\n",
        "      self.gamma_1 = self.gamma_2 = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    if self.gamma_1 is None:\n",
        "      x = x + self.drop_path(self.attn(self.norm1(x)))\n",
        "      x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "    else:\n",
        "      x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x)))\n",
        "      x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n",
        "    return x\n",
        "\n",
        "def to_2tuple(x): return (x, x) if not isinstance(x, tuple) else x\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "  def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, num_frames=16, tubelet_size=2):\n",
        "    super().__init__()\n",
        "    img_size, patch_size = to_2tuple(img_size), to_2tuple(patch_size)\n",
        "    num_spatial = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])\n",
        "    num_patches = num_spatial * (num_frames // tubelet_size)\n",
        "    self.img_size, self.patch_size, self.num_patches, self.tubelet_size = img_size, patch_size, num_patches, tubelet_size\n",
        "    self.proj = nn.Conv3d(in_chans, embed_dim, (tubelet_size, patch_size[0], patch_size[1]), (tubelet_size, patch_size[0], patch_size[1]))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, C, T, H, W = x.shape\n",
        "    return self.proj(x).flatten(2).transpose(1, 2)\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "  def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0., head_drop_rate=0., norm_layer=nn.LayerNorm, layer_norm_eps=1e-12, init_values=0., use_learnable_pos_emb=False, init_scale=0., num_frames=16, tubelet_size=2, use_mean_pooling=True, with_cp=False, cos_attn=False):\n",
        "    super().__init__()\n",
        "    self.num_classes, self.num_features, self.embed_dim, self.tubelet_size = num_classes, embed_dim, embed_dim, tubelet_size\n",
        "    self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim, num_frames, tubelet_size)\n",
        "    self.patch_size = patch_size\n",
        "    num_patches = self.patch_embed.num_patches\n",
        "    self.embedding_shape = (8, 14, 14)\n",
        "    self.with_cp = with_cp\n",
        "    norm_layer = functools.partial(eval(norm_layer), eps=layer_norm_eps)\n",
        "    self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim)) if use_learnable_pos_emb else get_sinusoid_encoding_table(num_patches, embed_dim)\n",
        "    self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "    dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
        "    self.blocks = nn.ModuleList([Block(embed_dim, num_heads, mlp_ratio, qkv_bias, qk_scale, drop_rate, attn_drop_rate, dpr[i], norm_layer=norm_layer, init_values=init_values, cos_attn=cos_attn) for i in range(depth)])\n",
        "    self.norm = nn.Identity() if use_mean_pooling else norm_layer(embed_dim)\n",
        "    self.fc_norm = norm_layer(embed_dim) if use_mean_pooling else None\n",
        "    self.head_dropout = nn.Dropout(head_drop_rate)\n",
        "    self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "    if use_learnable_pos_emb:\n",
        "      nn.init.trunc_normal_(self.pos_embed, std=.02)\n",
        "\n",
        "  def interpolate_pos_encoding(self, x, h, w):\n",
        "    x = x.reshape(self.embedding_shape + (-1,))\n",
        "    dim = x.shape[-1]\n",
        "    x = F.interpolate(\n",
        "        x.permute(0, 3, 1, 2),\n",
        "        scale_factor=(h / self.embedding_shape[-2], w / self.embedding_shape[-1]),\n",
        "        mode=\"bicubic\",\n",
        "    )\n",
        "    x = x.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "    return x\n",
        "\n",
        "  def forward(self, x):\n",
        "    B = x.size(0)\n",
        "    _, _, _, h, w = x.shape\n",
        "    x = self.patch_embed(x)\n",
        "    # x = x + self.pos_embed.expand(B, -1, -1).type_as(x).to(x.device).clone().detach()\n",
        "    pos_embed = self.pos_embed.type_as(x).to(x.device).clone().detach()\n",
        "    h = h // self.patch_size\n",
        "    w = w // self.patch_size\n",
        "    x = x + self.interpolate_pos_encoding(pos_embed, h, w)\n",
        "    x = self.pos_drop(x)\n",
        "    for blk in self.blocks:\n",
        "      x = blk(x)\n",
        "    return self.fc_norm(x)\n",
        "\n",
        "\n",
        "class VideoMAEv2(transformers.PreTrainedModel):\n",
        "  config_class = VideoMAEv2Config\n",
        "  def __init__(self, config=None):\n",
        "    super().__init__(config=config)\n",
        "    self.model_config = config.model_config\n",
        "    self.model = VisionTransformer(**self.model_config)\n",
        "\n",
        "  def forward(self, video):\n",
        "    return self.model(video)\n",
        "\n",
        "model = VideoMAEv2.from_pretrained('OpenGVLab/VideoMAEv2-Large')\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.eval()\n",
        "torch.set_grad_enabled(False)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "0w9FX2enT90p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Feature extraction function\n",
        "\n",
        "WINDOW_LENGTH = 16\n",
        "PATCH_SIZE = 16\n",
        "\n",
        "def extract_features(model, video):\n",
        "  video = video.astype(np.float32) / 255.0\n",
        "  video = (video - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
        "  h, w = video.shape[1] // PATCH_SIZE, video.shape[2] // PATCH_SIZE  # feature resolution\n",
        "\n",
        "  num_frames = video.shape[0]\n",
        "  if num_frames % WINDOW_LENGTH: # VideoMAE requires 16 frames per clip\n",
        "    video = np.pad(video, ((0, WINDOW_LENGTH - num_frames % WINDOW_LENGTH), (0, 0), (0, 0), (0, 0)))\n",
        "\n",
        "  torch.cuda.empty_cache()\n",
        "  video = torch.tensor(video).permute(3, 0, 1, 2).float().cuda()  # (T, 3, th, tw)\n",
        "  features = []\n",
        "  for t in range(0, video.shape[1], WINDOW_LENGTH):\n",
        "    feature = model(video[:, t : t + WINDOW_LENGTH][None])[0]  # (h * w + 1, c)\n",
        "    feature = feature.view(WINDOW_LENGTH // 2, h, w, -1)  # (h, w, c)\n",
        "    feature = F.interpolate(feature.permute(3, 0, 1, 2)[None], size=(WINDOW_LENGTH, h, w), mode='trilinear')\n",
        "    features.append(feature[0].permute(1, 2, 3, 0))\n",
        "  features = torch.concatenate(features, dim=0)  # (T, h, w, c)\n",
        "  features = features[:num_frames]\n",
        "  return features"
      ],
      "metadata": {
        "id": "mD0NfmGRUl1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title PCA visualization and k-means clustering {form-width: \"20%\"}\n",
        "\n",
        "for video in videos:\n",
        "  features = extract_features(model, video)\n",
        "  features = features.cpu().numpy()\n",
        "  print(features.shape)\n",
        "\n",
        "  pca_video = visualize_pca(features, video)\n",
        "  kmeans_video = visualize_kmeans(features, video)\n",
        "  mixed_video = 0.5 * video / 255.0 + 0.5 * kmeans_video\n",
        "  video_titles = ['pca', 'kmeans', 'mixed']\n",
        "  media.show_videos([pca_video, kmeans_video, mixed_video], titles=video_titles, height=128, codec='gif', fps=16, columns=3)"
      ],
      "metadata": {
        "id": "bQ0p_nuWUpNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title DAVIS evaluation\n",
        "\n",
        "evaluate_davis(model, extract_features, dataset_path='/content/davis-2017/DAVIS/', height=480, width=880, patch_size=16)"
      ],
      "metadata": {
        "id": "aLLuOJsroR5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title JHMDB evaluation\n",
        "\n",
        "evaluate_jhmdb(model, extract_features, dataset_path='/content/jhmdb/', height=320, width=320, patch_size=16)"
      ],
      "metadata": {
        "id": "opPizCgBofJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title VIP evaluation\n",
        "\n",
        "evaluate_vip(model, extract_features, dataset_path='/content/VIP/', height=448, width=880, patch_size=16)"
      ],
      "metadata": {
        "id": "Wlc0jgHfoj2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73nrIX6XphZp"
      },
      "source": [
        "### V-JEPA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjycvMnExgRX"
      },
      "outputs": [],
      "source": [
        "# @title Download V-JEPA code\n",
        "%cd /content\n",
        "!git clone https://github.com/facebookresearch/jepa.git\n",
        "# !pip install -e jepa\n",
        "# !pip install timm einops torchcodec torchvision torch==2.6.0\n",
        "%cd /content/jepa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mPZBi9-z49q"
      },
      "outputs": [],
      "source": [
        "# @title Download V-JEPA checkpoints\n",
        "!mkdir checkpoints\n",
        "!wget -P checkpoints https://dl.fbaipublicfiles.com/jepa/vitl16/vitl16.pth.tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9OkVM_Lz6O3"
      },
      "outputs": [],
      "source": [
        "# @title Load V-JEPA encoder\n",
        "\n",
        "%cd /content/jepa\n",
        "\n",
        "import src.models.vision_transformer as vit\n",
        "\n",
        "model = vit.VisionTransformer(\n",
        "    patch_size=16, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,\n",
        "    norm_layer=functools.partial(nn.LayerNorm, eps=1e-6), img_size=224, num_frames=16, tubelet_size=2,\n",
        "    uniform_power=False, use_sdpa=False, use_SiLU=False, tight_SiLU=True)\n",
        "\n",
        "checkpoint = torch.load('checkpoints/vitl16.pth.tar', map_location='cpu')\n",
        "pretrained_dict = checkpoint['target_encoder']\n",
        "pretrained_dict = {k.replace('module.', ''): v for k, v in pretrained_dict.items()}\n",
        "pretrained_dict = {k.replace('backbone.', ''): v for k, v in pretrained_dict.items()}\n",
        "model.load_state_dict(pretrained_dict, strict=False)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.eval()\n",
        "torch.set_grad_enabled(False)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "es-L3n8jlBxO"
      },
      "outputs": [],
      "source": [
        "# @title Load V-JEPA encoder\n",
        "\n",
        "%cd /content/jepa\n",
        "\n",
        "from src.models.utils.patch_embed import PatchEmbed3D\n",
        "from src.models.utils.modules import Block\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "\n",
        "  def __init__(self, img_size=224, patch_size=16, num_frames=16, tubelet_size=2, in_chans=3,\n",
        "              embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None,\n",
        "              norm_layer=nn.LayerNorm):\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_size = img_size\n",
        "    self.patch_size = patch_size\n",
        "\n",
        "    self.num_frames = num_frames\n",
        "    self.tubelet_size = tubelet_size\n",
        "    grid_size, grid_depth = img_size // patch_size, num_frames // tubelet_size\n",
        "\n",
        "    self.patch_embed = PatchEmbed3D(patch_size=patch_size, tubelet_size=tubelet_size,\n",
        "                                    in_chans=in_chans, embed_dim=embed_dim)\n",
        "    self.num_patches = grid_depth * grid_size * grid_size\n",
        "    self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches, embed_dim), requires_grad=False)\n",
        "\n",
        "    self.blocks = nn.ModuleList([\n",
        "        Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n",
        "              qk_scale=qk_scale, drop=0.0, attn_drop=0.0, norm_layer=norm_layer,\n",
        "              act_layer=nn.GELU, grid_size=grid_size, grid_depth=grid_depth)\n",
        "        for _ in range(depth)\n",
        "    ])\n",
        "    self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.patch_embed(x) + self.interpolate_pos_encoding(x, self.pos_embed)\n",
        "    for _, block in enumerate(self.blocks):\n",
        "      x = block(x, mask=None)\n",
        "    x = self.norm(x)\n",
        "    return x\n",
        "\n",
        "  def interpolate_pos_encoding(self, x, pos_embed):\n",
        "    _, N, dim = pos_embed.shape\n",
        "    _, _, T, H, W = x.shape\n",
        "    if H == self.input_size and W == self.input_size and T == self.num_frames:\n",
        "      return pos_embed\n",
        "\n",
        "    T, H, W = T // self.tubelet_size, H // self.patch_size, W // self.patch_size\n",
        "    N_t = self.num_frames // self.tubelet_size\n",
        "    N_h = N_w = self.input_size // self.patch_size\n",
        "\n",
        "    pos_embed = F.interpolate(\n",
        "        pos_embed.reshape(1, N_t, N_h, N_w, dim).permute(0, 4, 1, 2, 3),\n",
        "        scale_factor=(T/N_t, H/N_h, W/N_w),\n",
        "        mode='trilinear')\n",
        "    pos_embed = pos_embed.permute(0, 2, 3, 4, 1).view(1, -1, dim)\n",
        "    return pos_embed\n",
        "\n",
        "model = VisionTransformer(\n",
        "    patch_size=16, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,\n",
        "    norm_layer=functools.partial(nn.LayerNorm, eps=1e-6), img_size=224, num_frames=16, tubelet_size=2)\n",
        "\n",
        "checkpoint = torch.load('checkpoints/vitl16.pth.tar', map_location='cpu')\n",
        "pretrained_dict = checkpoint['target_encoder']\n",
        "pretrained_dict = {k.replace('module.', ''): v for k, v in pretrained_dict.items()}\n",
        "pretrained_dict = {k.replace('backbone.', ''): v for k, v in pretrained_dict.items()}\n",
        "model.load_state_dict(pretrained_dict, strict=False)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.eval()\n",
        "torch.set_grad_enabled(False)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXhH26cmgDrj"
      },
      "outputs": [],
      "source": [
        "# @title Feature extraction function\n",
        "\n",
        "WINDOW_LENGTH = 16\n",
        "PATCH_SIZE = 16\n",
        "\n",
        "def extract_features(model, video):\n",
        "  video = video.astype(np.float32) / 255.0\n",
        "  video = (video - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
        "  h, w = video.shape[1] // PATCH_SIZE, video.shape[2] // PATCH_SIZE  # feature resolution\n",
        "\n",
        "  num_frames = video.shape[0]\n",
        "  if num_frames % WINDOW_LENGTH: # VideoMAE requires 16 frames per clip\n",
        "    video = np.pad(video, ((0, WINDOW_LENGTH - num_frames % WINDOW_LENGTH), (0, 0), (0, 0), (0, 0)))\n",
        "\n",
        "  torch.cuda.empty_cache()\n",
        "  video = torch.tensor(video).permute(3, 0, 1, 2).float().cuda()  # (3, T, th, tw)\n",
        "  features = []\n",
        "  for t in range(0, video.shape[1], WINDOW_LENGTH):\n",
        "    feature = model(video[:, t : t + WINDOW_LENGTH][None])[0]  # (h * w + 1, c)\n",
        "    feature = feature.view(WINDOW_LENGTH // 2, h, w, -1)  # (h, w, c)\n",
        "    feature = F.interpolate(feature.permute(3, 0, 1, 2)[None], size=(WINDOW_LENGTH, h, w), mode='trilinear')\n",
        "    features.append(feature[0].permute(1, 2, 3, 0))\n",
        "  features = torch.concatenate(features, dim=0)  # (T, h, w, c)\n",
        "  features = features[:num_frames]\n",
        "  return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EFiQPaG7Vew"
      },
      "outputs": [],
      "source": [
        "# @title PCA visualization and k-means clustering {form-width: \"20%\"}\n",
        "\n",
        "for video in videos:\n",
        "  features = extract_features(model, video)\n",
        "  features = features.cpu().numpy()\n",
        "  print(features.shape)\n",
        "\n",
        "  pca_video = visualize_pca(features, video)\n",
        "  kmeans_video = visualize_kmeans(features, video)\n",
        "  mixed_video = 0.5 * video / 255.0 + 0.5 * kmeans_video\n",
        "  video_titles = ['pca', 'kmeans', 'mixed']\n",
        "  media.show_videos([pca_video, kmeans_video, mixed_video], titles=video_titles, height=128, codec='gif', fps=16, columns=3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title DAVIS evaluation\n",
        "\n",
        "evaluate_davis(model, extract_features, dataset_path='/content/davis-2017/DAVIS/', height=480, width=880, patch_size=16)"
      ],
      "metadata": {
        "id": "CAQPTw-TIpAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title JHMDB evaluation\n",
        "\n",
        "evaluate_jhmdb(model, extract_features, dataset_path='/content/jhmdb/', height=320, width=320, patch_size=16)"
      ],
      "metadata": {
        "id": "h6hnI270IrHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title VIP evaluation\n",
        "\n",
        "evaluate_vip(model, extract_features, dataset_path='/content/VIP/', height=448, width=880, patch_size=16)"
      ],
      "metadata": {
        "id": "Kv1aalpkIsw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFUEDNhSKiIt"
      },
      "source": [
        "### V-JEPA 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5k91TyxoA6i"
      },
      "outputs": [],
      "source": [
        "# @title Load V-JEPA 2 model\n",
        "\n",
        "import transformers\n",
        "\n",
        "class VJEPA2Model(transformers.VJEPA2PreTrainedModel):\n",
        "  def __init__(self, config):\n",
        "    super().__init__(config)\n",
        "    self.config = config\n",
        "    self.encoder = transformers.models.vjepa2.modeling_vjepa2.VJEPA2Encoder(config)\n",
        "    self.predictor = transformers.models.vjepa2.modeling_vjepa2.VJEPA2Predictor(config)\n",
        "\n",
        "  def forward(self, video):\n",
        "    encoder_outputs = self.encoder(\n",
        "        pixel_values_videos=video,\n",
        "        head_mask=None,\n",
        "    )\n",
        "    sequence_output = encoder_outputs.last_hidden_state\n",
        "\n",
        "    return sequence_output\n",
        "\n",
        "model = VJEPA2Model.from_pretrained(\"facebook/vjepa2-vitl-fpc64-256\").to(\"cuda\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.eval()\n",
        "torch.set_grad_enabled(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTQGb7bcwTDW"
      },
      "outputs": [],
      "source": [
        "# @title Load V-JEPA 2 encoder\n",
        "\n",
        "from transformers.models.vjepa2.modeling_vjepa2 import VJEPA2MLP\n",
        "from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS\n",
        "\n",
        "class VJEPA2PatchEmbeddings3D(nn.Module):\n",
        "  def __init__(self, config, hidden_size: int = 1024):\n",
        "    super().__init__()\n",
        "    self.patch_size = config.patch_size\n",
        "    self.tubelet_size = config.tubelet_size\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    self.proj = nn.Conv3d(\n",
        "        in_channels=config.in_chans,\n",
        "        out_channels=hidden_size,\n",
        "        kernel_size=(config.tubelet_size, config.patch_size, config.patch_size),\n",
        "        stride=(config.tubelet_size, config.patch_size, config.patch_size),\n",
        "    )\n",
        "\n",
        "  def forward(self, video):\n",
        "    x = self.proj(video)\n",
        "    num_frames, height, width = x.shape[-3:]\n",
        "    x = x.flatten(2).transpose(1, 2)\n",
        "    return x, num_frames, height, width\n",
        "\n",
        "class VJEPA2Embeddings(nn.Module):\n",
        "  def __init__(self, config, hidden_size: int = 1024):\n",
        "    super().__init__()\n",
        "    self.patch_embeddings = VJEPA2PatchEmbeddings3D(config, hidden_size=hidden_size)\n",
        "\n",
        "  def forward(self, video):\n",
        "    video = video.permute(0, 2, 1, 3, 4)\n",
        "    embeddings, num_frames, height, width = self.patch_embeddings(video)\n",
        "    return embeddings, num_frames, height, width\n",
        "\n",
        "def eager_attention_forward(module, query, key, value, attention_mask, scaling):\n",
        "  attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n",
        "  attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
        "  if attention_mask is not None:\n",
        "    attn_weights = attn_weights * attention_mask\n",
        "  attn_output = torch.matmul(attn_weights, value)\n",
        "  attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "  return attn_output, attn_weights\n",
        "\n",
        "def rotate_queries_or_keys(x, pos):\n",
        "  B, num_heads, N, D = x.size()\n",
        "  omega = torch.arange(D // 2, dtype=x.dtype, device=x.device)\n",
        "  omega /= D / 2.0\n",
        "  omega = 1.0 / 10000**omega  # (D/2,)\n",
        "  freq = pos.unsqueeze(-1) * omega  # (..., N, D/2), outer product\n",
        "  emb_sin = freq.sin()  # (..., N, D/2)\n",
        "  emb_cos = freq.cos()  # (..., N, D/2)\n",
        "  emb_sin = emb_sin.squeeze(-1).repeat(1, 1, 1, 2)\n",
        "  emb_cos = emb_cos.squeeze(-1).repeat(1, 1, 1, 2)\n",
        "  y = x.unflatten(-1, (-1, 2))\n",
        "  y1, y2 = y.unbind(dim=-1)\n",
        "  y = torch.stack((-y2, y1), dim=-1)\n",
        "  y = y.flatten(-2)\n",
        "  return (x * emb_cos) + (y * emb_sin)\n",
        "\n",
        "class VJEPA2RopeAttention(nn.Module):\n",
        "  def __init__(self, config, hidden_size=1024, num_attention_heads=16):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_attention_heads = num_attention_heads\n",
        "    self.attention_head_size = hidden_size // num_attention_heads\n",
        "\n",
        "    self.query = nn.Linear(hidden_size, hidden_size, bias=config.qkv_bias)\n",
        "    self.key = nn.Linear(hidden_size, hidden_size, bias=config.qkv_bias)\n",
        "    self.value = nn.Linear(hidden_size, hidden_size, bias=config.qkv_bias)\n",
        "    self.proj = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    self.crop_size = self.config.crop_size\n",
        "    self.grid_size = self.config.crop_size // self.config.patch_size\n",
        "\n",
        "    self.d_dim = int(2 * ((self.attention_head_size // 3) // 2))\n",
        "    self.h_dim = int(2 * ((self.attention_head_size // 3) // 2))\n",
        "    self.w_dim = int(2 * ((self.attention_head_size // 3) // 2))\n",
        "\n",
        "    self.scaling = self.attention_head_size**-0.5\n",
        "    self.is_causal = False\n",
        "\n",
        "  def _get_frame_pos(self, ids):\n",
        "    tokens_per_frame = self.grid_size ** 2\n",
        "    return ids // tokens_per_frame\n",
        "\n",
        "  def _get_height_pos(self, ids):\n",
        "    return (ids % (self.grid_size ** 2)) // self.grid_size\n",
        "\n",
        "  def get_position_ids(self, x, height, width):\n",
        "    device = x.device\n",
        "    token_size = x.size(1)\n",
        "    num_frames = token_size // height // width\n",
        "    ids = torch.arange(token_size, device=device)\n",
        "    # Original position ids\n",
        "    # frame_ids = self._get_frame_pos(ids)\n",
        "    # height_ids = self._get_height_pos(ids)\n",
        "    # width_ids = ids - self.grid_size ** 2 * frame_ids - self.grid_size * height_ids\n",
        "\n",
        "    # Interpolated position ids\n",
        "    frame_ids = ids // (height * width)\n",
        "    iy = torch.arange(self.grid_size, device=device, dtype=torch.float32)\n",
        "    ix = torch.arange(self.grid_size, device=device, dtype=torch.float32)\n",
        "    iy, ix = torch.meshgrid(iy, ix, indexing='ij')\n",
        "    height_ids = F.interpolate(iy[None, None], size=(height, width), mode='bilinear')[0, 0]\n",
        "    width_ids = F.interpolate(ix[None, None], size=(height, width), mode='bilinear')[0, 0]\n",
        "    height_ids = height_ids.flatten()\n",
        "    width_ids = width_ids.flatten()\n",
        "    height_ids = height_ids.repeat(num_frames)\n",
        "    width_ids = width_ids.repeat(num_frames)\n",
        "    return frame_ids, height_ids, width_ids\n",
        "\n",
        "  def apply_rotary_embeddings(self, qk, pos_ids):\n",
        "    d, h, w = pos_ids\n",
        "    s = 0\n",
        "    qkd = rotate_queries_or_keys(qk[..., s : s + self.d_dim], pos=d);\n",
        "    s += self.d_dim\n",
        "    qkh = rotate_queries_or_keys(qk[..., s : s + self.h_dim], pos=h);\n",
        "    s += self.h_dim\n",
        "    qkw = rotate_queries_or_keys(qk[..., s : s + self.w_dim], pos=w);\n",
        "    s += self.w_dim\n",
        "    if s < self.attention_head_size:\n",
        "      qkr = qk[..., s:]\n",
        "      qk = torch.cat([qkd, qkh, qkw, qkr], dim=-1)\n",
        "    else:\n",
        "      qk = torch.cat([qkd, qkh, qkw], dim=-1)\n",
        "    return qk\n",
        "\n",
        "  def forward(self, hidden_states, height, width, head_mask=None):\n",
        "    batch_size, seq_length, _ = hidden_states.shape\n",
        "    query = self.query(hidden_states).view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
        "    key = self.key(hidden_states).view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
        "    value = self.value(hidden_states).view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
        "\n",
        "    pos_ids = self.get_position_ids(hidden_states, height, width)\n",
        "    key = self.apply_rotary_embeddings(key, pos_ids)\n",
        "    query = self.apply_rotary_embeddings(query, pos_ids)\n",
        "\n",
        "    attn_fn = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n",
        "    context, _ = attn_fn(self, query, key, value, head_mask, is_causal=self.is_causal, scaling=self.scaling, dropout=0.0)\n",
        "    context = context.reshape(context.size()[:-2] + (self.hidden_size,))\n",
        "    outputs = self.proj(context)\n",
        "    return outputs\n",
        "\n",
        "class VJEPA2Layer(nn.Module):\n",
        "  def __init__(self, config, hidden_size=1024, num_attention_heads=16, mlp_ratio=4.0):\n",
        "    super().__init__()\n",
        "    self.norm1 = nn.LayerNorm(hidden_size, eps=config.layer_norm_eps)\n",
        "    self.attention = VJEPA2RopeAttention(config, hidden_size, num_attention_heads)\n",
        "    self.norm2 = nn.LayerNorm(hidden_size, eps=config.layer_norm_eps)\n",
        "    self.mlp = VJEPA2MLP(config, hidden_size=hidden_size, mlp_ratio=mlp_ratio)\n",
        "\n",
        "  def forward(self, hidden_states, height, width):\n",
        "    x = self.norm1(hidden_states)\n",
        "    attn_out = self.attention(x, height, width, head_mask=None)\n",
        "    x = attn_out + hidden_states\n",
        "    x_res = self.norm2(x)\n",
        "    x = self.mlp(x_res) + x\n",
        "    return x\n",
        "\n",
        "class VJEPA2Encoder(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.embeddings = VJEPA2Embeddings(config, hidden_size=config.hidden_size)\n",
        "    self.layer = nn.ModuleList([\n",
        "        VJEPA2Layer(config, config.hidden_size, config.num_attention_heads, config.mlp_ratio)\n",
        "        for _ in range(config.num_hidden_layers)\n",
        "    ])\n",
        "    self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "  def forward(self, video):\n",
        "    x, num_frames, height, width = self.embeddings(video)\n",
        "    for layer in self.layer:\n",
        "      x = layer(x, height, width)\n",
        "    x = self.layernorm(x)\n",
        "    return x\n",
        "\n",
        "class VJEPA2Model(transformers.VJEPA2PreTrainedModel):\n",
        "  def __init__(self, config):\n",
        "    super().__init__(config)\n",
        "    self.config = config\n",
        "    print(config)\n",
        "    self.encoder = VJEPA2Encoder(config)\n",
        "    self.predictor = transformers.models.vjepa2.modeling_vjepa2.VJEPA2Predictor(config)\n",
        "\n",
        "  def forward(self, video):\n",
        "    encoder_outputs = self.encoder(video)\n",
        "    return encoder_outputs\n",
        "\n",
        "model = VJEPA2Model.from_pretrained(\"facebook/vjepa2-vitl-fpc64-256\").to(\"cuda\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.eval()\n",
        "torch.set_grad_enabled(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XhbFvrfq0LU"
      },
      "outputs": [],
      "source": [
        "# @title Feature extraction function\n",
        "\n",
        "def extract_features_whole(model, video, h, w):\n",
        "  video = video.astype(np.float32) / 255.0\n",
        "  video = (video - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
        "  h, w = video.shape[1] // PATCH_SIZE, video.shape[2] // PATCH_SIZE  # feature resolution\n",
        "\n",
        "  num_frames = video.shape[0]\n",
        "  if num_frames % 2: # V-JEPA 2 requires even frames per clip\n",
        "    video = np.pad(video, ((0, 2 - num_frames % 2), (0, 0), (0, 0), (0, 0)))\n",
        "\n",
        "  torch.cuda.empty_cache()\n",
        "  video = torch.tensor(video).permute(0, 3, 1, 2).float().cuda()  # (T, 3, th, tw)\n",
        "  features = model(video[None])[0]\n",
        "  features = features.reshape(video.shape[0] // 2, h, w, features.shape[-1])\n",
        "  features = F.interpolate(features.permute(3, 0, 1, 2)[None], size=(video.shape[0], h, w), mode='trilinear')\n",
        "  features = features[0].permute(1, 2, 3, 0)\n",
        "  features = features[:num_frames]\n",
        "  return features\n",
        "\n",
        "WINDOW_LENGTH = 16\n",
        "PATCH_SIZE = 16\n",
        "\n",
        "def extract_features(model, video):\n",
        "  video = video.astype(np.float32) / 255.0\n",
        "  video = (video - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
        "  h, w = video.shape[1] // PATCH_SIZE, video.shape[2] // PATCH_SIZE  # feature resolution\n",
        "\n",
        "  num_frames = video.shape[0]\n",
        "  if num_frames % WINDOW_LENGTH: # VideoMAE requires 16 frames per clip\n",
        "    video = np.pad(video, ((0, WINDOW_LENGTH - num_frames % WINDOW_LENGTH), (0, 0), (0, 0), (0, 0)))\n",
        "  h, w = video.shape[1] // PATCH_SIZE, video.shape[2] // PATCH_SIZE  # feature resolution\n",
        "\n",
        "  torch.cuda.empty_cache()\n",
        "  video = torch.tensor(video).permute(0, 3, 1, 2).float().cuda()  # (3, T, th, tw)\n",
        "  features = []\n",
        "  for t in range(0, video.shape[0], WINDOW_LENGTH):\n",
        "    feature = model(video[t : t + WINDOW_LENGTH][None])[0]  # (S * h * w, c)\n",
        "    feature = feature.view(WINDOW_LENGTH // 2, h, w, feature.shape[-1])  # (S, h, w, c)\n",
        "    feature = F.interpolate(feature.permute(3, 0, 1, 2)[None], size=(WINDOW_LENGTH, h, w), mode='trilinear')\n",
        "    features.append(feature[0].permute(1, 2, 3, 0))\n",
        "  features = torch.concatenate(features, dim=0)  # (T, h, w, c)\n",
        "  features = features[:num_frames]\n",
        "  return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgCUg4JTrsQF"
      },
      "outputs": [],
      "source": [
        "# @title PCA visualization and k-means clustering {form-width: \"20%\"}\n",
        "\n",
        "for video in videos:\n",
        "  features = extract_features(model, video)\n",
        "  features = features.cpu().numpy()\n",
        "  print(features.shape)\n",
        "\n",
        "  pca_video = visualize_pca(features, video)\n",
        "  kmeans_video = visualize_kmeans(features, video)\n",
        "  mixed_video = 0.5 * video / 255.0 + 0.5 * kmeans_video\n",
        "  video_titles = ['pca', 'kmeans', 'mixed']\n",
        "  media.show_videos([pca_video, kmeans_video, mixed_video], titles=video_titles, height=128, codec='gif', fps=16, columns=3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title DAVIS evaluation\n",
        "\n",
        "evaluate_davis(model, extract_features, dataset_path='/content/davis-2017/DAVIS/', height=480, width=880, patch_size=16)"
      ],
      "metadata": {
        "id": "cW512XruRCQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title JHMDB evaluation\n",
        "\n",
        "evaluate_jhmdb(model, extract_features, dataset_path='/content/jhmdb/', height=320, width=320, patch_size=16)"
      ],
      "metadata": {
        "id": "aQhWdJh4RD_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title VIP evaluation\n",
        "\n",
        "evaluate_vip(model, extract_features, dataset_path='/content/VIP/', height=448, width=880, patch_size=16)"
      ],
      "metadata": {
        "id": "JHuRpBOuRFUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlE0TwAkCpbF"
      },
      "source": [
        "### DINO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qO0Hi1buCpbF"
      },
      "outputs": [],
      "source": [
        "# @title Download DINO code\n",
        "\n",
        "%cd /content\n",
        "!git clone https://github.com/facebookresearch/dino.git\n",
        "%cd /content/dino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35WNpm0ICpbF"
      },
      "outputs": [],
      "source": [
        "# @title Load DINO model\n",
        "\n",
        "%cd /content/dino\n",
        "\n",
        "import vision_transformer as vits\n",
        "\n",
        "model = vits.__dict__['vit_base'](patch_size=16, num_classes=0)\n",
        "model.cuda()\n",
        "# url = \"dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\"\n",
        "# url = \"dino_deitsmall8_pretrain/dino_deitsmall8_pretrain.pth\"\n",
        "url = \"dino_vitbase16_pretrain/dino_vitbase16_pretrain.pth\"\n",
        "# url = \"dino_vitbase8_pretrain/dino_vitbase8_pretrain.pth\"\n",
        "state_dict = torch.hub.load_state_dict_from_url(url=\"https://dl.fbaipublicfiles.com/dino/\" + url)\n",
        "model.load_state_dict(state_dict, strict=True)\n",
        "torch.set_grad_enabled(False)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtf6RY0FCpbF"
      },
      "outputs": [],
      "source": [
        "# @title Feature extraction function\n",
        "\n",
        "PATCH_SIZE = 16\n",
        "\n",
        "def extract_features(model, video):\n",
        "  video = video.astype(np.float32) / 255.0\n",
        "  video = (video - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
        "  h, w = video.shape[1] // PATCH_SIZE, video.shape[2] // PATCH_SIZE  # feature resolution\n",
        "\n",
        "  torch.cuda.empty_cache()\n",
        "  video = torch.tensor(video).permute(0, 3, 1, 2).float().cuda()  # (T, 3, th, tw)\n",
        "  features = []\n",
        "  for t in range(video.shape[0]):\n",
        "    feature = model.get_intermediate_layers(video[t:t+1], n=1)[0]  # (h * w + 1, c)\n",
        "    feature = feature[0, 1:, :].view(h, w, -1)  # discard the [CLS] token, (h * w, c)\n",
        "    features.append(feature)\n",
        "  features = torch.stack(features, dim=0)  # (T, h, w, c)\n",
        "  return features"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title PCA visualization and k-means clustering {form-width: \"20%\"}\n",
        "\n",
        "for video in videos:\n",
        "  features = extract_features(model, video)\n",
        "  features = features.cpu().numpy()\n",
        "  print(features.shape)\n",
        "\n",
        "  pca_video = visualize_pca(features, video)\n",
        "  kmeans_video = visualize_kmeans(features, video)\n",
        "  mixed_video = 0.5 * video / 255.0 + 0.5 * kmeans_video\n",
        "  video_titles = ['pca', 'kmeans', 'mixed']\n",
        "  media.show_videos([pca_video, kmeans_video, mixed_video], titles=video_titles, height=128, codec='gif', fps=16, columns=3)"
      ],
      "metadata": {
        "id": "6FsHkJBsC4Bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title DAVIS evaluation\n",
        "\n",
        "evaluate_davis(model, extract_features, dataset_path='/content/davis-2017/DAVIS/', height=480, width=880, patch_size=16)"
      ],
      "metadata": {
        "id": "gXqS8o_FXtYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title JHMDB evaluation\n",
        "\n",
        "evaluate_jhmdb(model, extract_features, dataset_path='/content/jhmdb/', height=320, width=320, patch_size=16)"
      ],
      "metadata": {
        "id": "TpEi1daFXvWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title VIP evaluation\n",
        "\n",
        "evaluate_vip(model, extract_features, dataset_path='/content/VIP/', height=448, width=880, patch_size=16)"
      ],
      "metadata": {
        "id": "9Dt0AjmkXwk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DINO v2"
      ],
      "metadata": {
        "id": "jqXOdbfFBvLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load DINO v2 model\n",
        "\n",
        "import transformers\n",
        "from transformers.models.dinov2.modeling_dinov2 import Dinov2Embeddings, Dinov2Encoder\n",
        "\n",
        "class Dinov2Model(transformers.Dinov2PreTrainedModel):\n",
        "  def __init__(self, config):\n",
        "    super().__init__(config)\n",
        "    self.config = config\n",
        "    self.embeddings = Dinov2Embeddings(config)\n",
        "    self.encoder = Dinov2Encoder(config)\n",
        "    self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "  def forward(self, image):\n",
        "    embedding_output = self.embeddings(image, bool_masked_pos=None)\n",
        "    encoder_outputs = self.encoder(\n",
        "        embedding_output,\n",
        "        head_mask=None,\n",
        "    )\n",
        "    sequence_output = self.layernorm(encoder_outputs[0])\n",
        "    return sequence_output\n",
        "\n",
        "model = Dinov2Model.from_pretrained(\"facebook/dinov2-large\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.eval()\n",
        "torch.set_grad_enabled(False)"
      ],
      "metadata": {
        "id": "XWBInTp9B1Qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Feature extraction function\n",
        "\n",
        "PATCH_SIZE = 14\n",
        "\n",
        "def extract_features(model, video):\n",
        "  video = video.astype(np.float32) / 255.0\n",
        "  video = (video - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
        "  h, w = video.shape[1] // PATCH_SIZE, video.shape[2] // PATCH_SIZE  # feature resolution\n",
        "\n",
        "  torch.cuda.empty_cache()\n",
        "  video = torch.tensor(video).permute(0, 3, 1, 2).float().cuda()  # (T, 3, th, tw)\n",
        "  features = []\n",
        "  for t in range(video.shape[0]):\n",
        "    feature = model(video[t:t+1])[0]  # (h * w + 1, c)\n",
        "    feature = feature[1:, :].view(h, w, feature.shape[-1])  # discard the [CLS] token, (h * w, c)\n",
        "    features.append(feature)\n",
        "  features = torch.stack(features, dim=0)  # (T, h, w, c)\n",
        "  return features"
      ],
      "metadata": {
        "id": "JTAxXbzsCYqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title PCA visualization and k-means clustering {form-width: \"20%\"}\n",
        "\n",
        "for video in videos:\n",
        "  features = extract_features(model, video)\n",
        "  features = features.cpu().numpy()\n",
        "  print(features.shape)\n",
        "\n",
        "  pca_video = visualize_pca(features, video)\n",
        "  kmeans_video = visualize_kmeans(features, video)\n",
        "  mixed_video = 0.5 * video / 255.0 + 0.5 * kmeans_video\n",
        "  video_titles = ['pca', 'kmeans', 'mixed']\n",
        "  media.show_videos([pca_video, kmeans_video, mixed_video], titles=video_titles, height=128, codec='gif', fps=16, columns=3)"
      ],
      "metadata": {
        "id": "MNU7-FgICi1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title DAVIS evaluation\n",
        "\n",
        "evaluate_davis(model, extract_features, dataset_path='/content/davis-2017/DAVIS/', height=480, width=880, patch_size=14)"
      ],
      "metadata": {
        "id": "LJpUJc7PYFk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title JHMDB evaluation\n",
        "\n",
        "evaluate_jhmdb(model, extract_features, dataset_path='/content/jhmdb/', height=320, width=320, patch_size=14)"
      ],
      "metadata": {
        "id": "vlVMPBPvYGxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title VIP evaluation\n",
        "\n",
        "evaluate_vip(model, extract_features, dataset_path='/content/VIP/', height=448, width=880, patch_size=14)"
      ],
      "metadata": {
        "id": "Dy_C3EBoYJ-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DINO v2 with registers"
      ],
      "metadata": {
        "id": "7ekUPZXX1YhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load DINO v2 with registers model\n",
        "\n",
        "import transformers\n",
        "from transformers.models.dinov2_with_registers.modeling_dinov2_with_registers import Dinov2WithRegistersEmbeddings, Dinov2WithRegistersEncoder\n",
        "\n",
        "class Dinov2WithRegistersModel(transformers.Dinov2WithRegistersPreTrainedModel):\n",
        "  def __init__(self, config):\n",
        "    super().__init__(config)\n",
        "    self.config = config\n",
        "    self.embeddings = Dinov2WithRegistersEmbeddings(config)\n",
        "    self.encoder = Dinov2WithRegistersEncoder(config)\n",
        "    self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "  def forward(self, image):\n",
        "    embedding_output = self.embeddings(image, bool_masked_pos=None)\n",
        "    encoder_outputs = self.encoder(\n",
        "        embedding_output,\n",
        "        head_mask=None,\n",
        "    )\n",
        "    sequence_output = self.layernorm(encoder_outputs[0])\n",
        "    patch_tokens = sequence_output[:, 1 + self.config.num_register_tokens :]\n",
        "    return patch_tokens\n",
        "\n",
        "model = Dinov2WithRegistersModel.from_pretrained(\"facebook/dinov2-with-registers-large\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.eval()\n",
        "torch.set_grad_enabled(False)"
      ],
      "metadata": {
        "id": "OZeckyPL1bUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Feature extraction function\n",
        "\n",
        "PATCH_SIZE = 14\n",
        "\n",
        "def extract_features(model, video):\n",
        "  video = video.astype(np.float32) / 255.0\n",
        "  video = (video - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
        "  h, w = video.shape[1] // PATCH_SIZE, video.shape[2] // PATCH_SIZE  # feature resolution\n",
        "\n",
        "  torch.cuda.empty_cache()\n",
        "  video = torch.tensor(video).permute(0, 3, 1, 2).float().cuda()  # (T, 3, th, tw)\n",
        "  features = []\n",
        "  for t in range(video.shape[0]):\n",
        "    feature = model(video[t:t+1])[0]  # (h * w + 1, c)\n",
        "    feature = feature.view(h, w, feature.shape[-1])  # discard the [CLS] token, (h * w, c)\n",
        "    features.append(feature)\n",
        "  features = torch.stack(features, dim=0)  # (T, h, w, c)\n",
        "  return features"
      ],
      "metadata": {
        "id": "OuWQUFxT2SVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title PCA visualization and k-means clustering {form-width: \"20%\"}\n",
        "\n",
        "for video in videos:\n",
        "  features = extract_features(model, video)\n",
        "  features = features.cpu().numpy()\n",
        "  print(features.shape)\n",
        "\n",
        "  pca_video = visualize_pca(features, video)\n",
        "  kmeans_video = visualize_kmeans(features, video)\n",
        "  mixed_video = 0.5 * video / 255.0 + 0.5 * kmeans_video\n",
        "  video_titles = ['pca', 'kmeans', 'mixed']\n",
        "  media.show_videos([pca_video, kmeans_video, mixed_video], titles=video_titles, height=128, codec='gif', fps=16, columns=3)"
      ],
      "metadata": {
        "id": "u3aRwHAK2VQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title DAVIS evaluation\n",
        "\n",
        "evaluate_davis(model, extract_features, dataset_path='/content/davis-2017/DAVIS/', height=480, width=880, patch_size=14)"
      ],
      "metadata": {
        "id": "D3uy3s20bBC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title JHMDB evaluation\n",
        "\n",
        "evaluate_jhmdb(model, extract_features, dataset_path='/content/jhmdb/', height=320, width=320, patch_size=14)"
      ],
      "metadata": {
        "id": "IVdAEjoBbEah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title VIP evaluation\n",
        "\n",
        "evaluate_vip(model, extract_features, dataset_path='/content/VIP/', height=448, width=880, patch_size=14)"
      ],
      "metadata": {
        "id": "ppLNXyXjbFsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAh5ArYeTSbV"
      },
      "source": [
        "### CropMAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YI93dVCDTRja"
      },
      "outputs": [],
      "source": [
        "# @title Download CropMAE code\n",
        "\n",
        "%cd /content\n",
        "!git clone https://github.com/alexandre-eymael/CropMAE.git\n",
        "%cd /content/CropMAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-sGBEMbDMev"
      },
      "outputs": [],
      "source": [
        "# @title Download CropMAE checkpoints\n",
        "\n",
        "%cd /content/CropMAE\n",
        "\n",
        "import gdown\n",
        "\n",
        "gdown.download('https://drive.google.com/uc?id=1Hwxpck0MGBJkPNpXRxMOyydtehhhtZbj', '/content/CropMAE/cropmae_in_new.pth', quiet=False)\n",
        "gdown.download('https://drive.google.com/uc?id=1oMXiX_uyGzyQB7S-MYkdJvKFmIuPXkYb', '/content/CropMAE/cropmae_k400.pth', quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uq9a4eORW04p"
      },
      "outputs": [],
      "source": [
        "# @title Load CropMAE model\n",
        "\n",
        "%cd /content/CropMAE\n",
        "\n",
        "from timm.models.vision_transformer import Block\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "  \"\"\" Image to Patch Embedding\"\"\"\n",
        "  def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "    super().__init__()\n",
        "    num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "    self.img_size = img_size\n",
        "    self.patch_size = patch_size\n",
        "    self.num_patches = num_patches\n",
        "    self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "    return x\n",
        "\n",
        "class MaskedAutoencoderViT(nn.Module):\n",
        "  \"\"\" Masked Autoencoder with VisionTransformer backbone\"\"\"\n",
        "  def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4., norm_layer=nn.LayerNorm, ckpt_path=None):\n",
        "    super().__init__()\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # MAE encoder specifics\n",
        "    self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
        "    num_patches = self.patch_embed.num_patches\n",
        "\n",
        "    self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "    self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
        "\n",
        "    self.blocks = nn.ModuleList([Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer) for i in range(depth)])\n",
        "    self.norm = norm_layer(embed_dim)\n",
        "\n",
        "  def interpolate_pos_encoding(self, x, w, h):\n",
        "    npatch = x.shape[1] - 1\n",
        "    N = self.pos_embed.shape[1] - 1\n",
        "    if npatch == N and w == h:\n",
        "      return self.pos_embed\n",
        "    class_pos_embed = self.pos_embed[:, 0]\n",
        "    patch_pos_embed = self.pos_embed[:, 1:]\n",
        "    dim = x.shape[-1]\n",
        "    w0 = w // self.patch_embed.patch_size\n",
        "    h0 = h // self.patch_embed.patch_size\n",
        "    # we add a small number to avoid floating point error in the interpolation\n",
        "    # see discussion at https://github.com/facebookresearch/dino/issues/8\n",
        "    w0, h0 = w0 + 0.1, h0 + 0.1\n",
        "    patch_pos_embed = nn.functional.interpolate(\n",
        "        patch_pos_embed.reshape(\n",
        "            1, int(math.sqrt(N)), int(math.sqrt(N)), dim\n",
        "        ).permute(0, 3, 1, 2),\n",
        "        scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n",
        "        mode=\"bicubic\",\n",
        "    )\n",
        "    assert int(w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]\n",
        "    patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "    return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, nc, w, h = x.shape\n",
        "    x = self.patch_embed(x)  # patch linear embedding\n",
        "\n",
        "    # add the [CLS] token to the embed patch tokens\n",
        "    cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "    x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "    # add positional encoding to each token\n",
        "    x = x + self.interpolate_pos_encoding(x, w, h)\n",
        "\n",
        "    for blk in self.blocks:\n",
        "      x = blk(x)\n",
        "    x = self.norm(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "model = MaskedAutoencoderViT(\n",
        "    patch_size=16, embed_dim=384, depth=12, num_heads=6,\n",
        "    mlp_ratio=4, norm_layer=functools.partial(nn.LayerNorm, eps=1e-6))\n",
        "\n",
        "ckpt = torch.load('/content/CropMAE/cropmae_k400.pth', weights_only=False)\n",
        "pretrain_state = ckpt[\"model\"]\n",
        "model_state = model.state_dict()\n",
        "match_state = {k: v for k, v in pretrain_state.items() if k in model_state}\n",
        "model_state.update(match_state)\n",
        "model.load_state_dict(model_state)\n",
        "\n",
        "model = model.cuda()\n",
        "model.eval()\n",
        "torch.set_grad_enabled(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcdOsKrDL4Cu"
      },
      "outputs": [],
      "source": [
        "# @title Feature extraction function\n",
        "\n",
        "PATCH_SIZE = 16\n",
        "\n",
        "def extract_features(model, video):\n",
        "  video = video.astype(np.float32) / 255.0\n",
        "  video = (video - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
        "  h, w = video.shape[1] // PATCH_SIZE, video.shape[2] // PATCH_SIZE  # feature resolution\n",
        "\n",
        "  torch.cuda.empty_cache()\n",
        "  video = torch.tensor(video).permute(0, 3, 1, 2).float().cuda()  # (T, 3, th, tw)\n",
        "  features = []\n",
        "  for t in range(video.shape[0]):\n",
        "    feature = model(video[t:t+1])[0]  # (h * w + 1, c)\n",
        "    feature = feature[1:].view(h, w, feature.shape[-1])  # (h, w, c)\n",
        "    features.append(feature)\n",
        "  features = torch.stack(features, dim=0)  # (T, h, w, c)\n",
        "  return features"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title PCA visualization and k-means clustering {form-width: \"20%\"}\n",
        "\n",
        "for video in videos:\n",
        "  features = extract_features(model, video)\n",
        "  features = features.cpu().numpy()\n",
        "  print(features.shape)\n",
        "\n",
        "  pca_video = visualize_pca(features, video)\n",
        "  kmeans_video = visualize_kmeans(features, video)\n",
        "  mixed_video = 0.5 * video / 255.0 + 0.5 * kmeans_video\n",
        "  video_titles = ['pca', 'kmeans', 'mixed']\n",
        "  media.show_videos([pca_video, kmeans_video, mixed_video], titles=video_titles, height=128, codec='gif', fps=16, columns=3)"
      ],
      "metadata": {
        "id": "c7Gz02RnbJRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9OPV7wpWL-O"
      },
      "outputs": [],
      "source": [
        "# @title DAVIS evaluation\n",
        "\n",
        "evaluate_davis(model, extract_features, dataset_path='/content/davis-2017/DAVIS/', height=480, width=880, patch_size=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uh9ucYW6WLNP"
      },
      "outputs": [],
      "source": [
        "# @title JHMDB evaluation\n",
        "\n",
        "evaluate_jhmdb(model, extract_features, dataset_path='/content/jhmdb/', height=320, width=320, patch_size=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19EwZDEqWFCL"
      },
      "outputs": [],
      "source": [
        "# @title VIP evaluation\n",
        "\n",
        "evaluate_vip(model, extract_features, dataset_path='/content/VIP/', height=448, width=880, patch_size=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSg1Yz1JCJua"
      },
      "source": [
        "### 4DS-VideoMAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSv5Qv-wCLW2"
      },
      "outputs": [],
      "source": [
        "# @title Download 4DS-VideoMAE code\n",
        "\n",
        "%cd /content\n",
        "!git clone https://github.com/google-deepmind/representations4d.git\n",
        "%cd /content/representations4d\n",
        "!pip install ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YioIozw2Cduf"
      },
      "outputs": [],
      "source": [
        "# @title Download checkpoint\n",
        "\n",
        "%cd /content/representations4d\n",
        "\n",
        "!wget https://storage.googleapis.com/representations4d/checkpoints/scaling4d_dist_b_depth.npz\n",
        "# !wget https://storage.googleapis.com/representations4d/checkpoints/scaling4d_e.npz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gbZ_Wb1C_5Q"
      },
      "outputs": [],
      "source": [
        "# @title Define model\n",
        "\n",
        "from flax import linen as nn\n",
        "from kauldron import kd\n",
        "from kauldron import kontext\n",
        "from kauldron.modules import pos_embeddings, vit as kd_vit, attention, transformers\n",
        "from kauldron.typing import DType, Initializer\n",
        "\n",
        "class LearnedEmbedding(nn.Module):\n",
        "  dtype: DType = jnp.float32\n",
        "  emb_init: Initializer = nn.initializers.normal(stddev=0.02)  # From BERT.\n",
        "  emb_name: str = 'embeddings'\n",
        "  embedding_shape: tuple = (8, 14, 14)\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, shape, *, axis):\n",
        "    emb_shape = self.embedding_shape + (shape[-1],)\n",
        "    pe = self.param(self.emb_name, self.emb_init, emb_shape, self.dtype)\n",
        "    h, w = self.embedding_shape[-2], self.embedding_shape[-1]\n",
        "    *b, tokens_h, tokens_w, d = shape\n",
        "    for _ in range(len(b)-1):\n",
        "      pe = jnp.expand_dims(pe, axis=0)\n",
        "    if tokens_h != h or tokens_w != w:\n",
        "      pe = jax.image.resize(pe, (*b, tokens_h, tokens_w, d), method='bicubic')\n",
        "    return pe\n",
        "\n",
        "class GeneralizedTransformer(nn.Module):\n",
        "  layers: list\n",
        "  n_iter: int = 1\n",
        "\n",
        "  def __call__(self, tokens):\n",
        "    aux = [jnp.reshape(tokens, tokens.shape)]\n",
        "    latent_state = tokens\n",
        "    for h in range(self.n_iter):\n",
        "      if h > 0:\n",
        "        latent_state = jnp.concatenate([latent_state, tokens], axis=-2)\n",
        "      for layer in self.layers:\n",
        "        if h == self.n_iter - 1:\n",
        "          aux.append(latent_state)\n",
        "        latent_state = layer(latent_state)\n",
        "        latent_state = jnp.reshape(latent_state, [latent_state.shape[0], -1, latent_state.shape[-1]])\n",
        "    return aux\n",
        "\n",
        "  @classmethod\n",
        "  def from_variant_str(cls, variant_str, **kwargs):\n",
        "    vit_spec = kd_vit.ViTSpec.from_variant_string(variant_str)\n",
        "    all_kwargs = vit_spec.kwargs | kwargs\n",
        "    all_kwargs.pop('patch_size', None)\n",
        "    all_kwargs.pop('hidden_size', None)\n",
        "    return cls.from_spec(**all_kwargs)\n",
        "\n",
        "  @classmethod\n",
        "  def from_spec(cls, num_heads, num_layers, mlp_size=None, qk_features=None, v_features=None, **kwargs):\n",
        "    blocks = []\n",
        "    for _ in range(num_layers):\n",
        "      blocks.append(\n",
        "        transformers.PreNormBlock(\n",
        "          attention_norm=nn.LayerNorm(),\n",
        "          mlp_norm=nn.LayerNorm(),\n",
        "          attention=attention.ImprovedMultiHeadDotProductAttention(\n",
        "            num_heads=num_heads,\n",
        "            qk_features=qk_features,\n",
        "            v_features=v_features,\n",
        "            kernel_init=nn.initializers.lecun_normal()),\n",
        "          mlp=transformers.TransformerMLP(hidden_size=mlp_size, kernel_init=nn.initializers.xavier_uniform()),\n",
        "        )\n",
        "      )\n",
        "    return cls(layers=tuple(blocks), **kwargs)\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "  encoder: nn.Module\n",
        "  processor: nn.Module\n",
        "\n",
        "  def __call__(self, video):\n",
        "    tokens = self.encoder(video)\n",
        "    features = self.processor(tokens)\n",
        "    return features[-1]\n",
        "\n",
        "\n",
        "class Tokenizer(nn.Module):\n",
        "  patch_embedding: nn.Module\n",
        "  posenc: nn.Module\n",
        "\n",
        "  def __call__(self, images):\n",
        "    tokens = self.patch_embedding(images)\n",
        "    posenc = self.posenc(tokens.shape, axis=(-4, -3, -2))\n",
        "    tokens += posenc\n",
        "    tokens = einops.rearrange(tokens, '... T h w D -> ... (T h w) D')\n",
        "    return tokens\n",
        "\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "  patch_size: tuple\n",
        "  num_features: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, images):\n",
        "    return nn.Conv(features=self.num_features, kernel_size=self.patch_size, strides=self.patch_size, padding='VALID')(images)\n",
        "\n",
        "\n",
        "class EncoderToReadout(nn.Module):\n",
        "  embedding_shape: tuple\n",
        "  readout_depth: int\n",
        "  num_input_frames: int\n",
        "\n",
        "  def __call__(self, all_features):\n",
        "    readout_id = int(len(all_features) * self.readout_depth) - 1\n",
        "    features = all_features[readout_id]\n",
        "    readout_features = jnp.reshape(features,\n",
        "      (features.shape[0], self.embedding_shape[0],\n",
        "       self.embedding_shape[1] * self.embedding_shape[2], features.shape[-1])\n",
        "    )\n",
        "    out_shape = (readout_features.shape[0], self.num_input_frames,\n",
        "                 self.embedding_shape[0] * self.embedding_shape[1] * self.embedding_shape[2] // self.embedding_shape[0],\n",
        "                 readout_features.shape[3])\n",
        "    readout_features = jax.image.resize(readout_features, out_shape, jax.image.ResizeMethod.CUBIC)\n",
        "    return readout_features\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  hidden_size: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    out_dim = x.shape[-1]\n",
        "    x = nn.gelu(nn.Dense(self.hidden_size)(x))\n",
        "    return nn.Dense(out_dim)(x)\n",
        "\n",
        "\n",
        "class AttentionReadout(nn.Module):\n",
        "  num_classes: int\n",
        "  num_params: int\n",
        "  num_heads: int\n",
        "  num_queries: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs):\n",
        "    feats = nn.LayerNorm()(inputs) # (1, 16, 196, 768)\n",
        "    feats += kd.nn.LearnedEmbedding(name='temporal_posenc')(feats.shape, axis=-3)\n",
        "    feats = einops.rearrange(feats, '... T N C -> ... (T N) C') # (1, 3136, 768)\n",
        "    query = self.param('query', nn.initializers.normal(0.02), [self.num_queries, self.num_heads, self.num_params // self.num_heads]) # (6272, 16, 64)\n",
        "    query = jnp.broadcast_to(query, (feats.shape[0],) + query.shape) # (1, 6272, 16, 64)\n",
        "    key = nn.DenseGeneral(features=(self.num_heads, self.num_params // self.num_heads), axis=-1, use_bias=True, name='key_embedding')(feats) # (1, 3136, 16, 64)\n",
        "    val = nn.DenseGeneral(features=(self.num_heads, self.num_params // self.num_heads), axis=-1, use_bias=True, name='value_embedding')(feats) # (1, 3136, 16, 64)\n",
        "    token = nn.dot_product_attention(query=query, key=key, value=val) # (1, 6272, 16, 64)\n",
        "    token = einops.rearrange(token, '... Q N c -> ... Q (N c)') # (1, 6272, 1024)\n",
        "    query = einops.rearrange(query, '... Q N c -> ... Q (N c)') # (1, 6272, 1024)\n",
        "    token = query + nn.Dense(self.num_params)(token)\n",
        "    token = token + MLP(self.num_params * 4)(nn.LayerNorm()(token))\n",
        "    out = nn.Dense(self.num_classes)(token) # (1, 6272, 128)\n",
        "    return out\n",
        "\n",
        "\n",
        "model = nn.Sequential([\n",
        "    Model(\n",
        "        encoder=Tokenizer(\n",
        "            patch_embedding=PatchEmbedding(patch_size=(2,16,16), num_features=768),\n",
        "            posenc=LearnedEmbedding()),\n",
        "        processor=GeneralizedTransformer.from_variant_str(variant_str=\"B\")),\n",
        "        # processor=GeneralizedTransformer.from_variant_str(variant_str=\"e\")),\n",
        "    # EncoderToReadout(embedding_shape=(8,14,14), readout_depth=0.95, num_input_frames=16),\n",
        "    # AttentionReadout(num_classes=128, num_params=1024, num_heads=16, num_queries=6272)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSmMIlEUDCKk"
      },
      "outputs": [],
      "source": [
        "# @title Load checkpoint\n",
        "\n",
        "def recover_tree(flat_dict):\n",
        "  tree = {}\n",
        "  for k, v in flat_dict.items():\n",
        "    parts = k.split(\"/\")\n",
        "    node = tree\n",
        "    for part in parts[:-1]:\n",
        "      if part not in node:\n",
        "        node[part] = {}\n",
        "      node = node[part]\n",
        "    node[parts[-1]] = v\n",
        "  return tree\n",
        "\n",
        "restored_params = recover_tree(np.load(\"scaling4d_dist_b_depth.npz\", allow_pickle=False))\n",
        "# restored_params = recover_tree(np.load(\"scaling4d_e.npz\", allow_pickle=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usd4Y9S4DPFK"
      },
      "outputs": [],
      "source": [
        "# @title Feature extraction function\n",
        "\n",
        "WINDOW_LENGTH = 16\n",
        "PATCH_SIZE = 16\n",
        "\n",
        "def extract_features(model, params, video):\n",
        "  video = video.astype(np.float32) / 255.0\n",
        "  h, w = video.shape[1] // PATCH_SIZE, video.shape[2] // PATCH_SIZE  # feature resolution\n",
        "\n",
        "  num_frames = video.shape[0]\n",
        "  if num_frames % WINDOW_LENGTH:\n",
        "    video = jnp.pad(video, ((0, WINDOW_LENGTH - num_frames % WINDOW_LENGTH), (0, 0), (0, 0), (0, 0)))\n",
        "\n",
        "  def forward(params, video):\n",
        "    return model.apply(params, video, is_training_property=False)\n",
        "\n",
        "  features = []\n",
        "  for t in range(0, num_frames, WINDOW_LENGTH):\n",
        "    outputs = forward(params, video[t : t + WINDOW_LENGTH][None])\n",
        "    feature = outputs[0].reshape(WINDOW_LENGTH // 2, h, w, outputs[0].shape[-1])\n",
        "    feature = jax.image.resize(feature, (WINDOW_LENGTH, h, w, feature.shape[-1]), jax.image.ResizeMethod.CUBIC)\n",
        "    features.append(feature)\n",
        "  features = np.concatenate(features, axis=0)\n",
        "  features = features[:num_frames]\n",
        "  return features"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title PCA visualization and k-means clustering {form-width: \"20%\"}\n",
        "\n",
        "for video in videos:\n",
        "  features = extract_features(model, restored_params, video)\n",
        "  print(features.shape)\n",
        "\n",
        "  pca_video = visualize_pca(features, video)\n",
        "  kmeans_video = visualize_kmeans(features, video)\n",
        "  mixed_video = 0.5 * video / 255.0 + 0.5 * kmeans_video\n",
        "  video_titles = ['pca', 'kmeans', 'mixed']\n",
        "  media.show_videos([pca_video, kmeans_video, mixed_video], titles=video_titles, height=128, codec='gif', fps=16, columns=3)"
      ],
      "metadata": {
        "id": "vN1b0HyZ_2n_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDO4WbKLEuw3"
      },
      "outputs": [],
      "source": [
        "# @title DAVIS evaluation\n",
        "\n",
        "evaluate_davis(model, extract_features, dataset_path='/content/davis-2017/DAVIS/', height=480, width=880, patch_size=16, params=restored_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_ku9Q1M-Cb1"
      },
      "outputs": [],
      "source": [
        "# @title JHMDB evaluation\n",
        "\n",
        "evaluate_jhmdb(model, extract_features, dataset_path='/content/jhmdb/', height=320, width=320, patch_size=16, params=restored_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fk23S6I3AQId"
      },
      "outputs": [],
      "source": [
        "# @title VIP evaluation\n",
        "\n",
        "evaluate_vip(model, extract_features, dataset_path='/content/VIP/', height=448, width=880, patch_size=16, params=restored_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RVM"
      ],
      "metadata": {
        "id": "d7Qiqzqb_k-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download checkpoint\n",
        "\n",
        "%mkdir /content/rvm\n",
        "%cd /content/rvm\n",
        "\n",
        "!wget https://storage.googleapis.com/dm-tapnet/tmp/pretrain_rvm_large16_256_xid175558463_wid1.npz"
      ],
      "metadata": {
        "id": "4EgDXCb2_o_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load checkpoint\n",
        "\n",
        "%cd /content/rvm\n",
        "\n",
        "def recover_tree(flat_dict):\n",
        "  tree = {}\n",
        "  for k, v in flat_dict.items():\n",
        "    parts = k.split(\"/\")\n",
        "    node = tree\n",
        "    for part in parts[:-1]:\n",
        "      if part not in node:\n",
        "        node[part] = {}\n",
        "      node = node[part]\n",
        "    node[parts[-1]] = v\n",
        "  return tree\n",
        "\n",
        "restored_params = recover_tree(np.load(\"pretrain_rvm_large16_256_xid175558463_wid1.npz\", allow_pickle=False))"
      ],
      "metadata": {
        "id": "3e3JCNU1_uVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define model {form-width: \"20%\"}\n",
        "\n",
        "import dataclasses\n",
        "import re\n",
        "from typing import Any\n",
        "import jax.numpy as jnp\n",
        "from flax import linen as nn\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "  \"\"\"Extracts patches with a single learned linear projection.\"\"\"\n",
        "  patch_size: list[int]\n",
        "  num_features: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, images):\n",
        "    return nn.Conv(features=self.num_features, kernel_size=self.patch_size, strides=self.patch_size, padding='VALID')(images)\n",
        "\n",
        "def get_mae_sinusoid_encoding_table(n_position, d_hid, dtype=jnp.float32):\n",
        "  \"\"\"Sinusoid positional encoding table for MAE.\"\"\"\n",
        "  def get_position_angle_vec(position):\n",
        "    return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]\n",
        "\n",
        "  sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n",
        "  sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
        "  sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
        "\n",
        "  return jnp.asarray(sinusoid_table, dtype)[None, ...]\n",
        "\n",
        "class SincosPosEmb(nn.Module):\n",
        "  \"\"\"Returns sinusoidal positional embedding given the shape of the tokens.\"\"\"\n",
        "  base_token_shape: list[int] | None = None\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, tokens_shape):\n",
        "    d = tokens_shape[-1]\n",
        "    if self.base_token_shape is not None:\n",
        "      h, w = self.base_token_shape\n",
        "    else:\n",
        "      h, w = tokens_shape[-3], tokens_shape[-2]\n",
        "\n",
        "    posenc = get_mae_sinusoid_encoding_table(np.prod((h, w)), d)\n",
        "    posenc = einops.rearrange(posenc, '... (h w) d -> ... h w d', h=h, w=w)\n",
        "    *b, tokens_h, tokens_w, _ = tokens_shape\n",
        "    for _ in range(len(b)-1):\n",
        "      posenc = jnp.expand_dims(posenc, axis=0)\n",
        "    if tokens_h != h or tokens_w != w:\n",
        "      posenc = jax.image.resize(posenc, (*b, tokens_h, tokens_w, d), method='bicubic')\n",
        "\n",
        "    return posenc\n",
        "\n",
        "class Tokenizer(nn.Module):\n",
        "  \"\"\"Simple tokenizer.\"\"\"\n",
        "  patch_embedding: nn.Module\n",
        "  posenc: nn.Module\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, image):\n",
        "    tokens = self.patch_embedding(image)\n",
        "    posenc = self.posenc(tokens.shape)\n",
        "    tokens += posenc\n",
        "    return tokens\n",
        "\n",
        "class TransformerMLP(nn.Module):\n",
        "  \"\"\"Simple MLP with a single hidden layer for use in Transformer blocks.\"\"\"\n",
        "  hidden_size: int = None  # Defaults to 4 times input dims.\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs):\n",
        "    d = inputs.shape[-1]\n",
        "    hidden_size = 4 * d if self.hidden_size is None else self.hidden_size\n",
        "    h = nn.Dense(\n",
        "        features=hidden_size,\n",
        "        name='dense_in',\n",
        "        kernel_init=nn.initializers.xavier_uniform(),\n",
        "        bias_init=nn.initializers.zeros,\n",
        "        dtype=inputs.dtype,\n",
        "    )(inputs)\n",
        "    h = nn.gelu(h)\n",
        "    return nn.Dense(\n",
        "        features=inputs.shape[-1],\n",
        "        name='dense_out',\n",
        "        kernel_init=nn.initializers.xavier_uniform(),\n",
        "        bias_init=nn.initializers.zeros,\n",
        "        dtype=h.dtype,\n",
        "    )(h)\n",
        "\n",
        "class PreNormBlock(nn.Module):\n",
        "  \"\"\"Pre-LN Transformer layer (default transformer layer).\"\"\"\n",
        "\n",
        "  attention: Any\n",
        "  mlp: nn.Module\n",
        "  attention_norm: Any\n",
        "  mlp_norm: Any\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, tokens):\n",
        "    norm_tokens = self.attention_norm(tokens)\n",
        "    tokens += self.attention(\n",
        "        inputs_q=norm_tokens,\n",
        "        inputs_k=norm_tokens,\n",
        "        inputs_v=norm_tokens,\n",
        "    )\n",
        "    norm_tokens = self.mlp_norm(tokens)\n",
        "    return tokens + self.mlp(norm_tokens)\n",
        "\n",
        "VIT_SIZES = {\n",
        "    'mu': (32, 1, 128, 2),\n",
        "    'Ti': (192, 12, 768, 3),\n",
        "    'S': (384, 12, 1536, 6),\n",
        "    'M': (512, 12, 2048, 8),\n",
        "    'B': (768, 12, 3072, 12),\n",
        "    'L': (1024, 24, 4096, 16),\n",
        "    'H': (1280, 32, 5120, 16),\n",
        "    'g': (1408, 40, 6144, 16),\n",
        "    'G': (1664, 48, 8192, 16),\n",
        "    'e': (1792, 56, 15360, 16),\n",
        "}\n",
        "\n",
        "@dataclasses.dataclass(frozen=True)\n",
        "class ViTSpec:\n",
        "  \"\"\"Spec for the size of a Vision Transformer.\"\"\"\n",
        "\n",
        "  hidden_size: int  # Dimension of tokens passed between blocks.\n",
        "  num_layers: int  # Number of trasformer blocks.\n",
        "  mlp_size: int  # Hidden dimension of the MLP in each block.\n",
        "  num_heads: int  # Number of attention heads.\n",
        "  patch_size: int = None  # Patch size of initial image patches.\n",
        "\n",
        "  @classmethod\n",
        "  def from_variant_string(cls, variant_str: str):\n",
        "    \"\"\"Parse variant strings like \"ViT-L\", \"B\", or \"Ti/16\".\"\"\"\n",
        "    r = re.match(\n",
        "        r'^([Vv][Ii][Tt][-_])?(?P<name>[a-zA-Z]{1,2})(/(?P<patch>\\d+))?$',\n",
        "        variant_str,\n",
        "    )\n",
        "    if r is None:\n",
        "      raise ValueError(f'Invalid variant string: {variant_str!r}.')\n",
        "    name = r.groupdict()['name']\n",
        "    spec = cls(*VIT_SIZES[name])\n",
        "\n",
        "    patch_size = r.groupdict()['patch']\n",
        "    if patch_size is not None:\n",
        "      spec = dataclasses.replace(spec, patch_size=int(patch_size))\n",
        "    return spec\n",
        "\n",
        "  @property\n",
        "  def kwargs(self):\n",
        "    kwargs = dict(\n",
        "        hidden_size=self.hidden_size,\n",
        "        num_layers=self.num_layers,\n",
        "        mlp_size=self.mlp_size,\n",
        "        num_heads=self.num_heads,\n",
        "        patch_size=self.patch_size,\n",
        "    )\n",
        "    if self.patch_size is None:\n",
        "      del kwargs['patch_size']\n",
        "    return kwargs\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  \"\"\"Simple transformer model.\"\"\"\n",
        "  layers: tuple[Any]\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, tokens):\n",
        "    for layer in self.layers:\n",
        "      tokens = layer(tokens)\n",
        "    tokens = nn.LayerNorm(dtype=tokens.dtype)(tokens)\n",
        "    return tokens\n",
        "\n",
        "  @classmethod\n",
        "  def from_variant_str(cls, variant_str: str, **kwargs):\n",
        "    vit_spec = ViTSpec.from_variant_string(variant_str)\n",
        "    all_kwargs = vit_spec.kwargs | kwargs\n",
        "    all_kwargs.pop('patch_size', None)\n",
        "    all_kwargs.pop('hidden_size', None)\n",
        "    return cls.from_spec(**all_kwargs)\n",
        "\n",
        "  @classmethod\n",
        "  def from_spec(\n",
        "      cls,\n",
        "      num_heads: int,\n",
        "      num_layers: int,\n",
        "      mlp_size = None,\n",
        "      dtype=jnp.float32,\n",
        "      qk_features = None,\n",
        "      v_features = None,\n",
        "      **kwargs,\n",
        "  ):\n",
        "    return cls(\n",
        "        layers=tuple(\n",
        "            PreNormBlock(\n",
        "                attention_norm=nn.LayerNorm(dtype=dtype),\n",
        "                mlp_norm=nn.LayerNorm(dtype=dtype),\n",
        "                attention=ImprovedMultiHeadDotProductAttention(\n",
        "                    num_heads=num_heads,\n",
        "                    qk_features=qk_features,\n",
        "                    v_features=v_features,\n",
        "                ),\n",
        "                mlp=TransformerMLP(hidden_size=mlp_size),\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ),\n",
        "        **kwargs,\n",
        "    )\n",
        "\n",
        "class GatedTransformerCore(nn.Module):\n",
        "  transformer: nn.Module\n",
        "  initializer: nn.Module\n",
        "  token_dim: int\n",
        "  state_layer_norm: nn.Module\n",
        "\n",
        "  def setup(self):\n",
        "    self.input_update = nn.Dense(self.token_dim, use_bias=False)\n",
        "    self.input_reset = nn.Dense(self.token_dim, use_bias=False)\n",
        "    self.state_update = nn.Dense(self.token_dim, use_bias=False)\n",
        "    self.state_reset = nn.Dense(self.token_dim, use_bias=False)\n",
        "\n",
        "  def __call__(self, inputs, state):\n",
        "    update_gate = jax.nn.sigmoid(self.input_update(inputs) + self.state_update(state))\n",
        "    reset_gate = jax.nn.sigmoid(self.input_reset(inputs) + self.state_reset(state))\n",
        "    h = self.transformer(inputs, inputs_kv=reset_gate * self.state_layer_norm(state))\n",
        "    output = (1-update_gate)*state + update_gate * h\n",
        "    state = output\n",
        "    return output, state\n",
        "\n",
        "def softmax(x):\n",
        "  return jax.nn.softmax(x.astype(jnp.float32), axis=-1).astype(jnp.float32)\n",
        "\n",
        "def dot_product_attention_weights(query, key):\n",
        "  query = query / jnp.sqrt(query.shape[-1])\n",
        "  attn_weights = jnp.einsum('...qhd,...khd->...hqk', query, key)\n",
        "  return softmax(attn_weights)\n",
        "\n",
        "class ImprovedMultiHeadDotProductAttention(nn.Module):\n",
        "  \"\"\"Multi-head dot-product attention.\"\"\"\n",
        "\n",
        "  num_heads: int\n",
        "  qk_features: int = None\n",
        "  v_features: int = None\n",
        "  out_features: int = None\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(\n",
        "      self,\n",
        "      inputs_q,\n",
        "      inputs_k = None,\n",
        "      inputs_v = None,\n",
        "      *,\n",
        "      bias = None,\n",
        "      mask = None,\n",
        "  ):\n",
        "    qk_features = self.qk_features or inputs_q.shape[-1]\n",
        "    v_features = self.v_features or qk_features\n",
        "\n",
        "    if inputs_k is None:\n",
        "      inputs_k = inputs_q\n",
        "    if inputs_v is None:\n",
        "      inputs_v = inputs_k\n",
        "\n",
        "    def dense(name, x, features):\n",
        "      return nn.DenseGeneral(\n",
        "          features=(self.num_heads, features // self.num_heads),\n",
        "          kernel_init=nn.initializers.lecun_normal(),\n",
        "          bias_init=nn.initializers.zeros_init(),\n",
        "          use_bias=True,\n",
        "          dtype=x.dtype,\n",
        "          name=name,\n",
        "      )(x)\n",
        "\n",
        "    query = dense('query', inputs_q, qk_features)\n",
        "    key = dense('key', inputs_k, qk_features)\n",
        "    value = dense('value', inputs_v, v_features)\n",
        "\n",
        "    # Compute attention weights.\n",
        "    attn_weights = dot_product_attention_weights(query=query, key=key)\n",
        "\n",
        "    # Return weighted sum over values for each query position.\n",
        "    x = jnp.einsum('...hqk,...khd->...qhd', attn_weights, value)\n",
        "\n",
        "    # Back to the original input dimensions.\n",
        "    return nn.DenseGeneral(\n",
        "        features=self.out_features or inputs_q.shape[-1],\n",
        "        axis=(-2, -1),\n",
        "        kernel_init=nn.initializers.lecun_normal(),\n",
        "        bias_init=nn.initializers.zeros_init(),\n",
        "        use_bias=True,\n",
        "        dtype=x.dtype,\n",
        "        name='out',\n",
        "    )(x)\n",
        "\n",
        "class CrossAttentionTransformer(nn.Module):\n",
        "  \"\"\"Cross attention transformer.\"\"\"\n",
        "  num_heads: int\n",
        "  num_layers: int\n",
        "  num_feats: int\n",
        "  mlp_dim: int\n",
        "  dtype: Any\n",
        "\n",
        "  def setup(self):\n",
        "    self.xa_blocks = [CrossAttentionBlock(\n",
        "        num_heads=self.num_heads, num_feats=self.num_feats,\n",
        "        mlp_dim=self.mlp_dim, dtype=self.dtype,\n",
        "    ) for _ in range(self.num_layers)]\n",
        "    self.output_norm = nn.LayerNorm(dtype=self.dtype)\n",
        "\n",
        "  def __call__(self, inputs, inputs_kv):\n",
        "    for i in range(self.num_layers):\n",
        "      inputs = self.xa_blocks[i](inputs, inputs_kv)\n",
        "    return self.output_norm(inputs)\n",
        "\n",
        "class CrossAttentionBlock(nn.Module):\n",
        "  \"\"\"Cross attention block.\"\"\"\n",
        "\n",
        "  num_heads: int\n",
        "  num_feats: int\n",
        "  mlp_dim: int\n",
        "  dtype: Any\n",
        "\n",
        "  def setup(self):\n",
        "    self.attention_norm = nn.LayerNorm(dtype=self.dtype)\n",
        "    self.mlp_norm = nn.LayerNorm(dtype=self.dtype)\n",
        "    self.ca_attention_norm = nn.LayerNorm(dtype=self.dtype)\n",
        "    self.attention = ImprovedMultiHeadDotProductAttention(\n",
        "        num_heads=self.num_heads,\n",
        "        qk_features=self.num_feats,\n",
        "        v_features=self.num_feats,\n",
        "    )\n",
        "    self.ca_attention = ImprovedMultiHeadDotProductAttention(\n",
        "        num_heads=self.num_heads,\n",
        "        qk_features=self.num_feats,\n",
        "        v_features=self.num_feats,\n",
        "    )\n",
        "    self.mlp = TransformerMLP(hidden_size=self.mlp_dim)\n",
        "\n",
        "  def __call__(self, inputs, inputs_kv):\n",
        "    x = inputs\n",
        "    x = x + self.ca_attention(inputs_q=self.ca_attention_norm(x), inputs_k=inputs_kv, inputs_v=inputs_kv)\n",
        "    x = x + self.mlp(self.mlp_norm(x))\n",
        "    x = x + self.attention(self.attention_norm(x))\n",
        "    return x\n",
        "\n",
        "class RandomStateInit(nn.Module):\n",
        "  \"\"\"Random, non-learnable state initialization.\"\"\"\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs, batch_shape):\n",
        "    shape = inputs.shape[-2:]\n",
        "    state = 0 * jax.random.normal(key=self.make_rng(\"default\"), shape=batch_shape + shape)\n",
        "    return state\n",
        "\n",
        "class VideoSiamMAE(nn.Module):\n",
        "  \"\"\"Video Siamese masked autoencoder model.\"\"\"\n",
        "\n",
        "  tokenizer: nn.Module\n",
        "  encoder: nn.Module\n",
        "  rnn_core: nn.Module\n",
        "  latent_emb_dim: int = 384\n",
        "\n",
        "  def setup(self):\n",
        "    self.cls_token = self.param('cls_token', nn.initializers.normal(stddev=0.02), (1, self.latent_emb_dim))\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, frame, state = None):\n",
        "    frame_tokens = self.tokenizer(frame)\n",
        "    frame_tokens = einops.rearrange(frame_tokens, '... h w D -> ... (h w) D')\n",
        "\n",
        "    *b, _, _ = frame_tokens.shape\n",
        "    cls_token = jnp.broadcast_to(self.cls_token, b + [1, self.cls_token.shape[-1]])\n",
        "    frame_tokens = jnp.concatenate([cls_token, frame_tokens], axis=-2)\n",
        "\n",
        "    encoded_frame_tokens = self.encoder(frame_tokens)\n",
        "    if state is None:\n",
        "      state = self.rnn_core.initializer(encoded_frame_tokens, batch_shape=(1,))\n",
        "    features, state = self.rnn_core(encoded_frame_tokens, state)\n",
        "\n",
        "    return dict(features=features, state=state)\n",
        "\n",
        "model = VideoSiamMAE(\n",
        "    tokenizer=Tokenizer(\n",
        "        patch_embedding=PatchEmbedding(patch_size=[1, 16, 16], num_features=1024),\n",
        "        posenc=SincosPosEmb(base_token_shape=[16, 16]),\n",
        "    ),\n",
        "    encoder=Transformer.from_variant_str(variant_str='L', dtype=jax.numpy.bfloat16),\n",
        "    rnn_core=GatedTransformerCore(\n",
        "        transformer=CrossAttentionTransformer(\n",
        "            num_layers=4,\n",
        "            num_heads=16,\n",
        "            num_feats=1024,\n",
        "            mlp_dim=4096,\n",
        "            dtype=jax.numpy.bfloat16,\n",
        "        ),\n",
        "        initializer=RandomStateInit(),\n",
        "        token_dim=1024,\n",
        "        state_layer_norm=nn.LayerNorm(epsilon=0.0001, use_scale=True, use_bias=False),\n",
        "    ),\n",
        "    latent_emb_dim=1024,\n",
        ")"
      ],
      "metadata": {
        "id": "nDchT_aY_wCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Feature extraction function\n",
        "\n",
        "PATCH_SIZE = 16\n",
        "\n",
        "def extract_features(model, params, video):\n",
        "  video = video.astype(np.float32) / 255.0\n",
        "  h, w = video.shape[1] // PATCH_SIZE, video.shape[2] // PATCH_SIZE  # feature resolution\n",
        "\n",
        "  @jax.jit\n",
        "  def forward(params, frame, model_state, rng_key):\n",
        "    output = model.apply(\n",
        "        {'params': params},\n",
        "        frame,  # [B, H, W, 3]\n",
        "        model_state,  # [B, N, D]\n",
        "        capture_intermediates=False,\n",
        "        rngs=rng_key,\n",
        "    )\n",
        "    return output\n",
        "\n",
        "  rng_key = jax.random.PRNGKey(0)\n",
        "  model_state = None\n",
        "  features = []\n",
        "  for t in range(video.shape[0]):\n",
        "    output = forward(params, video[t][None], model_state, rng_key)\n",
        "    model_state, feature, cls_token = output['state'], output['features'][0, 1:, :], output['features'][0, 0:1, :]\n",
        "    feature = feature.reshape(h, w, -1)\n",
        "    features.append(feature)\n",
        "  features = np.stack(features, axis=0)\n",
        "  return features"
      ],
      "metadata": {
        "id": "r7OWo9dZ_xxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title PCA visualization and k-means clustering {form-width: \"20%\"}\n",
        "\n",
        "for video in videos:\n",
        "  features = extract_features(model, restored_params, video)\n",
        "  print(features.shape)\n",
        "\n",
        "  pca_video = visualize_pca(features, video)\n",
        "  kmeans_video = visualize_kmeans(features, video)\n",
        "  mixed_video = 0.5 * video / 255.0 + 0.5 * kmeans_video\n",
        "  video_titles = ['pca', 'kmeans', 'mixed']\n",
        "  media.show_videos([pca_video, kmeans_video, mixed_video], titles=video_titles, height=128, codec='gif', fps=16, columns=3)"
      ],
      "metadata": {
        "id": "7NIgdC2L_zro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title DAVIS evaluation\n",
        "\n",
        "evaluate_davis(model, extract_features, dataset_path='/content/davis-2017/DAVIS/', height=480, width=880, patch_size=16, params=restored_params)"
      ],
      "metadata": {
        "id": "x5-kboh2ACj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title JHMDB evaluation\n",
        "\n",
        "evaluate_jhmdb(model, extract_features, dataset_path='/content/jhmdb/', height=320, width=320, patch_size=16, params=restored_params)"
      ],
      "metadata": {
        "id": "JfkZurCrAFEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title VIP evaluation\n",
        "\n",
        "evaluate_vip(model, extract_features, dataset_path='/content/VIP/', height=448, width=880, patch_size=16, params=restored_params)"
      ],
      "metadata": {
        "id": "WL6hyBX9AHF0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "last_runtime": {
        "build_target": "//learning/pytorch/colab:kernel",
        "kind": "private"
      },
      "machine_shape": "hm",
      "provenance": [
        {
          "file_id": "1BA8StfouHvQChfUHN264qkkKWyQ4LWQa",
          "timestamp": 1765234975033
        },
        {
          "file_id": "1VX4rxrjP05POLVzRqjOo1KCXHbBCg_9c",
          "timestamp": 1755600525138
        },
        {
          "file_id": "1davdOLPoQ4KiWkKc6HKWRb48IJTqUET3",
          "timestamp": 1753193893350
        },
        {
          "file_id": "1gNb5SWr16OHEOcHIkRI783Cz-B5he_sK",
          "timestamp": 1752155778633
        },
        {
          "file_id": "1Sg7f1ibn8r6YaAz6by1no0zXn4K9kw-W",
          "timestamp": 1747303081093
        },
        {
          "file_id": "1Dla4d31nDY1skIkEKRCeSSW2l9RLCFH6",
          "timestamp": 1742835424224
        },
        {
          "file_id": "1NiZzcShUkJIyI3OBi4rHat0WXQhVVqN2",
          "timestamp": 1741194275175
        },
        {
          "file_id": "1Cmj_lBDUFDPp5A_YDU3xZUjgfDAd8L1P",
          "timestamp": 1741046636775
        },
        {
          "file_id": "1em9gkinQ6-F3TnBtwL5TdYZmvAkD7XRA",
          "timestamp": 1741034974907
        },
        {
          "file_id": "1npI3DYCA629BZXYbEUt8fHJGT_A-_9CN",
          "timestamp": 1741004612797
        }
      ],
      "toc_visible": true,
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
