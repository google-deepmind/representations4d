{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WS5t185lhfMa"
      },
      "outputs": [],
      "source": [
        "# @title Install dependency\n",
        "\n",
        "!pip install mediapy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8FfWGdoIq4B"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "\n",
        "import glob\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "\n",
        "import cv2\n",
        "import einops\n",
        "from PIL import Image\n",
        "import scipy.io as sio\n",
        "import mediapy as media\n",
        "import numpy as np\n",
        "import queue\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import tqdm\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import sklearn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download checkpoint\n",
        "\n",
        "%mkdir /content/rvm\n",
        "%cd /content/rvm\n",
        "\n",
        "!wget https://storage.googleapis.com/representations4d/checkpoints/pretrain_rvm_large16_256_175558463.npz"
      ],
      "metadata": {
        "id": "4EgDXCb2_o_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare Qualitative Examples"
      ],
      "metadata": {
        "id": "Iru6nvD-Ubiz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmODMFlwnD3X"
      },
      "outputs": [],
      "source": [
        "# @title Download DAVIS videos\n",
        "\n",
        "video_names = [\n",
        "    'goat',\n",
        "    # 'india',\n",
        "    # 'soapbox',\n",
        "]\n",
        "\n",
        "for video_name in video_names:\n",
        "  !wget https://storage.googleapis.com/representations4d/datasets/davis2017/{video_name}.mp4\n",
        "\n",
        "davis_videos = []\n",
        "for video_name in video_names:\n",
        "  video = media.read_video(f\"{video_name}.mp4\")\n",
        "  video = media.resize_video(video, (480, 880))\n",
        "  davis_videos.append(video)\n",
        "  media.show_video(video, fps=16, height=128, codec='gif')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mgqCSVRj0_Q"
      },
      "outputs": [],
      "source": [
        "# @title Download MoCA videos\n",
        "\n",
        "video_names = [\n",
        "    'scorpionfish_0',\n",
        "    # 'snow_leopard_8',\n",
        "    # 'white_tailed_ptarmigan',\n",
        "    # 'grasshopper_1',\n",
        "    # 'plaice',\n",
        "    # 'pygmy_seahorse_0',\n",
        "    # 'stick_insect_0',\n",
        "    # 'crab',\n",
        "    # 'crab_1',\n",
        "    # 'copperhead_snake',\n",
        "]\n",
        "\n",
        "for video_name in video_names:\n",
        "  !wget https://storage.googleapis.com/representations4d/datasets/MoCA/{video_name}.mp4\n",
        "\n",
        "moca_videos = []\n",
        "for video_name in video_names:\n",
        "  video = media.read_video(f\"{video_name}.mp4\")\n",
        "  video = media.resize_video(video, (360, 640))\n",
        "  moca_videos.append(video)\n",
        "  media.show_video(video, fps=16, width=256, codec='gif')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download Dalmatian videos\n",
        "\n",
        "!wget https://storage.googleapis.com/representations4d/assets/dalmatian_illusion.mp4\n",
        "\n",
        "dalmatian_video = media.read_video(\"dalmatian_illusion.mp4\")\n",
        "dalmatian_video = media.resize_video(dalmatian_video, (360, 480))\n",
        "media.show_video(dalmatian_video, fps=16, width=256, codec='gif')"
      ],
      "metadata": {
        "id": "17S-pK1rrpWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUWdU_F1jTwi"
      },
      "outputs": [],
      "source": [
        "# @title Create noise video\n",
        "\n",
        "def create_noise_movie(T, h, w):\n",
        "  background = np.random.rand(h, w, 3)\n",
        "  square = np.random.rand(h // 2, w // 2, 3)\n",
        "  movie = []\n",
        "  for t in range(T):\n",
        "    frame = background.copy()\n",
        "    # Calculate the position of the square\n",
        "    x = (t * (w - w // 2)) // T  # Move across the width\n",
        "    y = (h - h // 2) // 2  # Centered vertically\n",
        "    frame[y:y + h // 2, x:x + w // 2] = square\n",
        "    movie.append(frame)\n",
        "  return (np.array(movie) * 255.0).astype(np.uint8)\n",
        "\n",
        "noise_video = create_noise_movie(64, 512, 512)\n",
        "media.show_video(noise_video, fps=16, height=128, codec='gif')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RV1tHjRfSzu0"
      },
      "outputs": [],
      "source": [
        "# @title Combine qualitative videos\n",
        "\n",
        "videos = davis_videos + moca_videos + [dalmatian_video, noise_video]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Visualization functions\n",
        "\n",
        "def visualize_pca(features, video):\n",
        "  pca = sklearn.decomposition.PCA(n_components=3, whiten=True)\n",
        "  pca_data = pca.fit_transform(einops.rearrange(features, 't n m c -> (t n m) c'))\n",
        "  pca_data = pca_data.reshape(features.shape[:-1] + (3,))\n",
        "  pca_video = (pca_data - pca_data.min()) / (pca_data.max() - pca_data.min())\n",
        "  pca_video = jax.image.resize(pca_video, video.shape, method='nearest')\n",
        "  return pca_video\n",
        "\n",
        "def segmentations_to_video(masks):\n",
        "  num_objects = np.max(masks)  # assume consecutive numbering\n",
        "  # palette = [(0, 0, 0)] + sns.color_palette(n_colors=num_objects)\n",
        "  palette = sns.color_palette(n_colors=num_objects + 1)\n",
        "  video = np.zeros((masks.shape[0], masks.shape[1], masks.shape[2], 3))\n",
        "  for i in range(num_objects + 1):\n",
        "    video[masks == i] = palette[i]\n",
        "  return video\n",
        "\n",
        "def visualize_kmeans(features, video, n_clusters=5):\n",
        "  kmeans = sklearn.cluster.KMeans(n_clusters, init='k-means++')\n",
        "  result = kmeans.fit(einops.rearrange(features, 't n m c -> (t n m) c'))\n",
        "  kmeans_labels = jnp.reshape(result.labels_, features.shape[:-1])\n",
        "  kmeans_video = segmentations_to_video(kmeans_labels)\n",
        "  kmeans_video = jax.image.resize(kmeans_video, video.shape, method='nearest')\n",
        "  return kmeans_video"
      ],
      "metadata": {
        "id": "cisN-wkIJAPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Flax Model"
      ],
      "metadata": {
        "id": "6ldIQcBBT84n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define model {form-width: \"20%\"}\n",
        "\n",
        "import dataclasses\n",
        "import re\n",
        "from typing import Any\n",
        "import jax.numpy as jnp\n",
        "from flax import linen as nn\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "  \"\"\"Extracts patches with a single learned linear projection.\"\"\"\n",
        "  patch_size: list[int]\n",
        "  num_features: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, images):\n",
        "    return nn.Conv(features=self.num_features, kernel_size=self.patch_size, strides=self.patch_size, padding='VALID')(images)\n",
        "\n",
        "def get_mae_sinusoid_encoding_table(n_position, d_hid, dtype=jnp.float32):\n",
        "  \"\"\"Sinusoid positional encoding table for MAE.\"\"\"\n",
        "  def get_position_angle_vec(position):\n",
        "    return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]\n",
        "\n",
        "  sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n",
        "  sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
        "  sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
        "\n",
        "  return jnp.asarray(sinusoid_table, dtype)[None, ...]\n",
        "\n",
        "class SincosPosEmb(nn.Module):\n",
        "  \"\"\"Returns sinusoidal positional embedding given the shape of the tokens.\"\"\"\n",
        "  base_token_shape: list[int] | None = None\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, tokens_shape):\n",
        "    d = tokens_shape[-1]\n",
        "    if self.base_token_shape is not None:\n",
        "      h, w = self.base_token_shape\n",
        "    else:\n",
        "      h, w = tokens_shape[-3], tokens_shape[-2]\n",
        "\n",
        "    posenc = get_mae_sinusoid_encoding_table(np.prod((h, w)), d)\n",
        "    posenc = einops.rearrange(posenc, '... (h w) d -> ... h w d', h=h, w=w)\n",
        "    *b, tokens_h, tokens_w, _ = tokens_shape\n",
        "    for _ in range(len(b)-1):\n",
        "      posenc = jnp.expand_dims(posenc, axis=0)\n",
        "    if tokens_h != h or tokens_w != w:\n",
        "      posenc = jax.image.resize(posenc, (*b, tokens_h, tokens_w, d), method='bicubic')\n",
        "\n",
        "    return posenc\n",
        "\n",
        "class Tokenizer(nn.Module):\n",
        "  \"\"\"Simple tokenizer.\"\"\"\n",
        "  patch_embedding: nn.Module\n",
        "  posenc: nn.Module\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, image):\n",
        "    tokens = self.patch_embedding(image)\n",
        "    posenc = self.posenc(tokens.shape)\n",
        "    tokens += posenc\n",
        "    return tokens\n",
        "\n",
        "class TransformerMLP(nn.Module):\n",
        "  \"\"\"Simple MLP with a single hidden layer for use in Transformer blocks.\"\"\"\n",
        "  hidden_size: int = None  # Defaults to 4 times input dims.\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs):\n",
        "    d = inputs.shape[-1]\n",
        "    hidden_size = 4 * d if self.hidden_size is None else self.hidden_size\n",
        "    h = nn.Dense(\n",
        "        features=hidden_size,\n",
        "        name='dense_in',\n",
        "        kernel_init=nn.initializers.xavier_uniform(),\n",
        "        bias_init=nn.initializers.zeros,\n",
        "        dtype=inputs.dtype,\n",
        "    )(inputs)\n",
        "    h = nn.gelu(h)\n",
        "    return nn.Dense(\n",
        "        features=inputs.shape[-1],\n",
        "        name='dense_out',\n",
        "        kernel_init=nn.initializers.xavier_uniform(),\n",
        "        bias_init=nn.initializers.zeros,\n",
        "        dtype=h.dtype,\n",
        "    )(h)\n",
        "\n",
        "class PreNormBlock(nn.Module):\n",
        "  \"\"\"Pre-LN Transformer layer (default transformer layer).\"\"\"\n",
        "\n",
        "  attention: Any\n",
        "  mlp: nn.Module\n",
        "  attention_norm: Any\n",
        "  mlp_norm: Any\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, tokens):\n",
        "    norm_tokens = self.attention_norm(tokens)\n",
        "    tokens += self.attention(\n",
        "        inputs_q=norm_tokens,\n",
        "        inputs_k=norm_tokens,\n",
        "        inputs_v=norm_tokens,\n",
        "    )\n",
        "    norm_tokens = self.mlp_norm(tokens)\n",
        "    return tokens + self.mlp(norm_tokens)\n",
        "\n",
        "VIT_SIZES = {\n",
        "    'mu': (32, 1, 128, 2),\n",
        "    'Ti': (192, 12, 768, 3),\n",
        "    'S': (384, 12, 1536, 6),\n",
        "    'M': (512, 12, 2048, 8),\n",
        "    'B': (768, 12, 3072, 12),\n",
        "    'L': (1024, 24, 4096, 16),\n",
        "    'H': (1280, 32, 5120, 16),\n",
        "    'g': (1408, 40, 6144, 16),\n",
        "    'G': (1664, 48, 8192, 16),\n",
        "    'e': (1792, 56, 15360, 16),\n",
        "}\n",
        "\n",
        "@dataclasses.dataclass(frozen=True)\n",
        "class ViTSpec:\n",
        "  \"\"\"Spec for the size of a Vision Transformer.\"\"\"\n",
        "\n",
        "  hidden_size: int  # Dimension of tokens passed between blocks.\n",
        "  num_layers: int  # Number of trasformer blocks.\n",
        "  mlp_size: int  # Hidden dimension of the MLP in each block.\n",
        "  num_heads: int  # Number of attention heads.\n",
        "  patch_size: int = None  # Patch size of initial image patches.\n",
        "\n",
        "  @classmethod\n",
        "  def from_variant_string(cls, variant_str: str):\n",
        "    \"\"\"Parse variant strings like \"ViT-L\", \"B\", or \"Ti/16\".\"\"\"\n",
        "    r = re.match(\n",
        "        r'^([Vv][Ii][Tt][-_])?(?P<name>[a-zA-Z]{1,2})(/(?P<patch>\\d+))?$',\n",
        "        variant_str,\n",
        "    )\n",
        "    if r is None:\n",
        "      raise ValueError(f'Invalid variant string: {variant_str!r}.')\n",
        "    name = r.groupdict()['name']\n",
        "    spec = cls(*VIT_SIZES[name])\n",
        "\n",
        "    patch_size = r.groupdict()['patch']\n",
        "    if patch_size is not None:\n",
        "      spec = dataclasses.replace(spec, patch_size=int(patch_size))\n",
        "    return spec\n",
        "\n",
        "  @property\n",
        "  def kwargs(self):\n",
        "    kwargs = dict(\n",
        "        hidden_size=self.hidden_size,\n",
        "        num_layers=self.num_layers,\n",
        "        mlp_size=self.mlp_size,\n",
        "        num_heads=self.num_heads,\n",
        "        patch_size=self.patch_size,\n",
        "    )\n",
        "    if self.patch_size is None:\n",
        "      del kwargs['patch_size']\n",
        "    return kwargs\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  \"\"\"Simple transformer model.\"\"\"\n",
        "  layers: tuple[Any]\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, tokens):\n",
        "    for layer in self.layers:\n",
        "      tokens = layer(tokens)\n",
        "    tokens = nn.LayerNorm(dtype=tokens.dtype)(tokens)\n",
        "    return tokens\n",
        "\n",
        "  @classmethod\n",
        "  def from_variant_str(cls, variant_str: str, **kwargs):\n",
        "    vit_spec = ViTSpec.from_variant_string(variant_str)\n",
        "    all_kwargs = vit_spec.kwargs | kwargs\n",
        "    all_kwargs.pop('patch_size', None)\n",
        "    all_kwargs.pop('hidden_size', None)\n",
        "    return cls.from_spec(**all_kwargs)\n",
        "\n",
        "  @classmethod\n",
        "  def from_spec(\n",
        "      cls,\n",
        "      num_heads: int,\n",
        "      num_layers: int,\n",
        "      mlp_size = None,\n",
        "      dtype=jnp.float32,\n",
        "      qk_features = None,\n",
        "      v_features = None,\n",
        "      **kwargs,\n",
        "  ):\n",
        "    return cls(\n",
        "        layers=tuple(\n",
        "            PreNormBlock(\n",
        "                attention_norm=nn.LayerNorm(dtype=dtype),\n",
        "                mlp_norm=nn.LayerNorm(dtype=dtype),\n",
        "                attention=ImprovedMultiHeadDotProductAttention(\n",
        "                    num_heads=num_heads,\n",
        "                    qk_features=qk_features,\n",
        "                    v_features=v_features,\n",
        "                ),\n",
        "                mlp=TransformerMLP(hidden_size=mlp_size),\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ),\n",
        "        **kwargs,\n",
        "    )\n",
        "\n",
        "class GatedTransformerCore(nn.Module):\n",
        "  transformer: nn.Module\n",
        "  initializer: nn.Module\n",
        "  token_dim: int\n",
        "  state_layer_norm: nn.Module\n",
        "\n",
        "  def setup(self):\n",
        "    self.input_update = nn.Dense(self.token_dim, use_bias=False)\n",
        "    self.input_reset = nn.Dense(self.token_dim, use_bias=False)\n",
        "    self.state_update = nn.Dense(self.token_dim, use_bias=False)\n",
        "    self.state_reset = nn.Dense(self.token_dim, use_bias=False)\n",
        "\n",
        "  def __call__(self, inputs, state):\n",
        "    update_gate = jax.nn.sigmoid(self.input_update(inputs) + self.state_update(state))\n",
        "    reset_gate = jax.nn.sigmoid(self.input_reset(inputs) + self.state_reset(state))\n",
        "    h = self.transformer(inputs, inputs_kv=reset_gate * self.state_layer_norm(state))\n",
        "    output = (1-update_gate)*state + update_gate * h\n",
        "    state = output\n",
        "    return output, state\n",
        "\n",
        "def softmax(x):\n",
        "  return jax.nn.softmax(x.astype(jnp.float32), axis=-1).astype(jnp.float32)\n",
        "\n",
        "def dot_product_attention_weights(query, key):\n",
        "  query = query / jnp.sqrt(query.shape[-1])\n",
        "  attn_weights = jnp.einsum('...qhd,...khd->...hqk', query, key)\n",
        "  return softmax(attn_weights)\n",
        "\n",
        "class ImprovedMultiHeadDotProductAttention(nn.Module):\n",
        "  \"\"\"Multi-head dot-product attention.\"\"\"\n",
        "\n",
        "  num_heads: int\n",
        "  qk_features: int = None\n",
        "  v_features: int = None\n",
        "  out_features: int = None\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(\n",
        "      self,\n",
        "      inputs_q,\n",
        "      inputs_k = None,\n",
        "      inputs_v = None,\n",
        "      *,\n",
        "      bias = None,\n",
        "      mask = None,\n",
        "  ):\n",
        "    qk_features = self.qk_features or inputs_q.shape[-1]\n",
        "    v_features = self.v_features or qk_features\n",
        "\n",
        "    if inputs_k is None:\n",
        "      inputs_k = inputs_q\n",
        "    if inputs_v is None:\n",
        "      inputs_v = inputs_k\n",
        "\n",
        "    def dense(name, x, features):\n",
        "      return nn.DenseGeneral(\n",
        "          features=(self.num_heads, features // self.num_heads),\n",
        "          kernel_init=nn.initializers.lecun_normal(),\n",
        "          bias_init=nn.initializers.zeros_init(),\n",
        "          use_bias=True,\n",
        "          dtype=x.dtype,\n",
        "          name=name,\n",
        "      )(x)\n",
        "\n",
        "    query = dense('query', inputs_q, qk_features)\n",
        "    key = dense('key', inputs_k, qk_features)\n",
        "    value = dense('value', inputs_v, v_features)\n",
        "\n",
        "    # Compute attention weights.\n",
        "    attn_weights = dot_product_attention_weights(query=query, key=key)\n",
        "\n",
        "    # Return weighted sum over values for each query position.\n",
        "    x = jnp.einsum('...hqk,...khd->...qhd', attn_weights, value)\n",
        "\n",
        "    # Back to the original input dimensions.\n",
        "    return nn.DenseGeneral(\n",
        "        features=self.out_features or inputs_q.shape[-1],\n",
        "        axis=(-2, -1),\n",
        "        kernel_init=nn.initializers.lecun_normal(),\n",
        "        bias_init=nn.initializers.zeros_init(),\n",
        "        use_bias=True,\n",
        "        dtype=x.dtype,\n",
        "        name='out',\n",
        "    )(x)\n",
        "\n",
        "class CrossAttentionTransformer(nn.Module):\n",
        "  \"\"\"Cross attention transformer.\"\"\"\n",
        "  num_heads: int\n",
        "  num_layers: int\n",
        "  num_feats: int\n",
        "  mlp_dim: int\n",
        "  dtype: Any\n",
        "\n",
        "  def setup(self):\n",
        "    self.xa_blocks = [CrossAttentionBlock(\n",
        "        num_heads=self.num_heads, num_feats=self.num_feats,\n",
        "        mlp_dim=self.mlp_dim, dtype=self.dtype,\n",
        "    ) for _ in range(self.num_layers)]\n",
        "    self.output_norm = nn.LayerNorm(dtype=self.dtype)\n",
        "\n",
        "  def __call__(self, inputs, inputs_kv):\n",
        "    for i in range(self.num_layers):\n",
        "      inputs = self.xa_blocks[i](inputs, inputs_kv)\n",
        "    return self.output_norm(inputs)\n",
        "\n",
        "class CrossAttentionBlock(nn.Module):\n",
        "  \"\"\"Cross attention block.\"\"\"\n",
        "\n",
        "  num_heads: int\n",
        "  num_feats: int\n",
        "  mlp_dim: int\n",
        "  dtype: Any\n",
        "\n",
        "  def setup(self):\n",
        "    self.attention_norm = nn.LayerNorm(dtype=self.dtype)\n",
        "    self.mlp_norm = nn.LayerNorm(dtype=self.dtype)\n",
        "    self.ca_attention_norm = nn.LayerNorm(dtype=self.dtype)\n",
        "    self.attention = ImprovedMultiHeadDotProductAttention(\n",
        "        num_heads=self.num_heads,\n",
        "        qk_features=self.num_feats,\n",
        "        v_features=self.num_feats,\n",
        "    )\n",
        "    self.ca_attention = ImprovedMultiHeadDotProductAttention(\n",
        "        num_heads=self.num_heads,\n",
        "        qk_features=self.num_feats,\n",
        "        v_features=self.num_feats,\n",
        "    )\n",
        "    self.mlp = TransformerMLP(hidden_size=self.mlp_dim)\n",
        "\n",
        "  def __call__(self, inputs, inputs_kv):\n",
        "    x = inputs\n",
        "    x = x + self.ca_attention(inputs_q=self.ca_attention_norm(x), inputs_k=inputs_kv, inputs_v=inputs_kv)\n",
        "    x = x + self.mlp(self.mlp_norm(x))\n",
        "    x = x + self.attention(self.attention_norm(x))\n",
        "    return x\n",
        "\n",
        "class RandomStateInit(nn.Module):\n",
        "  \"\"\"Random, non-learnable state initialization.\"\"\"\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs, batch_shape):\n",
        "    shape = inputs.shape[-2:]\n",
        "    state = 0 * jax.random.normal(key=self.make_rng(\"default\"), shape=batch_shape + shape)\n",
        "    return state\n",
        "\n",
        "class VideoSiamMAE(nn.Module):\n",
        "  \"\"\"Video Siamese masked autoencoder model.\"\"\"\n",
        "\n",
        "  tokenizer: nn.Module\n",
        "  encoder: nn.Module\n",
        "  rnn_core: nn.Module\n",
        "  latent_emb_dim: int = 384\n",
        "\n",
        "  def setup(self):\n",
        "    self.cls_token = self.param('cls_token', nn.initializers.normal(stddev=0.02), (1, self.latent_emb_dim))\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, frame, state = None):\n",
        "    frame_tokens = self.tokenizer(frame)\n",
        "    frame_tokens = einops.rearrange(frame_tokens, '... h w D -> ... (h w) D')\n",
        "\n",
        "    *b, _, _ = frame_tokens.shape\n",
        "    cls_token = jnp.broadcast_to(self.cls_token, b + [1, self.cls_token.shape[-1]])\n",
        "    frame_tokens = jnp.concatenate([cls_token, frame_tokens], axis=-2)\n",
        "\n",
        "    encoded_frame_tokens = self.encoder(frame_tokens)\n",
        "    if state is None:\n",
        "      state = self.rnn_core.initializer(encoded_frame_tokens, batch_shape=(1,))\n",
        "    features, state = self.rnn_core(encoded_frame_tokens, state)\n",
        "\n",
        "    return dict(features=features, state=state)\n",
        "\n",
        "model = VideoSiamMAE(\n",
        "    tokenizer=Tokenizer(\n",
        "        patch_embedding=PatchEmbedding(patch_size=[1, 16, 16], num_features=1024),\n",
        "        posenc=SincosPosEmb(base_token_shape=[16, 16]),\n",
        "    ),\n",
        "    encoder=Transformer.from_variant_str(variant_str='L', dtype=jax.numpy.bfloat16),\n",
        "    rnn_core=GatedTransformerCore(\n",
        "        transformer=CrossAttentionTransformer(\n",
        "            num_layers=4,\n",
        "            num_heads=16,\n",
        "            num_feats=1024,\n",
        "            mlp_dim=4096,\n",
        "            dtype=jax.numpy.bfloat16,\n",
        "        ),\n",
        "        initializer=RandomStateInit(),\n",
        "        token_dim=1024,\n",
        "        state_layer_norm=nn.LayerNorm(epsilon=0.0001, use_scale=True, use_bias=False),\n",
        "    ),\n",
        "    latent_emb_dim=1024,\n",
        ")"
      ],
      "metadata": {
        "id": "nDchT_aY_wCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load checkpoint\n",
        "\n",
        "%cd /content/rvm\n",
        "\n",
        "def recover_tree(flat_dict):\n",
        "  tree = {}\n",
        "  for k, v in flat_dict.items():\n",
        "    parts = k.split(\"/\")\n",
        "    node = tree\n",
        "    for part in parts[:-1]:\n",
        "      if part not in node:\n",
        "        node[part] = {}\n",
        "      node = node[part]\n",
        "    node[parts[-1]] = v\n",
        "  return tree\n",
        "\n",
        "restored_params = recover_tree(np.load(\"pretrain_rvm_large16_256_175558463.npz\", allow_pickle=False))"
      ],
      "metadata": {
        "id": "3e3JCNU1_uVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Feature extraction function\n",
        "\n",
        "PATCH_SIZE = 16\n",
        "\n",
        "def extract_features(model, params, video):\n",
        "  video = video.astype(np.float32) / 255.0\n",
        "  h, w = video.shape[1] // PATCH_SIZE, video.shape[2] // PATCH_SIZE  # feature resolution\n",
        "\n",
        "  @jax.jit\n",
        "  def forward(params, frame, model_state, rng_key):\n",
        "    output = model.apply(\n",
        "        {'params': params},\n",
        "        frame,  # [B, H, W, 3]\n",
        "        model_state,  # [B, N, D]\n",
        "        capture_intermediates=False,\n",
        "        rngs=rng_key,\n",
        "    )\n",
        "    return output\n",
        "\n",
        "  rng_key = jax.random.PRNGKey(0)\n",
        "  model_state = None\n",
        "  features = []\n",
        "  for t in range(video.shape[0]):\n",
        "    output = forward(params, video[t][None], model_state, rng_key)\n",
        "    model_state, feature, cls_token = output['state'], output['features'][0, 1:, :], output['features'][0, 0:1, :]\n",
        "    feature = feature.reshape(h, w, -1)\n",
        "    features.append(feature)\n",
        "  features = np.stack(features, axis=0)\n",
        "  return features"
      ],
      "metadata": {
        "id": "r7OWo9dZ_xxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Label propagation functions (jax)\n",
        "\n",
        "def draw_labelmap_np(img, pt, sigma=0.5):\n",
        "  # Draw a 2D gaussian\n",
        "  # Adopted from https://github.com/anewell/pose-hg-train/blob/master/src/pypose/draw.py\n",
        "\n",
        "  # Check that any part of the gaussian is in-bounds\n",
        "  ul = [int(pt[0] - 3 * sigma), int(pt[1] - 3 * sigma)]\n",
        "  br = [int(pt[0] + 3 * sigma + 1), int(pt[1] + 3 * sigma + 1)]\n",
        "  if (ul[0] >= img.shape[1] or ul[1] >= img.shape[0] or br[0] < 0 or br[1] < 0):\n",
        "    # If not, just return the image as is\n",
        "    return img\n",
        "\n",
        "  # Generate gaussian\n",
        "  size = 6 * sigma + 1\n",
        "  x = np.arange(0, size, 1, float)\n",
        "  y = x[:, np.newaxis]\n",
        "  x0 = y0 = size // 2\n",
        "  g = np.exp(- ((x - x0) ** 2 + (y - y0) ** 2) / (2 * sigma ** 2))\n",
        "\n",
        "  # Usable gaussian range\n",
        "  g_x = max(0, -ul[0]), min(br[0], img.shape[1]) - ul[0]\n",
        "  g_y = max(0, -ul[1]), min(br[1], img.shape[0]) - ul[1]\n",
        "  # Image range\n",
        "  img_x = max(0, ul[0]), min(br[0], img.shape[1])\n",
        "  img_y = max(0, ul[1]), min(br[1], img.shape[0])\n",
        "\n",
        "  img[img_y[0]:img_y[1], img_x[0]:img_x[1]] = g[g_y[0]:g_y[1], g_x[0]:g_x[1]]\n",
        "  return img\n",
        "\n",
        "def mask2heatmap(mask, h, w, height, width):\n",
        "  \"\"\"Convert segmentation mask to heatmap (resize and one-hot encode)\"\"\"\n",
        "  label_map = np.unique(mask)\n",
        "  mask = cv2.resize(mask, (width, height), interpolation=cv2.INTER_NEAREST)\n",
        "  heatmap = np.stack([mask == l for l in label_map], axis=-1).astype(np.float32)\n",
        "  heatmap = cv2.resize(heatmap, (w, h), interpolation=cv2.INTER_LINEAR)\n",
        "  return heatmap, label_map\n",
        "\n",
        "def pose2heatmap(pose, h, w, height, width):\n",
        "  \"\"\"Convert pose to heatmap (resize and one-hot encode)\"\"\"\n",
        "  n_class = pose.shape[0]\n",
        "  coord = pose * np.array([w / width, h / height])\n",
        "  heatmap = np.zeros((h, w, n_class + 1), dtype=np.float32)\n",
        "  for i in range(n_class):\n",
        "    heatmap[..., i + 1] = draw_labelmap_np(np.zeros((h, w)), coord[i])\n",
        "  heatmap[..., 0] = heatmap.sum(axis=-1) == 0\n",
        "  return heatmap\n",
        "\n",
        "def heatmap2pose(preds, h, w, ori_height, ori_width, topk=5):\n",
        "  \"\"\"Convert heatmap to pose (argmax and resize).\"\"\"\n",
        "  current_coords, jnt_visibles = [], []\n",
        "  for t in range(preds.shape[0]):\n",
        "    pred = preds[t][..., 1:]  # [h, w, n_class]\n",
        "    flatlbls = pred.reshape(-1, pred.shape[-1])  # [h * w, n_class]\n",
        "    # Get top k values and indices for each class\n",
        "    vals, ids = jax.lax.top_k(flatlbls.T, k=topk) # [n_class, k]\n",
        "    # Normalize values\n",
        "    vals = vals / vals.sum(1, keepdims=True)\n",
        "    # Calculate coordinates\n",
        "    xx, yy = ids % pred.shape[1], ids // pred.shape[1]\n",
        "    current_coord = np.stack([(xx * vals).sum(1), (yy * vals).sum(1)], axis=1) # [n_class, 2]\n",
        "    # Resize coordinates to original size\n",
        "    current_coord[:, 0] = current_coord[:, 0] / w * ori_width\n",
        "    current_coord[:, 1] = current_coord[:, 1] / h * ori_height\n",
        "    # Set invisible joints to -1\n",
        "    current_coord[flatlbls.sum(0) == 0] = -1\n",
        "    # Get visibility flag\n",
        "    jnt_visible = (current_coord[:, 0] >= 0).astype(float)\n",
        "    current_coords.append(current_coord)\n",
        "    jnt_visibles.append(jnt_visible)\n",
        "  current_coords = np.stack(current_coords, axis=0)\n",
        "  jnt_visibles = np.stack(jnt_visibles, axis=0)\n",
        "  return current_coords, jnt_visibles\n",
        "\n",
        "def heatmap2mask(preds, lbl_map, height, width, ori_height, ori_width):\n",
        "  \"\"\"Convert heatmap to segmentation mask (argmax and resize).\"\"\"\n",
        "  pred_lbls = []\n",
        "  for t in range(preds.shape[0]):\n",
        "    pred = np.array(preds[t])\n",
        "    # Upsample predicted soft label maps\n",
        "    pred = cv2.resize(pred, (width, height))\n",
        "    # Argmax to get the hard label for index\n",
        "    pred_lbl = np.argmax(pred, axis=-1)\n",
        "    pred_lbl = np.array(lbl_map, dtype=np.int32)[pred_lbl]\n",
        "    pred_lbl = cv2.resize(pred_lbl, (ori_width, ori_height), interpolation=cv2.INTER_NEAREST_EXACT)\n",
        "    pred_lbls.append(pred_lbl)\n",
        "  pred_lbls = np.stack(pred_lbls, axis=0)\n",
        "  return pred_lbls\n",
        "\n",
        "def label_propagation(feats, heatmap, n_context=20, temperature=0.7, topk=7, radius=20, restrict_neighborhood=True, norm_mask=False):\n",
        "  \"\"\"Propagation of the heatmap based on feature similarity.\"\"\"\n",
        "\n",
        "  # Creates a mask indicating valid neighbors for each grid element.\n",
        "  gx, gy = np.meshgrid(np.arange(0, h), np.arange(0, w), indexing=\"ij\")  # (h, w)\n",
        "  D = (gx[None, None, :, :] - gx[:, :, None, None])**2 + (gy[None, None, :, :] - gy[:, :, None, None])**2\n",
        "  D = D.astype(np.float32) ** 0.5\n",
        "  D = (D < radius).astype(np.float32)  # (h, w, h, w)\n",
        "  D[D == 0] = -1e10\n",
        "  D[D == 1] = 0\n",
        "  D = D.transpose(2, 3, 0, 1)  # (h, w, h, w)\n",
        "\n",
        "  # The queue stores the context frames\n",
        "  que = queue.Queue(n_context)\n",
        "  for _ in range(n_context):\n",
        "    que.put([feats[0], heatmap])\n",
        "\n",
        "  preds = []\n",
        "  for t in tqdm.tqdm(range(feats.shape[0])):\n",
        "    # Use first and previous frames as context\n",
        "    ctx_feats = jnp.stack([feats[0]] + [pair[0] for pair in que.queue])\n",
        "    ctx_lbls = jnp.stack([heatmap] + [pair[1] for pair in que.queue])\n",
        "\n",
        "    aff = jnp.einsum('hwc, tmnc -> hwtmn', feats[t], ctx_feats) / temperature  # (h, w, n_context+1, h, w)\n",
        "    if restrict_neighborhood:\n",
        "      # aff[:, :, 1:] += D[:, :, None]  # (h, w, n_context+1, h, w)\n",
        "      aff.at[:, :, 1:].add(D[:, :, None])  # (h, w, n_context+1, h, w)\n",
        "    aff = aff.reshape(aff.shape[0], aff.shape[1], -1)  # (h, w, n_context+1 * h * w)\n",
        "\n",
        "    weights, ids = jax.lax.top_k(aff, topk)  # (h, w, topk), (h, w, topk)\n",
        "    weights = jax.nn.softmax(weights, axis=-1)  # (h, w, topk)\n",
        "    ctx_lbls = ctx_lbls.reshape(-1, ctx_lbls.shape[-1])  # (n_context+1 * h * w, n_class)\n",
        "    pred = jnp.einsum('hwlk, hwl -> hwk', ctx_lbls[ids], weights) # (h, w, n_class)\n",
        "\n",
        "    if que.qsize() == n_context:\n",
        "      que.get()\n",
        "    que.put([feats[t], pred])\n",
        "\n",
        "    if norm_mask:\n",
        "      pred -= pred.min(-1)[0][..., None]\n",
        "      pred /= pred.max(-1)[0][..., None]\n",
        "\n",
        "    preds.append(pred)\n",
        "  preds = jnp.stack(preds)\n",
        "  return preds"
      ],
      "metadata": {
        "id": "X3RTV3XFeOfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title PCA visualization and k-means clustering {form-width: \"20%\"}\n",
        "\n",
        "for video in videos:\n",
        "  features = extract_features(model, restored_params, video)\n",
        "  print(features.shape)\n",
        "\n",
        "  pca_video = visualize_pca(features, video)\n",
        "  kmeans_video = visualize_kmeans(features, video)\n",
        "  mixed_video = 0.5 * video / 255.0 + 0.5 * kmeans_video\n",
        "  video_titles = ['pca', 'kmeans', 'mixed']\n",
        "  media.show_videos([pca_video, kmeans_video, mixed_video], titles=video_titles, height=128, codec='gif', fps=16, columns=3)"
      ],
      "metadata": {
        "id": "7NIgdC2L_zro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PyTorch Model"
      ],
      "metadata": {
        "id": "RM39D0ZXUAQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define model {form-width: \"20%\"}\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import einops\n",
        "import re\n",
        "import dataclasses\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "  def __init__(self, patch_size=(16, 16), num_features=1024):\n",
        "    super().__init__()\n",
        "    self.patch_size = patch_size\n",
        "    self.num_features = num_features\n",
        "    self.Conv_0 = nn.Conv2d(in_channels=3, out_channels=num_features, kernel_size=patch_size, stride=patch_size, padding=0)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.permute(0, 3, 1, 2)\n",
        "    return self.Conv_0(x).permute(0, 2, 3, 1)\n",
        "\n",
        "def get_mae_sinusoid_encoding_table(n_position, d_hid, dtype=torch.float32):\n",
        "  \"\"\"Sinusoid positional encoding table for MAE.\"\"\"\n",
        "  def get_position_angle_vec(position):\n",
        "    return [position / math.pow(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]\n",
        "\n",
        "  sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n",
        "  sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
        "  sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
        "\n",
        "  return torch.tensor(sinusoid_table, dtype=dtype)[None, ...]\n",
        "\n",
        "class SincosPosEmb(nn.Module):\n",
        "  \"\"\"Returns sinusoidal positional embedding given the shape of the tokens.\"\"\"\n",
        "  def __init__(self, base_token_shape=None, use_jax_interpolation=False):\n",
        "    super().__init__()\n",
        "    self.base_token_shape = base_token_shape\n",
        "    self.use_jax_interpolation = use_jax_interpolation\n",
        "\n",
        "  def forward(self, tokens_shape):\n",
        "    d = tokens_shape[-1]\n",
        "    if self.base_token_shape is not None:\n",
        "      h, w = self.base_token_shape\n",
        "    else:\n",
        "      h, w = tokens_shape[-3], tokens_shape[-2]\n",
        "\n",
        "    posenc = get_mae_sinusoid_encoding_table(h * w, d)  # [1, h*w, d]\n",
        "    posenc = posenc.view(1, h, w, d)  # [1, h, w, d]\n",
        "\n",
        "    *b, tokens_h, tokens_w, _ = tokens_shape\n",
        "    for _ in range(len(b)-1):\n",
        "      posenc = posenc.expand(*b, -1, -1, -1)\n",
        "\n",
        "    if tokens_h != h or tokens_w != w:\n",
        "      if self.use_jax_interpolation:\n",
        "        posenc = jnp.array(posenc.numpy())\n",
        "        posenc = jax.image.resize(posenc, (*b, tokens_h, tokens_w, d), method='bicubic')\n",
        "        posenc = torch.from_numpy(np.array(posenc))\n",
        "      else:\n",
        "        posenc = posenc.view(-1, h, w, d)\n",
        "        posenc = F.interpolate(\n",
        "          posenc.permute(0, 3, 1, 2),  # [B, D, H, W]\n",
        "          size=(tokens_h, tokens_w),\n",
        "          mode='bicubic',\n",
        "          align_corners=False\n",
        "        ).permute(0, 2, 3, 1)  # [B, H, W, D]\n",
        "        posenc = posenc.view(*b, tokens_h, tokens_w, d)\n",
        "\n",
        "    return posenc.cuda()\n",
        "\n",
        "class Tokenizer(nn.Module):\n",
        "  def __init__(self, patch_embedding, posenc):\n",
        "    super().__init__()\n",
        "    self.patch_embedding = patch_embedding\n",
        "    self.posenc = posenc\n",
        "\n",
        "  def forward(self, x):\n",
        "    tokens = self.patch_embedding(x)\n",
        "    # posenc = self.posenc(tokens.shape)\n",
        "    # tokens += posenc\n",
        "    return tokens\n",
        "\n",
        "class TransformerMLP(nn.Module):\n",
        "  \"\"\"Simple MLP with a single hidden layer for use in Transformer blocks.\"\"\"\n",
        "  def __init__(self, input_dim, hidden_size=None):\n",
        "    super().__init__()\n",
        "    self.hidden_size = 4 * input_dim if hidden_size is None else hidden_size\n",
        "    self.dense_in = nn.Linear(input_dim, self.hidden_size)\n",
        "    self.dense_out = nn.Linear(self.hidden_size, input_dim)\n",
        "    nn.init.xavier_uniform_(self.dense_in.weight)\n",
        "    nn.init.zeros_(self.dense_in.bias)\n",
        "    nn.init.xavier_uniform_(self.dense_out.weight)\n",
        "    nn.init.zeros_(self.dense_out.bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    h = F.gelu(self.dense_in(x))\n",
        "    return self.dense_out(h)\n",
        "\n",
        "def dot_product_attention_weights(query, key):\n",
        "  query = query / math.sqrt(query.size(-1))\n",
        "  attn_weights = torch.einsum('bqhd,bkhd->bhqk', query, key)\n",
        "  attn_weights = F.softmax(attn_weights, dim=-1)\n",
        "  return attn_weights\n",
        "\n",
        "class ImprovedMultiHeadDotProductAttention(nn.Module):\n",
        "  def __init__(self, embed_dim, num_heads, qk_features=None, v_features=None, out_features=None):\n",
        "    super().__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.qk_features = qk_features or embed_dim\n",
        "    self.v_features = v_features or self.qk_features\n",
        "    self.out_features = out_features or embed_dim\n",
        "\n",
        "    # Head dimensions\n",
        "    self.head_dim_qk = self.qk_features // self.num_heads\n",
        "    self.head_dim_v = self.v_features // self.num_heads\n",
        "\n",
        "    # Linear projections\n",
        "    self.query = nn.Linear(embed_dim, self.qk_features)\n",
        "    self.key  = nn.Linear(embed_dim, self.qk_features)\n",
        "    self.value = nn.Linear(embed_dim, self.v_features)\n",
        "\n",
        "    # Output projection\n",
        "    self.out = nn.Linear(self.v_features, self.out_features)\n",
        "\n",
        "  def forward(self, inputs_q, inputs_k=None, inputs_v=None, mask=None):\n",
        "    batch_size, seq_len_q, _ = inputs_q.shape\n",
        "    if inputs_k is None:\n",
        "      inputs_k = inputs_q\n",
        "    if inputs_v is None:\n",
        "      inputs_v = inputs_k\n",
        "\n",
        "    seq_len_k = inputs_k.shape[1]\n",
        "\n",
        "    # Linear projections and reshape to (batch, seq_len, num_heads, head_dim)\n",
        "    query = self.query(inputs_q).view(batch_size, seq_len_q, self.num_heads, self.head_dim_qk)\n",
        "    key   = self.key(inputs_k).view(batch_size, seq_len_k, self.num_heads, self.head_dim_qk)\n",
        "    value = self.value(inputs_v).view(batch_size, seq_len_k, self.num_heads, self.head_dim_v)\n",
        "\n",
        "    # Scaled dot-product attention\n",
        "    query_scaled = query / math.sqrt(self.head_dim_qk)\n",
        "    attn_weights = torch.einsum('bqhd,bkhd->bhqk', query_scaled, key)\n",
        "    if mask is not None:\n",
        "      attn_weights = attn_weights.masked_fill(mask == 0, float('-inf'))\n",
        "    attn_weights = F.softmax(attn_weights, dim=-1)\n",
        "\n",
        "    # Weighted sum over values\n",
        "    x = torch.einsum('bhqk,bkhd->bqhd', attn_weights, value)\n",
        "    x = x.reshape(batch_size, seq_len_q, self.num_heads * self.head_dim_v)\n",
        "\n",
        "    # Output projection\n",
        "    out = self.out(x)\n",
        "    return out\n",
        "\n",
        "class PreNormBlock(nn.Module):\n",
        "  def __init__(self, attention_norm, mlp_norm, attention, mlp):\n",
        "    super().__init__()\n",
        "    self.attention_norm = attention_norm\n",
        "    self.mlp_norm = mlp_norm\n",
        "    self.attention = attention\n",
        "    self.mlp = mlp\n",
        "\n",
        "  def forward(self, x):\n",
        "    norm_x = self.attention_norm(x)\n",
        "    x = x + self.attention(norm_x)\n",
        "    norm_x = self.mlp_norm(x)\n",
        "    x = x + self.mlp(norm_x)\n",
        "    return x\n",
        "\n",
        "VIT_SIZES = {\n",
        "    'mu': (32, 1, 128, 2),\n",
        "    'Ti': (192, 12, 768, 3),\n",
        "    'S': (384, 12, 1536, 6),\n",
        "    'M': (512, 12, 2048, 8),\n",
        "    'B': (768, 12, 3072, 12),\n",
        "    'L': (1024, 24, 4096, 16),\n",
        "    'H': (1280, 32, 5120, 16),\n",
        "    'g': (1408, 40, 6144, 16),\n",
        "    'G': (1664, 48, 8192, 16),\n",
        "    'e': (1792, 56, 15360, 16),\n",
        "}\n",
        "\n",
        "@dataclasses.dataclass(frozen=True)\n",
        "class ViTSpec:\n",
        "  hidden_size: int\n",
        "  num_layers: int\n",
        "  mlp_size: int\n",
        "  num_heads: int\n",
        "  patch_size: int = None\n",
        "\n",
        "  @classmethod\n",
        "  def from_variant_string(cls, variant_str: str):\n",
        "    r = re.match(r'^([Vv][Ii][Tt][-_])?(?P<name>[a-zA-Z]{1,2})(/(?P<patch>\\d+))?$', variant_str)\n",
        "    if r is None:\n",
        "      raise ValueError(f'Invalid variant string: {variant_str!r}.')\n",
        "    name = r.groupdict()['name']\n",
        "    spec = cls(*VIT_SIZES[name])\n",
        "    patch_size = r.groupdict()['patch']\n",
        "    if patch_size is not None:\n",
        "      spec = dataclasses.replace(spec, patch_size=int(patch_size))\n",
        "    return spec\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, num_layers, hidden_size, num_heads, mlp_size, qk_features=None, v_features=None):\n",
        "    super().__init__()\n",
        "    self.layers = nn.ModuleList([\n",
        "        PreNormBlock(\n",
        "            attention_norm=nn.LayerNorm(hidden_size, eps=1e-06, dtype=torch.float32),\n",
        "            mlp_norm=nn.LayerNorm(hidden_size, eps=1e-06, dtype=torch.float32),\n",
        "            attention=ImprovedMultiHeadDotProductAttention(\n",
        "                embed_dim=hidden_size,\n",
        "                num_heads=num_heads,\n",
        "                qk_features=qk_features or hidden_size,\n",
        "                v_features=v_features or hidden_size,\n",
        "            ),\n",
        "            mlp=TransformerMLP(input_dim=hidden_size, hidden_size=mlp_size),\n",
        "        )\n",
        "        for _ in range(num_layers)\n",
        "    ])\n",
        "    self.LayerNorm_0 = nn.LayerNorm(hidden_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    return self.LayerNorm_0(x)\n",
        "\n",
        "  @classmethod\n",
        "  def from_variant_str(cls, variant_str: str, **kwargs):\n",
        "    spec = ViTSpec.from_variant_string(variant_str)\n",
        "    all_kwargs = dict(\n",
        "      num_layers=spec.num_layers,\n",
        "      hidden_size=spec.hidden_size,\n",
        "      mlp_size=spec.mlp_size,\n",
        "      num_heads=spec.num_heads,\n",
        "    )\n",
        "    all_kwargs.update(kwargs)\n",
        "    return cls(**all_kwargs)\n",
        "\n",
        "class CrossAttentionBlock(nn.Module):\n",
        "  def __init__(self, num_heads, num_feats, mlp_dim, dtype=torch.float32):\n",
        "    super().__init__()\n",
        "    self.attention_norm = nn.LayerNorm(num_feats, eps=1e-6, dtype=dtype)\n",
        "    self.mlp_norm = nn.LayerNorm(num_feats, eps=1e-6, dtype=dtype)\n",
        "    self.ca_attention_norm = nn.LayerNorm(num_feats, eps=1e-6, dtype=dtype)\n",
        "\n",
        "    self.attention = ImprovedMultiHeadDotProductAttention(\n",
        "      embed_dim=num_feats, num_heads=num_heads\n",
        "    )\n",
        "    self.ca_attention = ImprovedMultiHeadDotProductAttention(\n",
        "      embed_dim=num_feats, num_heads=num_heads\n",
        "    )\n",
        "    self.mlp = TransformerMLP(input_dim=num_feats, hidden_size=mlp_dim)\n",
        "\n",
        "  def forward(self, x, x_kv):\n",
        "    residual = x\n",
        "    x = x + self.ca_attention(inputs_q=self.ca_attention_norm(x), inputs_k=x_kv, inputs_v=x_kv)\n",
        "    x = x + self.mlp(self.mlp_norm(x))\n",
        "    x = x + self.attention(self.attention_norm(x))\n",
        "    return x\n",
        "\n",
        "class CrossAttentionTransformer(nn.Module):\n",
        "  def __init__(self, num_layers, num_heads, num_feats, mlp_dim, dtype=torch.float32):\n",
        "    super().__init__()\n",
        "    self.xa_blocks = nn.ModuleList([\n",
        "      CrossAttentionBlock(num_heads, num_feats, mlp_dim, dtype=dtype)\n",
        "      for _ in range(num_layers)\n",
        "    ])\n",
        "    self.output_norm = nn.LayerNorm(num_feats, eps=1e-6, dtype=dtype)\n",
        "\n",
        "  def forward(self, inputs, inputs_kv):\n",
        "    x = inputs\n",
        "    for block in self.xa_blocks:\n",
        "      x = block(x, inputs_kv)\n",
        "    return self.output_norm(x)\n",
        "\n",
        "class RandomStateInit(nn.Module):\n",
        "  \"\"\"Random, non-learnable state initialization.\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, inputs, batch_shape):\n",
        "    shape = inputs.shape[-2:]\n",
        "    state = 0 * torch.randn(batch_shape + shape, dtype=inputs.dtype, device=inputs.device)\n",
        "    return state\n",
        "\n",
        "class GatedTransformerCore(nn.Module):\n",
        "  def __init__(self, transformer, initializer, token_dim, state_layer_norm):\n",
        "    super().__init__()\n",
        "    self.transformer = transformer\n",
        "    self.initializer = initializer\n",
        "    self.token_dim = token_dim\n",
        "    self.state_layer_norm = state_layer_norm\n",
        "\n",
        "    self.input_update = nn.Linear(token_dim, token_dim, bias=False)\n",
        "    self.input_reset = nn.Linear(token_dim, token_dim, bias=False)\n",
        "    self.state_update = nn.Linear(token_dim, token_dim, bias=False)\n",
        "    self.state_reset = nn.Linear(token_dim, token_dim, bias=False)\n",
        "\n",
        "  def forward(self, inputs, state):\n",
        "    update_gate = F.sigmoid(self.input_update(inputs) + self.state_update(state))\n",
        "    reset_gate = F.sigmoid(self.input_reset(inputs) + self.state_reset(state))\n",
        "    h = self.transformer(inputs, inputs_kv=reset_gate * self.state_layer_norm(state))\n",
        "    output = (1 - update_gate) * state + update_gate * h\n",
        "    state = output\n",
        "    return output, state\n",
        "\n",
        "class VideoSiamMAE(nn.Module):\n",
        "  \"\"\"Video Siamese masked autoencoder model.\"\"\"\n",
        "\n",
        "  def __init__(self, tokenizer, encoder, rnn_core, latent_emb_dim=384):\n",
        "    super().__init__()\n",
        "    self.tokenizer = tokenizer\n",
        "    self.encoder = encoder\n",
        "    self.rnn_core = rnn_core\n",
        "    self.latent_emb_dim = latent_emb_dim\n",
        "\n",
        "    # cls_token is a learnable parameter\n",
        "    self.cls_token = nn.Parameter(torch.randn(1, 1, latent_emb_dim) * 0.02)\n",
        "\n",
        "  def forward(self, frame, state=None):\n",
        "    # Tokenize input frame\n",
        "    frame_tokens = self.tokenizer(frame)  # shape [..., h, w, D] expected\n",
        "    frame_tokens = einops.rearrange(frame_tokens, '... h w d -> ... (h w) d')\n",
        "\n",
        "    *b, _, _ = frame_tokens.shape\n",
        "    # Broadcast cls_token across batch\n",
        "    cls_token = self.cls_token.expand(*b, -1, -1)  # shape [..., 1, D]\n",
        "\n",
        "    # Concatenate CLS with patch tokens\n",
        "    frame_tokens = torch.cat([cls_token, frame_tokens], dim=-2)\n",
        "\n",
        "    # Encode with transformer encoder\n",
        "    encoded_frame_tokens = self.encoder(frame_tokens)\n",
        "\n",
        "    # Initialize state if first step\n",
        "    if state is None:\n",
        "        # Expect initializer to accept (inputs, batch_shape)\n",
        "        state = self.rnn_core.initializer(encoded_frame_tokens, batch_shape=(1,))\n",
        "\n",
        "    # Recurrent core update\n",
        "    features, state = self.rnn_core(encoded_frame_tokens, state)\n",
        "\n",
        "    return features, state\n",
        "\n",
        "model = VideoSiamMAE(\n",
        "    tokenizer=Tokenizer(\n",
        "        patch_embedding=PatchEmbedding(patch_size=[16, 16], num_features=1024),\n",
        "        posenc=SincosPosEmb(base_token_shape=[16, 16]),\n",
        "    ),\n",
        "    encoder=Transformer.from_variant_str(variant_str='L'),\n",
        "    rnn_core=GatedTransformerCore(\n",
        "        transformer=CrossAttentionTransformer(\n",
        "            num_layers=4,\n",
        "            num_heads=16,\n",
        "            num_feats=1024,\n",
        "            mlp_dim=4096,\n",
        "            dtype=torch.float32,\n",
        "        ),\n",
        "        initializer=RandomStateInit(),\n",
        "        token_dim=1024,\n",
        "        state_layer_norm=nn.LayerNorm(1024, eps=0.0001, bias=False),\n",
        "    ),\n",
        "    latent_emb_dim=1024,\n",
        ")\n",
        "model = model.cuda()\n",
        "model = model.eval()\n",
        "torch.set_grad_enabled(False)"
      ],
      "metadata": {
        "id": "Joe_zq7BUCk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load checkpoint\n",
        "\n",
        "%cd /content/rvm\n",
        "\n",
        "def recover_tree(flat_dict):\n",
        "  tree = {}\n",
        "  for k, v in flat_dict.items():\n",
        "    parts = k.split(\"/\")\n",
        "    node = tree\n",
        "    for part in parts[:-1]:\n",
        "      if part not in node:\n",
        "        node[part] = {}\n",
        "      node = node[part]\n",
        "    node[parts[-1]] = v\n",
        "  return tree\n",
        "\n",
        "def flatten_flax_params(params, parent_key=\"\"):\n",
        "  \"\"\"\n",
        "  Flatten nested Flax params dict into {'a.b.c': subdict}.\n",
        "  \"\"\"\n",
        "  items = {}\n",
        "  for k, v in params.items():\n",
        "    new_key = f\"{parent_key}.{k}\" if parent_key else k\n",
        "    if isinstance(v, dict):\n",
        "      items.update(flatten_flax_params(v, new_key))\n",
        "    else:\n",
        "      items[new_key] = v\n",
        "  return items\n",
        "\n",
        "def flax_to_torch(flat_flax, torch_model):\n",
        "  for name, param in torch_model.named_parameters():\n",
        "    # Normalize naming\n",
        "    name_fixed = name.replace('layers.', 'layers_')\n",
        "    name_fixed = name_fixed.replace('blocks.', 'blocks_')\n",
        "\n",
        "    flax_key = None\n",
        "\n",
        "    if name == \"cls_token\":\n",
        "      flax_key = \"cls_token\"\n",
        "\n",
        "    elif name.endswith(\"weight\"):\n",
        "      # Try Linear/Conv kernels\n",
        "      flax_key = name_fixed.replace(\"weight\", \"kernel\")\n",
        "      if flax_key not in flat_flax:\n",
        "        # Try LayerNorm scale\n",
        "        flax_key = name_fixed.replace(\"weight\", \"scale\")\n",
        "\n",
        "    elif name.endswith(\"bias\"):\n",
        "      flax_key = name_fixed  # bias names usually match directly\n",
        "\n",
        "    if flax_key is None or flax_key not in flat_flax:\n",
        "      print(f\"[WARN] Missing weights for {name} (flax_key={flax_key})\")\n",
        "      continue\n",
        "\n",
        "    # Load array\n",
        "    array = np.array(flat_flax[flax_key])\n",
        "    tensor = torch.tensor(array)\n",
        "\n",
        "    # Handle Conv2d kernel\n",
        "    if param.ndim == 4:\n",
        "      # Flax: [H, W, in, out]  Torch: [out, in, H, W]\n",
        "      if tensor.ndim == 5 and tensor.shape[0] == 1:  # Sometimes an extra batch dim\n",
        "        tensor = tensor[0]\n",
        "      tensor = tensor.permute(3, 2, 0, 1)\n",
        "\n",
        "    # Handle Linear kernels\n",
        "    elif param.ndim == 2:\n",
        "      if tensor.ndim == 2:\n",
        "        # Dense: [in, out]  [out, in]\n",
        "        tensor = tensor.T\n",
        "      elif tensor.ndim == 3:\n",
        "        # DenseGeneral\n",
        "        if param.shape[0] == tensor.shape[-1] * tensor.shape[-2]:  # Q/K/V projection\n",
        "          tensor = tensor.reshape(tensor.shape[0], -1).T\n",
        "        else:  # Output projection\n",
        "          tensor = tensor.reshape(-1, tensor.shape[-1]).T\n",
        "      else:\n",
        "        raise ValueError(f\"Unexpected kernel shape {tensor.shape} for {name}\")\n",
        "\n",
        "    # Reshape if needed (bias, cls_token, norm, etc.)\n",
        "    tensor = tensor.reshape(param.shape)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      param.copy_(tensor)\n",
        "\n",
        "    print(f\"Loaded {name} from {flax_key}\")\n",
        "\n",
        "restored_params = recover_tree(np.load(\"pretrain_rvm_large16_256_175558463.npz\", allow_pickle=False))\n",
        "\n",
        "flat_flax = flatten_flax_params(restored_params)\n",
        "flax_to_torch(flat_flax, model)"
      ],
      "metadata": {
        "id": "69AgEM24VCWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Feature extraction function\n",
        "\n",
        "PATCH_SIZE = 16\n",
        "\n",
        "def extract_features(model, video):\n",
        "  video = video.astype(np.float32) / 255.0\n",
        "  video = torch.as_tensor(video)\n",
        "  h, w = video.shape[1] // PATCH_SIZE, video.shape[2] // PATCH_SIZE  # feature resolution\n",
        "\n",
        "  model_state = None\n",
        "  features = []\n",
        "  for t in range(video.shape[0]):\n",
        "    feature, model_state = model(video[t][None], model_state)\n",
        "    feature, cls_token =feature[0, 1:, :], feature[0, 0:1, :]\n",
        "    feature = feature.reshape(h, w, -1)\n",
        "    feature = feature.detach().numpy()\n",
        "    features.append(feature)\n",
        "  features = np.stack(features, axis=0)\n",
        "  return features"
      ],
      "metadata": {
        "id": "0HGrqI6ZVH8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AL_yz16bVOzq"
      },
      "outputs": [],
      "source": [
        "# @title Label propagation functions\n",
        "\n",
        "def draw_labelmap_np(img, pt, sigma=0.5):\n",
        "  # Draw a 2D gaussian\n",
        "  # Adopted from https://github.com/anewell/pose-hg-train/blob/master/src/pypose/draw.py\n",
        "\n",
        "  # Check that any part of the gaussian is in-bounds\n",
        "  ul = [int(pt[0] - 3 * sigma), int(pt[1] - 3 * sigma)]\n",
        "  br = [int(pt[0] + 3 * sigma + 1), int(pt[1] + 3 * sigma + 1)]\n",
        "  if (ul[0] >= img.shape[1] or ul[1] >= img.shape[0] or br[0] < 0 or br[1] < 0):\n",
        "    # If not, just return the image as is\n",
        "    return img\n",
        "\n",
        "  # Generate gaussian\n",
        "  size = 6 * sigma + 1\n",
        "  x = np.arange(0, size, 1, float)\n",
        "  y = x[:, np.newaxis]\n",
        "  x0 = y0 = size // 2\n",
        "  g = np.exp(- ((x - x0) ** 2 + (y - y0) ** 2) / (2 * sigma ** 2))\n",
        "\n",
        "  # Usable gaussian range\n",
        "  g_x = max(0, -ul[0]), min(br[0], img.shape[1]) - ul[0]\n",
        "  g_y = max(0, -ul[1]), min(br[1], img.shape[0]) - ul[1]\n",
        "  # Image range\n",
        "  img_x = max(0, ul[0]), min(br[0], img.shape[1])\n",
        "  img_y = max(0, ul[1]), min(br[1], img.shape[0])\n",
        "\n",
        "  img[img_y[0]:img_y[1], img_x[0]:img_x[1]] = g[g_y[0]:g_y[1], g_x[0]:g_x[1]]\n",
        "  return img\n",
        "\n",
        "def mask2heatmap(mask, h, w, height, width):\n",
        "  \"\"\"Convert segmentation mask to heatmap (resize and one-hot encode)\"\"\"\n",
        "  label_map = np.unique(mask)\n",
        "  mask = cv2.resize(mask, (width, height), interpolation=cv2.INTER_NEAREST)\n",
        "  heatmap = np.stack([mask == l for l in label_map], axis=-1).astype(np.float32)\n",
        "  heatmap = cv2.resize(heatmap, (w, h), interpolation=cv2.INTER_LINEAR)\n",
        "  return heatmap, label_map\n",
        "\n",
        "def pose2heatmap(pose, h, w, height, width):\n",
        "  \"\"\"Convert pose to heatmap (resize and one-hot encode)\"\"\"\n",
        "  n_class = pose.shape[0]\n",
        "  coord = pose * np.array([w / width, h / height])\n",
        "  heatmap = np.zeros((h, w, n_class + 1), dtype=np.float32)\n",
        "  for i in range(n_class):\n",
        "    heatmap[..., i + 1] = draw_labelmap_np(np.zeros((h, w)), coord[i])\n",
        "  heatmap[..., 0] = heatmap.sum(axis=-1) == 0\n",
        "  return heatmap\n",
        "\n",
        "def process_pose(preds, h, w, ori_height, ori_width, topk=5):\n",
        "  current_coords, jnt_visibles = [], []\n",
        "  for t in range(preds.shape[0]):\n",
        "    pred = preds[t][..., 1:]\n",
        "    flatlbls = pred.flatten(0, 1)\n",
        "    vals, ids = torch.topk(flatlbls, k=topk, dim=0)\n",
        "    vals /= vals.sum(0)[None]\n",
        "    xx, yy = ids % pred.shape[1], ids // pred.shape[1]\n",
        "    current_coord = torch.stack([(xx * vals).sum(0), (yy * vals).sum(0)], dim=0)\n",
        "    current_coord[0, :] = current_coord[0, :] / w * ori_width\n",
        "    current_coord[1, :] = current_coord[1, :] / h * ori_height\n",
        "    current_coord[:, flatlbls.sum(0) == 0] = -1\n",
        "    current_coord = current_coord.cpu().numpy().transpose(1, 0)\n",
        "    jnt_visible = (current_coord[:, 0] >= 0).astype(float)  # (n_class)\n",
        "    current_coords.append(current_coord)\n",
        "    jnt_visibles.append(jnt_visible)\n",
        "  current_coords = np.stack(current_coords, axis=0)\n",
        "  jnt_visibles = np.stack(jnt_visibles, axis=0)\n",
        "  return current_coords, jnt_visibles\n",
        "\n",
        "def process_segmentation(preds, lbl_map, height, width, ori_height, ori_width):\n",
        "  pred_lbls = []\n",
        "  for t in range(preds.shape[0]):\n",
        "    pred = preds[t]\n",
        "    pred = pred.cpu().numpy()\n",
        "    # Upsample predicted soft label maps\n",
        "    pred_dist = cv2.resize(pred, (width, height))[:]\n",
        "    # Argmax to get the hard label for index\n",
        "    pred_lbl = np.argmax(pred_dist, axis=-1)\n",
        "    pred_lbl = np.array(lbl_map, dtype=np.int32)[pred_lbl]\n",
        "    pred_lbl = cv2.resize(pred_lbl, (ori_width, ori_height), interpolation=cv2.INTER_NEAREST_EXACT)\n",
        "    pred_lbls.append(pred_lbl)\n",
        "  pred_lbls = np.stack(pred_lbls, axis=0)\n",
        "  return pred_lbls\n",
        "\n",
        "def label_propagation(feats, heatmap, n_context=20, temperature=0.7, topk=7, radius=20, restrict_neighborhood=True, norm_mask=False):\n",
        "  \"\"\"Propagation of the heatmap based on feature similarity.\"\"\"\n",
        "\n",
        "  # Creates a mask indicating valid neighbors for each grid element.\n",
        "  h, w = feats.shape[1], feats.shape[2]\n",
        "  gx, gy = torch.meshgrid(torch.arange(0, h), torch.arange(0, w), indexing=\"ij\")  # (h, w)\n",
        "  D = ((gx[None, None, :, :] - gx[:, :, None, None])**2 + (gy[None, None, :, :] - gy[:, :, None, None])**2).float() ** 0.5\n",
        "  D = (D < radius).float().to('cuda')\n",
        "  D[D == 0] = -1e10\n",
        "  D[D == 1] = 0\n",
        "  D = D.permute(2, 3, 0, 1)  # (h, w, h, w)\n",
        "\n",
        "  # The queue stores the context frames\n",
        "  que = queue.Queue(n_context)\n",
        "  for _ in range(n_context):\n",
        "    que.put([feats[0], heatmap])\n",
        "\n",
        "  preds = []\n",
        "  for t in range(feats.shape[0]):\n",
        "    # Use first and previous frames as context\n",
        "    ctx_feats = torch.stack([feats[0]] + [pair[0] for pair in que.queue])\n",
        "    ctx_lbls = torch.stack([heatmap] + [pair[1] for pair in que.queue])\n",
        "\n",
        "    aff = torch.einsum('hwc, tmnc -> hwtmn', feats[t], ctx_feats) / temperature  # (h, w, n_context+1, h, w)\n",
        "    if restrict_neighborhood:\n",
        "      aff[:, :, 1:] += D[:, :, None]  # (h, w, n_context+1, h, w)\n",
        "    aff = aff.view(aff.shape[0], aff.shape[1], -1)  # (h, w, n_context+1 * h * w)\n",
        "\n",
        "    weights, ids = torch.topk(aff, topk, dim=-1)  # (h, w, topk), (h, w, topk)\n",
        "    weights = F.softmax(weights, dim=-1)  # (h, w, topk)\n",
        "    ctx_lbls = ctx_lbls.view(-1, ctx_lbls.shape[-1])  # (n_context+1 * h * w, n_class)\n",
        "    pred = torch.einsum('hwlk, hwl -> hwk', ctx_lbls[ids], weights) # (h, w, n_class)\n",
        "\n",
        "    if que.qsize() == n_context:\n",
        "      que.get()\n",
        "    que.put([feats[t], pred])\n",
        "\n",
        "    if norm_mask:\n",
        "      pred -= pred.min(-1)[0][..., None]\n",
        "      pred /= pred.max(-1)[0][..., None]\n",
        "\n",
        "    preds.append(pred)\n",
        "  preds = torch.stack(preds)\n",
        "  return preds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title PCA visualization and k-means clustering {form-width: \"20%\"}\n",
        "\n",
        "features = extract_features(model, noise_video)\n",
        "print(features.shape)\n",
        "\n",
        "pca_video = visualize_pca(features, noise_video)\n",
        "kmeans_video = visualize_kmeans(features, noise_video)\n",
        "mixed_video = 0.5 * noise_video / 255.0 + 0.5 * kmeans_video\n",
        "video_titles = ['pca', 'kmeans', 'mixed']\n",
        "media.show_videos([pca_video, kmeans_video, mixed_video], titles=video_titles, height=128, codec='gif', fps=16, columns=3)"
      ],
      "metadata": {
        "id": "B8_pywAzVTxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantitative Evaluation"
      ],
      "metadata": {
        "id": "m0yoRC31U5i2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download DAVIS-2017 dataset\n",
        "\n",
        "%cd /content\n",
        "!git clone https://github.com/davisvideochallenge/davis-2017\n",
        "%cd /content/davis-2017\n",
        "!./data/get_davis.sh"
      ],
      "metadata": {
        "id": "VZ9woeHz-35i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download JHMDB dataset\n",
        "\n",
        "%cd /content\n",
        "!wget https://storage.googleapis.com/representations4d/datasets/jhmdb.zip\n",
        "!unzip jhmdb.zip"
      ],
      "metadata": {
        "id": "GMVfjvCJ-74A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download VIP dataset\n",
        "\n",
        "%cd /content\n",
        "!wget https://storage.googleapis.com/representations4d/datasets/VIP.zip\n",
        "!unzip VIP.zip"
      ],
      "metadata": {
        "id": "75gFWGar_AKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2c5kZa_fUdBl"
      },
      "outputs": [],
      "source": [
        "# @title Dataset functions\n",
        "\n",
        "def create_davis_dataset(root_path):\n",
        "  \"\"\"DAVIS dataset, including fields required for video segmentation evaluation.\"\"\"\n",
        "  for video_id in tf.io.gfile.GFile(os.path.join(root_path, 'ImageSets/2017/val.txt'), 'r'):\n",
        "    video_id = video_id.strip()\n",
        "    frame_files = sorted(tf.io.gfile.glob(os.path.join(root_path, 'JPEGImages/480p', video_id, '*.jpg')))\n",
        "    frames = np.stack([np.array(Image.open(tf.io.gfile.GFile(f, 'rb')).convert('RGB')) for f in frame_files])\n",
        "    label_files = sorted(tf.io.gfile.glob(os.path.join(root_path, 'Annotations/480p', video_id, '*.png')))\n",
        "    labels = np.stack([np.array(Image.open(tf.io.gfile.GFile(f, 'rb'))) for f in label_files])\n",
        "    yield {'video': frames, 'mask': labels, 'video_id': video_id}\n",
        "\n",
        "def create_jhmdb_dataset(jhmdb_path):\n",
        "  \"\"\"JHMDB dataset, including fields required for PCK evaluation.\"\"\"\n",
        "  video_ids = []\n",
        "  for file in os.listdir(os.path.join(jhmdb_path, 'splits')):\n",
        "    if file.endswith('split1.txt'):\n",
        "      video_folder = '_'.join(file.split('_')[:-2])\n",
        "      lines = open(os.path.join(jhmdb_path, 'splits', file), 'r').readlines()\n",
        "      for line in lines:\n",
        "        video_name, traintest = line.split()\n",
        "        if int(traintest) == 2:\n",
        "          video_id = os.path.join(video_folder, video_name.split('.')[0])\n",
        "          video_ids.append(video_id)\n",
        "\n",
        "  random.shuffle(video_ids)\n",
        "  for video_id in video_ids:\n",
        "    joints = os.path.join(jhmdb_path, 'joint_positions', video_id, 'joint_positions.mat')\n",
        "    pose = sio.loadmat(joints)['pos_img'] - 1  # matlab -> python\n",
        "    pose = pose.astype(np.float32).transpose(2, 1, 0)  # (num_frames, num_points, 2)\n",
        "    frame_files = sorted(glob.glob(os.path.join(jhmdb_path, 'Rename_Images', video_id, '*.png')))\n",
        "    frames = np.stack([np.array(Image.open(open(f, 'rb')).convert('RGB')) for f in frame_files])\n",
        "    yield {'video': frames, 'pose': pose, 'video_id': video_id}\n",
        "\n",
        "def create_vip_dataset(root_path):\n",
        "  \"\"\"VIP dataset, including fields required for video segmentation evaluation.\"\"\"\n",
        "  for video_id in tf.io.gfile.GFile(os.path.join(root_path, 'lists/val_videos.txt'), 'r'):\n",
        "    video_id = video_id.strip()\n",
        "    frame_files = sorted(tf.io.gfile.glob(os.path.join(root_path, 'Images', video_id, '*.jpg')))\n",
        "    frames = np.stack([np.array(Image.open(tf.io.gfile.GFile(f, 'rb')).convert('RGB')) for f in frame_files])\n",
        "    label_files = sorted(tf.io.gfile.glob(os.path.join(root_path, 'Annotations/Category_ids', video_id, '*.png')))\n",
        "    labels = np.stack([np.array(Image.open(tf.io.gfile.GFile(f, 'rb'))) for f in label_files])\n",
        "    yield {'video': frames, 'mask': labels, 'video_id': video_id}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dw97pfvUPDV"
      },
      "outputs": [],
      "source": [
        "# @title DAVIS video segmentation evaluation metric\n",
        "# https://github.com/davisvideochallenge/davis2017-evaluation/blob/master/davis2017/metrics.py\n",
        "\n",
        "import math\n",
        "import cv2\n",
        "\n",
        "def db_eval_iou(annotation, segmentation, void_pixels=None):\n",
        "  \"\"\" Compute region similarity as the Jaccard Index.\n",
        "  Arguments:\n",
        "      annotation   (ndarray): binary annotation   map.\n",
        "      segmentation (ndarray): binary segmentation map.\n",
        "      void_pixels  (ndarray): optional mask with void pixels\n",
        "\n",
        "  Return:\n",
        "      jaccard (float): region similarity\n",
        "  \"\"\"\n",
        "  assert annotation.shape == segmentation.shape, \\\n",
        "      f'Annotation({annotation.shape}) and segmentation:{segmentation.shape} dimensions do not match.'\n",
        "  annotation = annotation.astype(bool)\n",
        "  segmentation = segmentation.astype(bool)\n",
        "\n",
        "  if void_pixels is not None:\n",
        "    assert annotation.shape == void_pixels.shape, \\\n",
        "        f'Annotation({annotation.shape}) and void pixels:{void_pixels.shape} dimensions do not match.'\n",
        "    void_pixels = void_pixels.astype(bool)\n",
        "  else:\n",
        "    void_pixels = np.zeros_like(segmentation)\n",
        "\n",
        "  # Intersection between all sets\n",
        "  inters = np.sum((segmentation & annotation) & np.logical_not(void_pixels), axis=(-2, -1))\n",
        "  union = np.sum((segmentation | annotation) & np.logical_not(void_pixels), axis=(-2, -1))\n",
        "\n",
        "  j = inters / union\n",
        "  if j.ndim == 0:\n",
        "    j = 1 if np.isclose(union, 0) else j\n",
        "  else:\n",
        "    j[np.isclose(union, 0)] = 1\n",
        "  return j\n",
        "\n",
        "\n",
        "def db_eval_boundary(annotation, segmentation, void_pixels=None, bound_th=0.008):\n",
        "  assert annotation.shape == segmentation.shape\n",
        "  if void_pixels is not None:\n",
        "    assert annotation.shape == void_pixels.shape\n",
        "  if annotation.ndim == 3:\n",
        "    n_frames = annotation.shape[0]\n",
        "    f_res = np.zeros(n_frames)\n",
        "    for frame_id in range(n_frames):\n",
        "      void_pixels_frame = None if void_pixels is None else void_pixels[frame_id, :, :, ]\n",
        "      f_res[frame_id] = f_measure(segmentation[frame_id, :, :, ], annotation[frame_id, :, :], void_pixels_frame, bound_th=bound_th)\n",
        "  elif annotation.ndim == 2:\n",
        "    f_res = f_measure(segmentation, annotation, void_pixels, bound_th=bound_th)\n",
        "  else:\n",
        "    raise ValueError(f'db_eval_boundary does not support tensors with {annotation.ndim} dimensions')\n",
        "  return f_res\n",
        "\n",
        "\n",
        "def f_measure(foreground_mask, gt_mask, void_pixels=None, bound_th=0.008):\n",
        "  \"\"\"\n",
        "  Compute mean,recall and decay from per-frame evaluation.\n",
        "  Calculates precision/recall for boundaries between foreground_mask and\n",
        "  gt_mask using morphological operators to speed it up.\n",
        "\n",
        "  Arguments:\n",
        "      foreground_mask (ndarray): binary segmentation image.\n",
        "      gt_mask         (ndarray): binary annotated image.\n",
        "      void_pixels     (ndarray): optional mask with void pixels\n",
        "\n",
        "  Returns:\n",
        "      F (float): boundaries F-measure\n",
        "  \"\"\"\n",
        "  assert np.atleast_3d(foreground_mask).shape[2] == 1\n",
        "  if void_pixels is not None:\n",
        "    void_pixels = void_pixels.astype(bool)\n",
        "  else:\n",
        "    void_pixels = np.zeros_like(foreground_mask).astype(bool)\n",
        "\n",
        "  bound_pix = bound_th if bound_th >= 1 else \\\n",
        "      np.ceil(bound_th * np.linalg.norm(foreground_mask.shape))\n",
        "\n",
        "  # Get the pixel boundaries of both masks\n",
        "  fg_boundary = _seg2bmap(foreground_mask * np.logical_not(void_pixels))\n",
        "  gt_boundary = _seg2bmap(gt_mask * np.logical_not(void_pixels))\n",
        "\n",
        "  from skimage.morphology import disk\n",
        "\n",
        "  # fg_dil = binary_dilation(fg_boundary, disk(bound_pix))\n",
        "  fg_dil = cv2.dilate(fg_boundary.astype(np.uint8), disk(bound_pix).astype(np.uint8))\n",
        "  # gt_dil = binary_dilation(gt_boundary, disk(bound_pix))\n",
        "  gt_dil = cv2.dilate(gt_boundary.astype(np.uint8), disk(bound_pix).astype(np.uint8))\n",
        "\n",
        "  # Get the intersection\n",
        "  gt_match = gt_boundary * fg_dil\n",
        "  fg_match = fg_boundary * gt_dil\n",
        "\n",
        "  # Area of the intersection\n",
        "  n_fg = np.sum(fg_boundary)\n",
        "  n_gt = np.sum(gt_boundary)\n",
        "\n",
        "  # % Compute precision and recall\n",
        "  if n_fg == 0 and n_gt > 0:\n",
        "    precision = 1\n",
        "    recall = 0\n",
        "  elif n_fg > 0 and n_gt == 0:\n",
        "    precision = 0\n",
        "    recall = 1\n",
        "  elif n_fg == 0 and n_gt == 0:\n",
        "    precision = 1\n",
        "    recall = 1\n",
        "  else:\n",
        "    precision = np.sum(fg_match) / float(n_fg)\n",
        "    recall = np.sum(gt_match) / float(n_gt)\n",
        "\n",
        "  # Compute F measure\n",
        "  if precision + recall == 0:\n",
        "    F = 0\n",
        "  else:\n",
        "    F = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "  return F\n",
        "\n",
        "\n",
        "def _seg2bmap(seg, width=None, height=None):\n",
        "  \"\"\"\n",
        "  From a segmentation, compute a binary boundary map with 1 pixel wide\n",
        "  boundaries.  The boundary pixels are offset by 1/2 pixel towards the\n",
        "  origin from the actual segment boundary.\n",
        "  Arguments:\n",
        "      seg     : Segments labeled from 1..k.\n",
        "      width   : Width of desired bmap  <= seg.shape[1]\n",
        "      height  : Height of desired bmap <= seg.shape[0]\n",
        "  Returns:\n",
        "      bmap (ndarray): Binary boundary map.\n",
        "   David Martin <dmartin@eecs.berkeley.edu>\n",
        "   January 2003\n",
        "  \"\"\"\n",
        "\n",
        "  seg = seg.astype(bool)\n",
        "  seg[seg > 0] = 1\n",
        "\n",
        "  assert np.atleast_3d(seg).shape[2] == 1\n",
        "\n",
        "  width = seg.shape[1] if width is None else width\n",
        "  height = seg.shape[0] if height is None else height\n",
        "\n",
        "  h, w = seg.shape[:2]\n",
        "\n",
        "  ar1 = float(width) / float(height)\n",
        "  ar2 = float(w) / float(h)\n",
        "\n",
        "  assert not (\n",
        "      width > w | height > h | abs(ar1 - ar2) > 0.01\n",
        "  ), \"Can't convert %dx%d seg to %dx%d bmap.\" % (w, h, width, height)\n",
        "\n",
        "  e = np.zeros_like(seg)\n",
        "  s = np.zeros_like(seg)\n",
        "  se = np.zeros_like(seg)\n",
        "\n",
        "  e[:, :-1] = seg[:, 1:]\n",
        "  s[:-1, :] = seg[1:, :]\n",
        "  se[:-1, :-1] = seg[1:, 1:]\n",
        "\n",
        "  b = seg ^ e | seg ^ s | seg ^ se\n",
        "  b[-1, :] = seg[-1, :] ^ e[-1, :]\n",
        "  b[:, -1] = seg[:, -1] ^ s[:, -1]\n",
        "  b[-1, -1] = 0\n",
        "\n",
        "  if w == width and h == height:\n",
        "    bmap = b\n",
        "  else:\n",
        "    bmap = np.zeros((height, width))\n",
        "    for x in range(w):\n",
        "      for y in range(h):\n",
        "        if b[y, x]:\n",
        "          j = 1 + math.floor((y - 1) + height / h)\n",
        "          i = 1 + math.floor((x - 1) + width / h)\n",
        "          bmap[j, i] = 1\n",
        "\n",
        "  return bmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJ3ZNja3UPDW"
      },
      "outputs": [],
      "source": [
        "# @title JHMDB pck evaluation metric\n",
        "# https://github.com/Liusifei/UVC/blob/jhmdb/eval_pck.py\n",
        "\n",
        "def compute_human_boxes(gts, jnt_visible_set):\n",
        "  human_boxes = []\n",
        "  for nowgt, jnt_visible in zip(gts, jnt_visible_set):\n",
        "    now_boxes = np.zeros(nowgt.shape[0])\n",
        "    for t in range(nowgt.shape[0]):\n",
        "      visible_pts = nowgt[t, jnt_visible[t] == 1]\n",
        "      if visible_pts.size:\n",
        "        min_pt, max_pt = visible_pts.min(axis=0), visible_pts.max(axis=0)\n",
        "        now_boxes[t] = 0.6 * np.linalg.norm(max_pt - min_pt)\n",
        "    human_boxes.append(now_boxes)\n",
        "  return human_boxes\n",
        "\n",
        "def compute_distances(gts, preds, human_boxes, jnt_visible_set):\n",
        "  distAll = {pidx: np.array([]) for pidx in range(15)}\n",
        "  for nowgt, predres, now_boxes, jnt_visible in zip(gts, preds, human_boxes, jnt_visible_set):\n",
        "    for i in range(nowgt.shape[1]):\n",
        "      for t in range(1, nowgt.shape[0]):\n",
        "        if jnt_visible[t, i]:\n",
        "          distAll[i] = np.append(distAll[i], np.linalg.norm(predres[t, i] - nowgt[t, i]) / now_boxes[t])\n",
        "  return distAll\n",
        "\n",
        "def computePCK(distAll, distThresh):\n",
        "  pckAll = np.zeros(len(distAll))\n",
        "  for pidx, distances in distAll.items():\n",
        "    pckAll[pidx] = 100.0 * np.sum(distances <= distThresh) / len(distances)\n",
        "  pck = np.mean(pckAll)\n",
        "  return pck, pckAll"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEcrf4hfUPDW"
      },
      "outputs": [],
      "source": [
        "# @title VIP video segmentation evaluation metric\n",
        "\n",
        "def fast_hist(a, b, n):\n",
        "  k = (a >= 0) & (a < n)\n",
        "  return np.bincount(n * a[k].astype(int) + b[k], minlength=n**2).reshape(n, n)\n",
        "\n",
        "def compute_iou(hist):\n",
        "  classes = ['background', 'hat', 'hair', 'sun-glasses', 'upper-clothes', 'dress',\n",
        "           'coat', 'socks', 'pants', 'gloves', 'scarf', 'skirt', 'torso-skin',\n",
        "           'face', 'right-arm', 'left-arm', 'right-leg', 'left-leg', 'right-shoe', 'left-shoe']\n",
        "\n",
        "  num_cor_pix = np.diag(hist)  # num of correct pixels\n",
        "  num_gt_pix = hist.sum(1)  # num of gt pixels\n",
        "  union = num_gt_pix + hist.sum(0) - num_cor_pix\n",
        "  iou_per_class = {}\n",
        "  for i in range(len(classes)):\n",
        "    iou_per_class[classes[i]] = num_cor_pix[i] / union[i]\n",
        "  iou = num_cor_pix / (num_gt_pix + hist.sum(0) - num_cor_pix)\n",
        "  iou = np.nanmean(iou)\n",
        "  return iou, iou_per_class"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title DAVIS evaluation function\n",
        "\n",
        "def evaluate_davis(model, extract_feature_function, dataset_path='/content/davis-2017/DAVIS/', height=480, width=880, patch_size=16, params=None):\n",
        "  davis_dataset = create_davis_dataset(dataset_path)\n",
        "  h, w = height // patch_size, width // patch_size  # feature resolution\n",
        "\n",
        "  n_max_class = 10\n",
        "  j_metrics, f_metrics, fj_metrics, counters = [], [], [], []\n",
        "  for sample in tqdm.tqdm(davis_dataset):\n",
        "    ori_height, ori_width = sample['video'].shape[1], sample['video'].shape[2]\n",
        "\n",
        "    # Extract features\n",
        "    video = media.resize_video(sample['video'], (height, width))\n",
        "    if params is None:\n",
        "      feats = extract_feature_function(model, video)  # PyTorch models\n",
        "    else:\n",
        "      feats = extract_feature_function(model, params, video)  # Jax models\n",
        "    if not isinstance(feats, torch.Tensor):\n",
        "      feats = torch.tensor(feats).cuda()\n",
        "    feats = torch.nn.functional.normalize(feats, dim=-1)\n",
        "\n",
        "    # Prepare downscaled first frame segmentation (resize and one-hot encode)\n",
        "    lbls_small, lbl_map = mask2heatmap(sample['mask'][0], h, w, height, width)\n",
        "    lbls_small = torch.tensor(lbls_small).cuda()\n",
        "\n",
        "    pred_lbls = label_propagation(feats, lbls_small, n_context=20, temperature=0.7, topk=7, radius=20, restrict_neighborhood=True, norm_mask=False)\n",
        "\n",
        "    pred_lbls = process_segmentation(pred_lbls, lbl_map, height, width, ori_height, ori_width)\n",
        "\n",
        "    n_class = int(sample['mask'].max())  # Get the number of objects in the segmentation map\n",
        "\n",
        "    masks = sample['mask'][1:-1]\n",
        "    all_gt_masks = np.eye(n_class + 1)[masks]\n",
        "    all_gt_masks = all_gt_masks[..., 1:]  # Remove background class\n",
        "\n",
        "    masks = pred_lbls[1:-1]\n",
        "    all_res_masks = np.eye(n_class + 1)[masks]\n",
        "    all_res_masks = all_res_masks[..., 1:]  # Remove background class\n",
        "\n",
        "    num_frames = all_gt_masks.shape[0]\n",
        "    j_metrics_res = np.zeros((n_class, num_frames))\n",
        "    f_metrics_res = np.zeros((n_class, num_frames))\n",
        "    for ii in range(n_class):\n",
        "      j_metrics_res[ii, :] = db_eval_iou(all_gt_masks[..., ii], all_res_masks[..., ii], None)\n",
        "      f_metrics_res[ii, :] = db_eval_boundary(all_gt_masks[..., ii], all_res_masks[..., ii], None)\n",
        "\n",
        "    JM, FM, FJM = np.zeros((n_max_class,)), np.zeros((n_max_class,)), np.zeros((n_max_class,))\n",
        "    for ii in range(n_class):\n",
        "      JM[ii] = np.nanmean(j_metrics_res[ii])\n",
        "      FM[ii] = np.nanmean(f_metrics_res[ii])\n",
        "      FJM[ii] = (JM[ii] + FM[ii]) / 2.\n",
        "    counter = np.zeros((n_max_class,))\n",
        "    counter[:n_class] = 1\n",
        "\n",
        "    j_metrics.append(JM)\n",
        "    f_metrics.append(FM)\n",
        "    fj_metrics.append(FJM)\n",
        "    counters.append(counter)\n",
        "\n",
        "  j_metrics = np.array(j_metrics)\n",
        "  f_metrics = np.array(f_metrics)\n",
        "  fj_metrics = np.array(fj_metrics)\n",
        "  counters = np.array(counters)\n",
        "\n",
        "  fj_metric = (fj_metrics * counters).sum() / counters.sum()\n",
        "  j_metric = (j_metrics * counters).sum() / counters.sum()\n",
        "  f_metric = (f_metrics * counters).sum() / counters.sum()\n",
        "\n",
        "  print('')\n",
        "  print('J&F-Mean',fj_metric)\n",
        "  print('J-Mean', j_metric)\n",
        "  print('F-Mean', f_metric)"
      ],
      "metadata": {
        "id": "8nJYZ_rtLSOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title JHMDB evaluation function\n",
        "\n",
        "def evaluate_jhmdb(model, extract_feature_function, dataset_path='/content/jhmdb/', height=320, width=320, patch_size=16, params=None):\n",
        "  jhmdb_dataset = create_jhmdb_dataset(jhmdb_path=dataset_path)\n",
        "  h, w = height // patch_size, width // patch_size  # feature resolution\n",
        "\n",
        "  gts, preds, jnt_visible_set = [], [], []\n",
        "  for sample in tqdm.tqdm(jhmdb_dataset):\n",
        "    ori_height, ori_width = sample['video'].shape[1], sample['video'].shape[2]\n",
        "\n",
        "    # Extract features\n",
        "    video = media.resize_video(sample['video'], (height, width))\n",
        "    if params is None:\n",
        "      feats = extract_feature_function(model, video)  # PyTorch models\n",
        "    else:\n",
        "      feats = extract_feature_function(model, params, video)  # Jax models\n",
        "    if not isinstance(feats, torch.Tensor):\n",
        "      feats = torch.tensor(feats).cuda()\n",
        "    feats = torch.nn.functional.normalize(feats, dim=-1)\n",
        "\n",
        "    # Prepare downscaled first frame heatmap (resize and one-hot encode)\n",
        "    lbls_small = pose2heatmap(sample['pose'][0], h, w, ori_height, ori_width)\n",
        "    lbls_small = torch.tensor(lbls_small).cuda()\n",
        "\n",
        "    pred_lbls = label_propagation(feats, lbls_small, n_context=20, temperature=0.7, topk=7, radius=20, restrict_neighborhood=True, norm_mask=False)\n",
        "\n",
        "    pred_lbls, jnt_visible = process_pose(pred_lbls, h, w, ori_height, ori_width)\n",
        "    preds.append(pred_lbls)\n",
        "    jnt_visible_set.append(jnt_visible)\n",
        "    gts.append(sample['pose'])\n",
        "\n",
        "  human_boxes = compute_human_boxes(gts, jnt_visible_set)\n",
        "  distAll = compute_distances(gts, preds, human_boxes, jnt_visible_set)\n",
        "\n",
        "  for threshold in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
        "    print(f\"PCK@{threshold}: {computePCK(distAll, threshold)[0]}\")"
      ],
      "metadata": {
        "id": "ewb62UaHfDTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title VIP evaluation function\n",
        "\n",
        "def evaluate_vip(model, extract_feature_function, dataset_path='/content/VIP/', height=448, width=880, patch_size=16, params=None):\n",
        "  vip_dataset = create_vip_dataset(root_path=dataset_path)\n",
        "  h, w = height // patch_size, width // patch_size  # feature resolution\n",
        "\n",
        "  n_class = 20\n",
        "  hist = np.zeros((n_class, n_class))\n",
        "  for sample in tqdm.tqdm(vip_dataset):\n",
        "    ori_height, ori_width = sample['video'].shape[1], sample['video'].shape[2]\n",
        "\n",
        "    # Extract features\n",
        "    video = media.resize_video(sample['video'], (height, width))\n",
        "    if params is None:\n",
        "      feats = extract_feature_function(model, video)  # PyTorch models\n",
        "    else:\n",
        "      feats = extract_feature_function(model, params, video)  # Jax models\n",
        "    if not isinstance(feats, torch.Tensor):\n",
        "      feats = torch.tensor(feats).cuda()\n",
        "    feats = torch.nn.functional.normalize(feats, dim=-1)\n",
        "\n",
        "    # Prepare downscaled first frame segmentation (resize and one-hot encode)\n",
        "    lbls_small, lbl_map = mask2heatmap(sample['mask'][0], h, w, height, width)\n",
        "    lbls_small = torch.tensor(lbls_small).cuda()\n",
        "\n",
        "    pred_lbls = label_propagation(feats, lbls_small, n_context=20, temperature=0.7, topk=10, radius=20, restrict_neighborhood=True, norm_mask=False)\n",
        "\n",
        "    pred_lbls = process_segmentation(pred_lbls, lbl_map, height, width, ori_height, ori_width)\n",
        "\n",
        "    for t in range(pred_lbls.shape[0]):\n",
        "      hist += fast_hist(sample['mask'][t], pred_lbls[t], n_class)\n",
        "\n",
        "  iou, iou_per_class = compute_iou(hist)\n",
        "\n",
        "  print('')\n",
        "  for key in iou_per_class:\n",
        "    print('%-15s: %f' % (key, iou_per_class[key]))\n",
        "  print ('mean IoU', iou)"
      ],
      "metadata": {
        "id": "plVS6zBnek0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title DAVIS evaluation\n",
        "\n",
        "evaluate_davis(model, extract_features, dataset_path='/content/davis-2017/DAVIS/', height=480, width=880, patch_size=16, params=restored_params)"
      ],
      "metadata": {
        "id": "x5-kboh2ACj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title JHMDB evaluation\n",
        "\n",
        "evaluate_jhmdb(model, extract_features, dataset_path='/content/jhmdb/', height=320, width=320, patch_size=16, params=restored_params)"
      ],
      "metadata": {
        "id": "JfkZurCrAFEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title VIP evaluation\n",
        "\n",
        "evaluate_vip(model, extract_features, dataset_path='/content/VIP/', height=448, width=880, patch_size=16, params=restored_params)"
      ],
      "metadata": {
        "id": "WL6hyBX9AHF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Visualize video segmentation on DAVIS\n",
        "\n",
        "model_name = 'rvm'\n",
        "\n",
        "os.makedirs(f'/content/davis_results/{model_name}', exist_ok=True)\n",
        "\n",
        "davis_dataset = create_davis_dataset('/content/davis-2017/DAVIS')\n",
        "height, width = 480, 880  # video resolution during processing\n",
        "h, w = height // PATCH_SIZE, width // PATCH_SIZE  # feature shape\n",
        "\n",
        "video_names = [\n",
        "    'bike-packing',\n",
        "    'bmx-trees',\n",
        "    'gold-fish',\n",
        "    'horsejump-high',\n",
        "    'judo',\n",
        "    'lab-coat',\n",
        "    'pigs'\n",
        "]\n",
        "\n",
        "for i, sample in enumerate(davis_dataset):\n",
        "  if sample['mask'].max() <= 1 or sample['video_id'] not in video_names:\n",
        "    continue\n",
        "  print(i, sample['video_id'])\n",
        "\n",
        "  # Extract features\n",
        "  video = media.resize_video(sample['video'], (height, width))\n",
        "  feats = extract_features(model, restored_params, video)\n",
        "  feats = feats / jnp.linalg.norm(feats, axis=-1, keepdims=True)\n",
        "\n",
        "  # Prepare downscaled first frame segmentation (resize and one-hot encode)\n",
        "  lbls_small, lbl_map = mask2heatmap(sample['mask'][0], h, w, height, width)\n",
        "  pred_lbls = label_propagation(feats, lbls_small, n_context=20, temperature=0.7, topk=7, radius=20, restrict_neighborhood=True, norm_mask=False)\n",
        "  pred_mask = heatmap2mask(pred_lbls, lbl_map, height, width, height, width)\n",
        "\n",
        "  seg_video = segmentations_to_video(pred_mask)\n",
        "  mixed_video = 0.5 * video / 255.0 + 0.5 * seg_video\n",
        "  media.show_video(mixed_video, height=128, codec='gif', fps=16)\n",
        "\n",
        "  filename = f'/content/davis_results/{model_name}/{sample['video_id']}.mp4'\n",
        "  media.write_video(filename, mixed_video, fps=16)\n",
        "\n",
        "  num_frames = video.shape[0]\n",
        "  for t in [0, num_frames // 4, num_frames // 2, num_frames - 1]:\n",
        "    image = Image.fromarray((np.array(mixed_video[t]) * 255.0).astype(np.uint8))\n",
        "    image.save(f'/content/davis_results/{model_name}/{sample['video_id']}_{t}.pdf')"
      ],
      "metadata": {
        "id": "rZMWZJ2McnFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Visualize pose tracking on JHMDB\n",
        "\n",
        "model_name = 'rvm'\n",
        "\n",
        "os.makedirs(f'/content/jhmdb_results/{model_name}', exist_ok=True)\n",
        "\n",
        "jhmdb_dataset = create_jhmdb_dataset(jhmdb_path='/content/jhmdb/')\n",
        "height, width = 320, 320  # video resolution during processing\n",
        "h, w = height // PATCH_SIZE, width // PATCH_SIZE  # feature shape\n",
        "\n",
        "num_videos_to_show = 5\n",
        "for i, sample in enumerate(jhmdb_dataset):\n",
        "  if i >= num_videos_to_show: break\n",
        "\n",
        "  print(i, sample['video_id'])\n",
        "  video_id = sample['video_id'].split('/')[-1]\n",
        "  ori_height, ori_width = sample['video'].shape[1], sample['video'].shape[2]\n",
        "\n",
        "  # Extract features\n",
        "  video = media.resize_video(sample['video'], (height, width))\n",
        "  feats = extract_features(model, restored_params, video)\n",
        "  feats = feats / jnp.linalg.norm(feats, axis=-1, keepdims=True)\n",
        "\n",
        "  # Prepare downscaled first frame heatmap (resize and one-hot encode)\n",
        "  lbls_small = pose2heatmap(sample['pose'][0], h, w, ori_height, ori_width)\n",
        "  pred_lbls = label_propagation(feats, lbls_small, n_context=20, temperature=0.7, topk=7, radius=20, restrict_neighborhood=True, norm_mask=False)\n",
        "  pred_pose, jnt_visible = heatmap2pose(pred_lbls, h, w, ori_height, ori_width)\n",
        "\n",
        "  pose_video = []\n",
        "  for t in range(sample['video'].shape[0]):\n",
        "    pose_video.append(vis_pose(sample['video'][t], pred_pose[t]))\n",
        "  pose_video = np.stack(pose_video, axis=0)\n",
        "  gt_video = []\n",
        "  for t in range(sample['video'].shape[0]):\n",
        "    gt_video.append(vis_pose(sample['video'][t], sample['pose'][t]))\n",
        "  gt_video = np.stack(gt_video, axis=0)\n",
        "  videos = np.concatenate([pose_video, gt_video], axis=2)\n",
        "  media.show_video(videos, height=128, fps=16, codec='gif')\n",
        "\n",
        "  filename = f'/content/jhmdb_results/{model_name}/{video_id}.mp4'\n",
        "  media.write_video(filename, pose_video, fps=16)\n",
        "\n",
        "  num_frames = video.shape[0]\n",
        "  for t in [0, num_frames // 4, num_frames // 2, num_frames - 1]:\n",
        "    image = Image.fromarray(pose_video[t])\n",
        "    image.save(f'/content/jhmdb_results/{model_name}/{video_id}_{t}.pdf')"
      ],
      "metadata": {
        "id": "eHN8BxIlcqlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Visualize video segmentation on VIP\n",
        "\n",
        "model_name = 'rvm'\n",
        "\n",
        "os.makedirs(f'/content/vip_results/{model_name}', exist_ok=True)\n",
        "\n",
        "vip_dataset = create_vip_dataset(root_path='/content/VIP/')\n",
        "height, width = 448, 880  # video resolution during processing\n",
        "h, w = height // PATCH_SIZE, width // PATCH_SIZE  # feature shape\n",
        "\n",
        "num_videos_to_show = 5\n",
        "for i, sample in enumerate(vip_dataset):\n",
        "  if sample['mask'].max() <= 5:\n",
        "    continue\n",
        "  if i >= num_videos_to_show: break\n",
        "\n",
        "  print(i, sample['video_id'])\n",
        "\n",
        "  # Extract features\n",
        "  video = media.resize_video(sample['video'], (height, width))\n",
        "  feats = extract_features(model, restored_params, video)\n",
        "  feats = feats / jnp.linalg.norm(feats, axis=-1, keepdims=True)\n",
        "\n",
        "  # Prepare downscaled first frame segmentation (resize and one-hot encode)\n",
        "  lbls_small, lbl_map = mask2heatmap(sample['mask'][0], h, w, height, width)\n",
        "  pred_lbls = label_propagation(feats, lbls_small, n_context=20, temperature=0.7, topk=7, radius=20, restrict_neighborhood=True, norm_mask=False)\n",
        "  pred_mask = heatmap2mask(pred_lbls, lbl_map, height, width, height, width)\n",
        "\n",
        "  seg_video = segmentations_to_video(pred_mask)\n",
        "  mixed_video = 0.5 * video / 255.0 + 0.5 * seg_video\n",
        "  media.show_video(mixed_video, height=128, codec='gif', fps=16)\n",
        "\n",
        "  filename = f'/content/vip_results/{model_name}/{sample['video_id']}.mp4'\n",
        "  media.write_video(filename, mixed_video, fps=16)\n",
        "\n",
        "  num_frames = video.shape[0]\n",
        "  for t in [0, num_frames // 4, num_frames // 2, num_frames - 1]:\n",
        "    image = Image.fromarray((np.array(mixed_video[t]) * 255.0).astype(np.uint8))\n",
        "    image.save(f'/content/vip_results/{model_name}/{sample['video_id']}_{t}.pdf')"
      ],
      "metadata": {
        "id": "igyzHOWLcsn2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "last_runtime": {
        "build_target": "//learning/pytorch/colab:kernel",
        "kind": "private"
      },
      "provenance": [
        {
          "file_id": "1TPdW1qEjdE0tDPnREuskhoUzOi8fmSsg",
          "timestamp": 1765591950984
        },
        {
          "file_id": "https://github.com/google-deepmind/representations4d/blob/main/colabs/rvm_evaluation_demo.ipynb",
          "timestamp": 1765587714975
        },
        {
          "file_id": "1BA8StfouHvQChfUHN264qkkKWyQ4LWQa",
          "timestamp": 1765234975033
        },
        {
          "file_id": "1VX4rxrjP05POLVzRqjOo1KCXHbBCg_9c",
          "timestamp": 1755600525138
        },
        {
          "file_id": "1davdOLPoQ4KiWkKc6HKWRb48IJTqUET3",
          "timestamp": 1753193893350
        },
        {
          "file_id": "1gNb5SWr16OHEOcHIkRI783Cz-B5he_sK",
          "timestamp": 1752155778633
        },
        {
          "file_id": "1Sg7f1ibn8r6YaAz6by1no0zXn4K9kw-W",
          "timestamp": 1747303081093
        },
        {
          "file_id": "1Dla4d31nDY1skIkEKRCeSSW2l9RLCFH6",
          "timestamp": 1742835424224
        },
        {
          "file_id": "1NiZzcShUkJIyI3OBi4rHat0WXQhVVqN2",
          "timestamp": 1741194275175
        },
        {
          "file_id": "1Cmj_lBDUFDPp5A_YDU3xZUjgfDAd8L1P",
          "timestamp": 1741046636775
        },
        {
          "file_id": "1em9gkinQ6-F3TnBtwL5TdYZmvAkD7XRA",
          "timestamp": 1741034974907
        },
        {
          "file_id": "1npI3DYCA629BZXYbEUt8fHJGT_A-_9CN",
          "timestamp": 1741004612797
        }
      ],
      "toc_visible": true,
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
