{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OUxK_j-FD2j"
      },
      "source": [
        "Copyright 2020 DeepMind Technologies Limited.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use\n",
        "this file except in compliance with the License. You may obtain a copy of the\n",
        "License at\n",
        "\n",
        "https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed\n",
        "under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR\n",
        "CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
        "specific language governing permissions and limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oULTTw9JFS3w"
      },
      "source": [
        "\u003cp align=\"center\"\u003e\n",
        "  \u003ch1 align=\"center\"\u003eScaling 4D Representations\u003c/h1\u003e\n",
        "  \u003cp align=\"center\"\u003e\n",
        "    João Carreira, Dilara Gokay, Michael King, Chuhan Zhang, Ignacio Rocco, Aravindh Mahendran, Thomas Albert Keck, Joseph Heyward, Skanda Koppula, Etienne Pot, Goker Erdogan, Yana Hasson, Yi Yang, Klaus Greff, Guillaume Le Moing, Sjoerd van Steenkiste, Daniel Zoran, Drew A. Hudson, Pedro Vélez, Luisa Polanía, Luke Friedman, Chris Duvarney, Ross Goroshin, Kelsey Allen, Jacob Walker, Rishabh Kabra, Eric Aboussouan, Jennifer Sun, Thomas Kipf, Carl Doersch, Viorica Pătrăucean, Dima Damen, Pauline Luc, Mehdi S. M. Sajjadi, Andrew Zisserman\n",
        "  \u003c/p\u003e\n",
        "  \u003ch3 align=\"center\"\u003e\u003ca href=\"https://arxiv.org/abs/2412.15212\"\u003ePaper\u003c/a\u003e | \u003ca href=\"https://github.com/google-deepmind/representations4d\"\u003eGitHub\u003c/a\u003e  \u003c/h3\u003e\n",
        "  \u003cdiv align=\"center\"\u003e\u003c/div\u003e\n",
        "\u003c/p\u003e\n",
        "\n",
        "\u003cp align=\"center\"\u003e\n",
        "  \u003ca href=\"\"\u003e\n",
        "    \u003cimg src=\"https://storage.googleapis.com/representations4d/assets/architecture.png\" alt=\"Logo\" width=\"50%\"\u003e\n",
        "  \u003c/a\u003e\n",
        "\u003c/p\u003e\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NgPswJY_aYBK",
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# @title Installation\n",
        "\n",
        "!git clone https://github.com/google-deepmind/representations4d.git\n",
        "%cd representations4d\n",
        "!pip install ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uyMDObj2146B",
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# @title Download example input and checkpoint\n",
        "!wget https://storage.googleapis.com/representations4d/checkpoints/scaling4d_dist_b_depth.npz\n",
        "!wget https://storage.googleapis.com/representations4d/assets/horsejump-high.mp4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kWDAwYNLDyHn"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "\n",
        "from flax import linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from kauldron.modules import pos_embeddings\n",
        "from kauldron.modules import vit as kd_vit\n",
        "import mediapy\n",
        "from models import model as model_lib\n",
        "from models import readout\n",
        "import numpy as np\n",
        "from utils import checkpoint_utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pewIY2vOavqg"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "d8LJ3IEkD2DC"
      },
      "outputs": [],
      "source": [
        "# @title Hyperparameters\n",
        "model_patch_size = (2, 16, 16)\n",
        "im_size = (224, 224)\n",
        "model_size = \"B\"\n",
        "dtype = jnp.float32\n",
        "model_output_patch_size = (2, 8, 8)\n",
        "n_pixels_patch = (\n",
        "    model_output_patch_size[0]\n",
        "    * model_output_patch_size[1]\n",
        "    * model_output_patch_size[2]\n",
        ")\n",
        "num_input_frames = 16\n",
        "n_pixels_video = num_input_frames * im_size[0] * im_size[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xO6HEugQD2AF"
      },
      "outputs": [],
      "source": [
        "# @title Define model\n",
        "encoder = model_lib.Model(\n",
        "    encoder=model_lib.Tokenizer(\n",
        "        patch_embedding=model_lib.PatchEmbedding(\n",
        "            patch_size=model_patch_size,\n",
        "            num_features=kd_vit.VIT_SIZES[model_size][0],\n",
        "        ),\n",
        "        posenc=pos_embeddings.LearnedEmbedding(dtype=dtype),\n",
        "        posenc_axes=(-4, -3, -2),\n",
        "    ),\n",
        "    processor=model_lib.GeneralizedTransformer.from_variant_str(\n",
        "        variant_str=model_size,\n",
        "        dtype=dtype,\n",
        "    ),\n",
        ")\n",
        "\n",
        "encoder2readout = model_lib.EncoderToReadout(\n",
        "    embedding_shape=(\n",
        "        num_input_frames // model_patch_size[0],\n",
        "        im_size[0] // model_patch_size[1],\n",
        "        im_size[1] // model_patch_size[2],\n",
        "    ),\n",
        "    readout_depth=0.95,\n",
        "    num_input_frames=num_input_frames,\n",
        ")\n",
        "\n",
        "readout_head = readout.AttentionReadout(\n",
        "    num_classes=n_pixels_patch,\n",
        "    num_params=1024,\n",
        "    num_heads=16,\n",
        "    num_queries=n_pixels_video // n_pixels_patch,\n",
        "    output_shape=(\n",
        "        num_input_frames,\n",
        "        im_size[0],\n",
        "        im_size[1],\n",
        "        1,\n",
        "    ),\n",
        "    decoding_patch_size=model_output_patch_size,\n",
        ")\n",
        "\n",
        "model = nn.Sequential([encoder, encoder2readout, readout_head])\n",
        "\n",
        "\n",
        "def forward(params, vid):\n",
        "  return model.apply(params, vid, is_training_property=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sSH7aRm0D19J"
      },
      "outputs": [],
      "source": [
        "# @title Initialize model\n",
        "key = jax.random.key(0)\n",
        "x = jnp.zeros((1, 16, 224, 224, 3)).astype(jnp.float32)\n",
        "\n",
        "model_params = model.init(key, x, is_training_property=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RU1yDeDJD5pP"
      },
      "outputs": [],
      "source": [
        "# @title Restore parameters\n",
        "\n",
        "restored_params = checkpoint_utils.recover_tree(\n",
        "    checkpoint_utils.npload(\"scaling4d_dist_b_depth.npz\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DzGy8qJSD5mn"
      },
      "outputs": [],
      "source": [
        "# @title Load example video from DAVIS\n",
        "\n",
        "video = mediapy.read_video(\"horsejump-high.mp4\")\n",
        "\n",
        "video = mediapy.resize_video(video, im_size) / 255.0\n",
        "video = video[jnp.newaxis, :num_input_frames].astype(jnp.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aE0coojiD8gZ"
      },
      "outputs": [],
      "source": [
        "# @title Run forward pass\n",
        "outputs = forward(restored_params, video)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QgQ_EsrHD-GC",
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# @title Visualize depth maps\n",
        "out = np.array(outputs[0])\n",
        "out = jnp.tile(out, [1, 1, 1, 3])\n",
        "out = out / np.max(out)\n",
        "vis = np.concatenate([video[0], out], axis=2)\n",
        "mediapy.show_video(vis, fps=20)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "last_runtime": {
        "build_target": "//learning/grp/tools/ml_python/gpu:ml_notebook",
        "kind": "private"
      },
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
