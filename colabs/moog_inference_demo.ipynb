{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Copyright 2025 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
      ],
      "metadata": {
        "id": "XbTo72SG2EnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Installation {form-width: \"10%\"}\n",
        "\n",
        "!pip install kauldron mediapy\n",
        "!pip install git+https://github.com/google-deepmind/tapnet.git"
      ],
      "metadata": {
        "id": "bQ3VQ93Dw0oY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Imports {form-width: \"10%\"}\n",
        "\n",
        "import io\n",
        "import os\n",
        "import random\n",
        "\n",
        "import einops\n",
        "from flax import linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import mediapy as media\n",
        "import numpy as np\n",
        "from typing import Callable, Optional\n",
        "\n",
        "from kauldron import kd\n",
        "from kauldron.typing import Float\n",
        "from tapnet.utils import viz_utils"
      ],
      "metadata": {
        "id": "cell1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "1ZuF1TMnsITE"
      },
      "cell_type": "code",
      "source": [
        "# @title Download checkpoints {form-width: \"10%\"}\n",
        "\n",
        "!wget https://storage.googleapis.com/representations4d/checkpoints/moog_ego4d_backbone_ckpt_164335139.npz\n",
        "!wget https://storage.googleapis.com/representations4d/checkpoints/moog_ego4d_point_track_head_ckpt_164335139.npz\n",
        "!wget https://storage.googleapis.com/representations4d/checkpoints/moog_ego4d_box_track_head_ckpt_164335139.npz"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Attention module {form-width: \"10%\"}\n",
        "\n",
        "class ImprovedMHDPAttention(nn.Module):\n",
        "  num_heads: int\n",
        "  qkv_size: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs_q, inputs_kv):\n",
        "    query = nn.DenseGeneral(features=(self.num_heads, self.qkv_size // self.num_heads), use_bias=False, name='dense_query')(inputs_q)\n",
        "    key = nn.DenseGeneral(features=(self.num_heads, self.qkv_size // self.num_heads), use_bias=False, name='dense_key')(inputs_kv)\n",
        "    value = nn.DenseGeneral(features=(self.num_heads, self.qkv_size // self.num_heads), use_bias=False, name='dense_value')(inputs_kv)\n",
        "    query = nn.RMSNorm(name='norm_query')(query)\n",
        "    key = nn.RMSNorm(name='norm_key')(key)\n",
        "    query = query / jnp.sqrt(query.shape[-1])\n",
        "    attn = jax.nn.softmax(jnp.einsum('...qhd,...khd->...hqk', query, key), axis=-1)\n",
        "    x = jnp.einsum('...hqk,...khd->...qhd', attn, value)\n",
        "    out = nn.DenseGeneral(features=inputs_q.shape[-1], axis=(-2, -1), use_bias=True, name='dense_out')(x)\n",
        "    return out\n",
        "\n",
        "\n",
        "class ImprovedTransformerBlock(nn.Module):\n",
        "  mlp_size: int\n",
        "  num_heads: int\n",
        "  qkv_size: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, queries, inputs_kv=None):\n",
        "    width = queries.shape[-1]\n",
        "    x = queries\n",
        "    query = nn.LayerNorm(use_bias=False, use_scale=True, name='norm_q')(queries)\n",
        "    x += ImprovedMHDPAttention(self.num_heads, self.qkv_size, name='self_att')(query, query)\n",
        "    if inputs_kv is not None:\n",
        "      x += ImprovedMHDPAttention(self.num_heads, self.qkv_size, name='cross_att')(query, inputs_kv)\n",
        "    normed_x = nn.LayerNorm(use_bias=False, use_scale=True, name='norm_attn')(x)\n",
        "    h = nn.gelu(nn.Dense(self.mlp_size, name='MLP_in')(normed_x))\n",
        "    mlp_out = nn.Dense(width, name='MLP_out')(h)\n",
        "    return x + mlp_out\n",
        "\n",
        "\n",
        "class ImprovedTransformer(nn.Module):\n",
        "  qkv_size: int\n",
        "  num_heads: int\n",
        "  mlp_size: int\n",
        "  num_layers: int\n",
        "  hidden_size: Optional[int] = None\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, queries, inputs_kv=None, num_token_axes=1):\n",
        "    first_token_axis = -1 - num_token_axes\n",
        "    batch_shape = queries.shape[:first_token_axis]\n",
        "    token_shape = queries.shape[first_token_axis:-1]\n",
        "    if num_token_axes > 1:\n",
        "      queries = jnp.reshape(queries, batch_shape + (np.prod(token_shape), queries.shape[-1]))\n",
        "\n",
        "    query_size = queries.shape[-1]\n",
        "    if self.hidden_size is not None and self.hidden_size != query_size:\n",
        "      output_size = query_size\n",
        "    else:\n",
        "      output_size = None\n",
        "    if self.hidden_size is not None:\n",
        "      queries = nn.Dense(features=self.hidden_size)(queries)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      queries = ImprovedTransformerBlock(\n",
        "          qkv_size=self.qkv_size,\n",
        "          num_heads=self.num_heads,\n",
        "          mlp_size=self.mlp_size,\n",
        "          name=f'layer_{i}',\n",
        "      )(queries, inputs_kv)\n",
        "\n",
        "    queries = nn.LayerNorm(use_bias=False, use_scale=True, name='norm_encoder')(queries)\n",
        "    if output_size is not None:\n",
        "      queries = nn.Dense(features=output_size)(queries)\n",
        "    if len(token_shape) > 1:\n",
        "      queries = jnp.reshape(queries, batch_shape + token_shape + (queries.shape[-1],))\n",
        "    return queries"
      ],
      "metadata": {
        "id": "jA2afxwvS0xB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Position embedding module {form-width: \"10%\"}\n",
        "\n",
        "def _convert_to_fourier_features(inputs, basis_degree):\n",
        "  n_dims = inputs.shape[-1]\n",
        "  freq_basis = jnp.concatenate([2**i * jnp.eye(n_dims) for i in range(basis_degree)], 1)\n",
        "  x = inputs @ freq_basis  # Project inputs onto frequency basis.\n",
        "  return jnp.sin(jnp.concatenate([x, x + 0.5 * jnp.pi], axis=-1))\n",
        "\n",
        "\n",
        "def _create_gradient_grid(samples_per_dim, value_range=(-1.0, 1.0)):\n",
        "  s = [jnp.linspace(value_range[0], value_range[1], n) for n in samples_per_dim]\n",
        "  return jnp.stack(jnp.meshgrid(*s, sparse=False, indexing='ij'), axis=-1)\n",
        "\n",
        "\n",
        "class FourierEmbedding(nn.Module):\n",
        "  num_fourier_bases: int\n",
        "  update_type: str\n",
        "  axes: tuple = (-2,)\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs):\n",
        "    emb_shape = tuple(inputs.shape[axis] for axis in self.axes)\n",
        "    coords = _create_gradient_grid(emb_shape, value_range=(-1.0, 1.0))\n",
        "    pos_embedding = _convert_to_fourier_features(coords * jnp.pi, self.num_fourier_bases)\n",
        "    all_axes = list(range(min(self.axes), -1))\n",
        "    axes_to_add = tuple(axis - min(self.axes) for axis in all_axes if axis not in self.axes)\n",
        "    pos_embedding = jnp.expand_dims(pos_embedding, axis=axes_to_add)\n",
        "\n",
        "    if self.update_type == 'project_add':\n",
        "      n_features = inputs.shape[-1]\n",
        "      x = inputs + nn.Dense(n_features, name='dense_pe')(pos_embedding)\n",
        "    elif self.update_type == 'concat':\n",
        "      pos_embedding = jnp.broadcast_to(pos_embedding, inputs.shape[:-1] + pos_embedding.shape[-1:])\n",
        "      x = jnp.concatenate((inputs, pos_embedding), axis=-1)\n",
        "    else:\n",
        "      raise ValueError('Invalid update type provided.')\n",
        "    return x\n",
        "\n",
        "\n",
        "class SampleFourierEmbedding(nn.Module):\n",
        "  num_fourier_bases: int\n",
        "  update_type: str\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs, coords=None):\n",
        "    if coords is None:\n",
        "      coords = inputs\n",
        "\n",
        "    pos_embedding = _convert_to_fourier_features(coords * jnp.pi, self.num_fourier_bases)\n",
        "\n",
        "    if self.update_type == 'project_add':\n",
        "      n_features = inputs.shape[-1]\n",
        "      x = inputs + nn.Dense(n_features, name='dense_pe')(pos_embedding)\n",
        "    elif self.update_type == 'concat':\n",
        "      pos_embedding = jnp.broadcast_to(pos_embedding, inputs.shape[:-1] + pos_embedding.shape[-1:])\n",
        "      x = jnp.concatenate((inputs, pos_embedding), axis=-1)\n",
        "    elif self.update_type == 'replace':\n",
        "      x = jnp.broadcast_to(pos_embedding, inputs.shape[:-1] + pos_embedding.shape[-1:])\n",
        "    else:\n",
        "      raise ValueError('Invalid update type provided.')\n",
        "    return x"
      ],
      "metadata": {
        "id": "UlKWwfFJXhM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Base module {form-width: \"10%\"}\n",
        "\n",
        "class RandomStateInit(nn.Module):\n",
        "  shape: tuple[int, ...]\n",
        "  random_init_scale: float\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs, batch_shape):\n",
        "    key = self.make_rng(\"default\")\n",
        "    state = self.random_init_scale * jax.random.normal(key=key, shape=batch_shape + self.shape)\n",
        "    return state\n",
        "\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "  features: tuple[int, ...]\n",
        "  kernel_sizes: tuple[tuple[int, ...], ...]\n",
        "  strides: tuple[tuple[int, ...], ...]\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    for n, (f, k, s) in enumerate(zip(self.features, self.kernel_sizes, self.strides)):\n",
        "      x = nn.Conv(f, k, s)(x)\n",
        "      if n < len(self.features) - 1:\n",
        "        x = nn.gelu(x)\n",
        "    x = nn.LayerNorm(use_bias=False, use_scale=True)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  hidden_size: int\n",
        "  output_size: Optional[int] = None\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    output_size = self.output_size or x.shape[-1]\n",
        "    x = nn.Dense(self.hidden_size)(x)\n",
        "    x = nn.gelu(x)\n",
        "    x = nn.Dense(output_size)(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "KRTkF_inYO6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Model module {form-width: \"10%\"}\n",
        "\n",
        "class SRTEncoder(nn.Module):\n",
        "  backbone: nn.Module\n",
        "  transformer: nn.Module | None\n",
        "  pos_embedding: nn.Module\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, image):\n",
        "    x = self.backbone(image)\n",
        "    x = self.pos_embedding(x)\n",
        "    x = einops.rearrange(x, '... h w d -> ... (h w) d')\n",
        "    if self.transformer is not None:\n",
        "      x = self.transformer(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class SRTDecoder(nn.Module):\n",
        "  transformer: nn.Module\n",
        "  pos_embedding: nn.Module\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs, targets):\n",
        "    h, w = targets.shape[-3], targets.shape[-2]\n",
        "    batch_shape, feat_dim = inputs.shape[:-2], inputs.shape[-1]\n",
        "    coords = _create_gradient_grid((h, w), (-1.0, 1.0))\n",
        "    coords = jnp.broadcast_to(coords, batch_shape + coords.shape)\n",
        "    zero_feats = jnp.zeros(batch_shape + (height, width, feat_dim))\n",
        "    x = self.pos_embedding(inputs=zero_feats, coords=coords)\n",
        "    x = self.transformer(x, inputs_kv=inputs, num_token_axes=2)\n",
        "    recon = nn.Dense(features=targets.shape[-1], name='recon')(x)\n",
        "    return recon, targets\n",
        "\n",
        "\n",
        "class MooG(nn.Module):\n",
        "  encoder: SRTEncoder\n",
        "  initializer: nn.Module\n",
        "  predictor_sa: nn.Module\n",
        "  predictor_ca: nn.Module\n",
        "  decoder: SRTDecoder\n",
        "  state_layer_norm: nn.Module = nn.LayerNorm(epsilon=1e-4, use_scale=True, use_bias=False)\n",
        "\n",
        "  def scan_over_time(self, encoded_inputs, state):\n",
        "    states = [state]\n",
        "    for t in range(encoded_inputs.shape[-3]):\n",
        "      state = self.state_layer_norm(state)\n",
        "      state = state + self.predictor_sa(state)\n",
        "      state = state + self.predictor_ca(queries=state, inputs_kv=encoded_inputs[..., t, :, :])\n",
        "      states.append(state)\n",
        "    return jnp.stack(states, axis=-3)\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, video):\n",
        "    x = self.encoder(video)\n",
        "    state = self.initializer(x[..., 0, :, :], batch_shape=x.shape[:-3])\n",
        "    states = self.scan_over_time(x, state)\n",
        "    out, targets = self.decoder(inputs=states[..., :-1, :, :], targets=video)\n",
        "    return dict(video_predicted=out, subsampled_targets_predicted=targets)\n",
        "\n",
        "\n",
        "class EvalWrapper(nn.Module):\n",
        "  pretrained_model: MooG\n",
        "\n",
        "  def setup(self):\n",
        "    self.encoder = self.pretrained_model.encoder\n",
        "    self.initializer = self.pretrained_model.initializer\n",
        "    self.predictor_ca = self.pretrained_model.predictor_ca\n",
        "    self.predictor_sa = self.pretrained_model.predictor_sa\n",
        "    self.decoder = self.pretrained_model.decoder\n",
        "    self.state_layer_norm = self.pretrained_model.state_layer_norm\n",
        "    self.scan_over_time = self.pretrained_model.scan_over_time\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, video, state=None):\n",
        "    x = self.encoder(video)\n",
        "    if state is None:\n",
        "      state = self.initializer(x[..., 0, :, :], batch_shape=x.shape[:-3])\n",
        "    states = self.scan_over_time(x, state)\n",
        "    return dict(features=states[..., 1:, :, :], state=states[..., -1, :, :])"
      ],
      "metadata": {
        "id": "EQPXOZRd13Up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Readout module {form-width: \"10%\"}\n",
        "\n",
        "class AttentionReadout(nn.Module):\n",
        "  num_classes: int\n",
        "  num_params: int\n",
        "  num_heads: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs, queries):\n",
        "    feats = nn.LayerNorm()(inputs) # (1, 16, 1024, 512)\n",
        "    feats += kd.nn.LearnedEmbedding(name='temporal_posenc')(feats.shape, axis=-3)\n",
        "    feats = einops.rearrange(feats, '... T N C -> ... (T N) C') # (1, 16384, 512)\n",
        "    query = nn.Dense(self.num_params)(queries) # (1, 408, 1024)\n",
        "    query = einops.rearrange(query, '... Q (h n) -> ... Q h n', h=self.num_heads) # (1, 408, 8, 128)\n",
        "    key = nn.DenseGeneral(features=(self.num_heads, self.num_params // self.num_heads), axis=-1, use_bias=True, name='key_embedding')(feats) # (1, 16384, 8, 128)\n",
        "    val = nn.DenseGeneral(features=(self.num_heads, self.num_params // self.num_heads), axis=-1, use_bias=True, name='value_embedding')(feats) # (1, 16384, 8, 128)\n",
        "    token = nn.dot_product_attention(query=query, key=key, value=val) # (1, 408, 8, 128)\n",
        "    token = einops.rearrange(token, '... Q N c -> ... Q (N c)') # (1, 408, 1024)\n",
        "    query = einops.rearrange(query, '... Q N c -> ... Q (N c)') # (1, 408, 1024)\n",
        "    token = query + nn.Dense(self.num_params)(token) # (1, 408, 1024)\n",
        "    token = token + MLP(self.num_params * 4)(nn.LayerNorm()(token)) # (1, 408, 1024)\n",
        "    out = nn.Dense(self.num_classes)(token) # (1, 408, 8)\n",
        "    return out\n",
        "\n",
        "\n",
        "class TrackingReadoutWrapper(nn.Module):\n",
        "  attention_readout: AttentionReadout\n",
        "  query_initializer: nn.Module | None = None\n",
        "  output_activation: Optional[Callable[[Float['*b']], Float['*b']]] = None\n",
        "  predict_visibility: bool = False\n",
        "  use_certainty: bool = True\n",
        "  certainty_threshold: float = 0.5\n",
        "  num_frames_per_query: int = 1\n",
        "  temporal_tile_size: int = 1\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs, queries):\n",
        "    if self.query_initializer is not None:\n",
        "      queries = self.query_initializer(queries)\n",
        "\n",
        "    if self.temporal_tile_size > 1:\n",
        "      queries = einops.repeat(queries, 'B Q D -> B Q k D', k=self.temporal_tile_size)\n",
        "      pos = kd.nn.LearnedEmbedding(name='temporal_tile_posenc')(queries.shape, axis=-2).astype(queries.dtype)\n",
        "      queries = einops.rearrange(queries + pos, 'B Q k D -> B (Q k) D')\n",
        "\n",
        "    out = self.attention_readout(inputs, queries)\n",
        "    out = einops.rearrange(out, 'B (Q k) C -> B Q (k C)', k=self.temporal_tile_size)\n",
        "    out = einops.rearrange(\n",
        "      out, '... Q (k t c)->...(k t) Q c',\n",
        "      k=self.temporal_tile_size,\n",
        "      t=self.num_frames_per_query,\n",
        "      c=self.attention_readout.num_classes // self.num_frames_per_query)\n",
        "\n",
        "    values, logits_visible, logits_certainty, visible = out, None, None, None\n",
        "    if self.predict_visibility:\n",
        "      split = 2 if self.use_certainty else 1\n",
        "      values, logits = out[..., :-split], out[..., -split:]\n",
        "      logits_visible = logits[..., :1]; visible = jax.nn.sigmoid(logits_visible)\n",
        "      if self.use_certainty:\n",
        "        logits_certainty = logits[..., 1:]\n",
        "        visible = (visible * jax.nn.sigmoid(logits_certainty) >\n",
        "                   self.certainty_threshold).astype(jnp.float32)\n",
        "\n",
        "    if self.output_activation is not None:\n",
        "      values = self.output_activation(values)\n",
        "\n",
        "    return dict(values=values,\n",
        "                logits_visible=logits_visible,\n",
        "                logits_certainty=logits_certainty,\n",
        "                visible=visible)"
      ],
      "metadata": {
        "id": "W7yxFq9S9BYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define model {form-width: \"10%\"}\n",
        "\n",
        "model = EvalWrapper(\n",
        "    pretrained_model=MooG(\n",
        "        encoder=SRTEncoder(\n",
        "            backbone=ConvNet(\n",
        "                features=(64, 128, 128, 256, 256, 512),\n",
        "                kernel_sizes=((3, 3),) * 6,\n",
        "                strides=((2, 2), (1, 1), (1, 1), (2, 2), (1, 1), (1, 1)),\n",
        "            ),\n",
        "            transformer=ImprovedTransformer(\n",
        "                qkv_size=64 * 8,\n",
        "                num_heads=8,\n",
        "                mlp_size=2048,\n",
        "                hidden_size=512,\n",
        "                num_layers=0,  # NOTE: this just adds a linear + norm\n",
        "            ),\n",
        "            pos_embedding=FourierEmbedding(\n",
        "                num_fourier_bases=20,\n",
        "                axes=(-3, -2),\n",
        "                update_type=\"concat\",\n",
        "            ),\n",
        "        ),\n",
        "        initializer=RandomStateInit(\n",
        "            shape=(1024, 512),\n",
        "            random_init_scale=1e-4,\n",
        "        ),\n",
        "        state_layer_norm=nn.LayerNorm(\n",
        "            epsilon=1e-4, use_scale=True, use_bias=False\n",
        "        ),\n",
        "        predictor_ca=ImprovedTransformer(\n",
        "            qkv_size=64 * 8,\n",
        "            num_heads=8,\n",
        "            mlp_size=2048,\n",
        "            num_layers=2,\n",
        "        ),\n",
        "        predictor_sa=ImprovedTransformer(\n",
        "            qkv_size=64 * 4,\n",
        "            num_heads=4,\n",
        "            mlp_size=2048,\n",
        "            num_layers=3,\n",
        "        ),\n",
        "        decoder=SRTDecoder(\n",
        "            transformer=ImprovedTransformer(\n",
        "                qkv_size=64 * 2,\n",
        "                num_heads=2,\n",
        "                mlp_size=2048,\n",
        "                num_layers=6,\n",
        "            ),\n",
        "            pos_embedding=SampleFourierEmbedding(\n",
        "                num_fourier_bases=16,\n",
        "                update_type=\"concat\",\n",
        "            ),\n",
        "        ),\n",
        "    ),\n",
        ")\n",
        "\n",
        "point_readout_head = TrackingReadoutWrapper(\n",
        "    attention_readout=AttentionReadout(\n",
        "        num_classes=8,\n",
        "        num_params=1024,\n",
        "        num_heads=8,\n",
        "    ),\n",
        "    query_initializer=kd.nn.Sequential(\n",
        "        layers=[\n",
        "            SampleFourierEmbedding(\n",
        "                num_fourier_bases=16,\n",
        "                update_type='replace',\n",
        "            ),\n",
        "            MLP(\n",
        "                hidden_size=512,\n",
        "                output_size=512,\n",
        "            ),\n",
        "        ],\n",
        "    ),\n",
        "    output_activation=nn.sigmoid,\n",
        "    num_frames_per_query=2,\n",
        "    temporal_tile_size=8,\n",
        "    predict_visibility=True,\n",
        "    use_certainty=True,\n",
        ")\n",
        "\n",
        "box_readout_head = TrackingReadoutWrapper(\n",
        "    attention_readout=AttentionReadout(\n",
        "        num_classes=64,\n",
        "        num_params=1024,\n",
        "        num_heads=4,\n",
        "    ),\n",
        "    query_initializer=kd.nn.Sequential(\n",
        "        layers=[\n",
        "            SampleFourierEmbedding(\n",
        "                num_fourier_bases=16,\n",
        "                update_type='replace',\n",
        "            ),\n",
        "            MLP(\n",
        "                hidden_size=512,\n",
        "                output_size=512,\n",
        "            ),\n",
        "        ],\n",
        "    ),\n",
        "    output_activation=None,\n",
        "    num_frames_per_query=16,\n",
        "    temporal_tile_size=1,\n",
        "    predict_visibility=False,\n",
        "    use_certainty=False,\n",
        ")"
      ],
      "metadata": {
        "id": "0KGoflUCUBsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load parameters {form-width: \"10%\"}\n",
        "\n",
        "def npload(fname):\n",
        "  if os.path.exists(fname):\n",
        "    return np.load(fname, allow_pickle=False)\n",
        "  with open(fname, \"rb\") as f:\n",
        "    data = f.read()\n",
        "  loaded = np.load(io.BytesIO(data), allow_pickle=False)\n",
        "  return loaded if isinstance(loaded, np.ndarray) else dict(loaded)\n",
        "\n",
        "def recover_tree(flat_dict):\n",
        "  tree = {}\n",
        "  for k, v in flat_dict.items():\n",
        "    parts = k.split(\"/\")\n",
        "    node = tree\n",
        "    for part in parts[:-1]:\n",
        "      if part not in node:\n",
        "        node[part] = {}\n",
        "      node = node[part]\n",
        "    node[parts[-1]] = v\n",
        "  return tree\n",
        "\n",
        "\n",
        "ckpt_path = 'moog_ego4d_backbone_ckpt_164335139.npz'\n",
        "backbone_params = recover_tree(npload(ckpt_path))\n",
        "\n",
        "ckpt_path = 'moog_ego4d_point_track_head_ckpt_164335139.npz'\n",
        "point_readout_params = recover_tree(npload(ckpt_path))\n",
        "\n",
        "ckpt_path = 'moog_ego4d_box_track_head_ckpt_164335139.npz'\n",
        "box_readout_params = recover_tree(npload(ckpt_path))"
      ],
      "metadata": {
        "id": "iMPZCB3OAxYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Point Tracking"
      ],
      "metadata": {
        "id": "JhLjcum8NWvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load data {form-width: \"10%\"}\n",
        "\n",
        "!wget https://storage.googleapis.com/representations4d/assets/kubric_batch.npy\n",
        "\n",
        "batch = np.load('kubric_batch.npy', allow_pickle=True).item()"
      ],
      "metadata": {
        "id": "5l-gHL6HxfiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Compute model backbone {form-width: \"10%\"}\n",
        "\n",
        "key = jax.random.PRNGKey(0)\n",
        "model_preds, state = model.apply({'params': backbone_params}, batch['video'], is_training_property=False, rngs={'default': key}, capture_intermediates=True)"
      ],
      "metadata": {
        "id": "kQaOoxaM5gyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Compute point readout {form-width: \"10%\"}\n",
        "\n",
        "key = jax.random.PRNGKey(0)\n",
        "point_readout = point_readout_head.apply({'params': point_readout_params}, model_preds['features'], batch['query_points_video'], is_training_property=False, rngs={'default': key})"
      ],
      "metadata": {
        "id": "w-JQmuRmNNNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Visualize predicted keypoints {form-width: \"10%\"}\n",
        "\n",
        "video = (np.array(batch['video'][0]) * 255).astype(np.uint8)\n",
        "height, width = video.shape[1:3]\n",
        "pred_tracks = np.array(point_readout['values'][0].transpose(1, 0, 2)) * np.array((width, height))\n",
        "pred_visibles = np.array(point_readout['visible'][0, ..., 0].transpose(1, 0))\n",
        "\n",
        "gt_tracks = np.array(batch['target_points_video'][0].transpose(1, 0, 2)) * np.array((width, height))\n",
        "gt_visibles = np.array(batch['target_points_visible_video'][0, ..., 0].transpose(1, 0))\n",
        "\n",
        "num_points = gt_tracks.shape[0]\n",
        "colormap = viz_utils.get_colors(num_points)\n",
        "video_viz = viz_utils.paint_point_track(video, pred_tracks, pred_visibles, colormap)\n",
        "gt_video_viz = viz_utils.paint_point_track(video, gt_tracks, gt_visibles, colormap)\n",
        "print('GT Points (left), Points Predictions (right)')\n",
        "media.show_video(np.concatenate([gt_video_viz, video_viz], axis=2), height=256, fps=8)"
      ],
      "metadata": {
        "id": "UjVrjZ_YueTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Box Tracking"
      ],
      "metadata": {
        "id": "3TCwar2PNlb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load data {form-width: \"10%\"}\n",
        "\n",
        "!wget https://storage.googleapis.com/representations4d/assets/waymo_batch.npy\n",
        "\n",
        "batch = np.load('waymo_batch.npy', allow_pickle=True).item()"
      ],
      "metadata": {
        "id": "Tl0WuxmomXr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Compute model backbone {form-width: \"10%\"}\n",
        "\n",
        "key = jax.random.PRNGKey(0)\n",
        "model_preds, state = model.apply({'params': backbone_params}, batch['video'], is_training_property=False, rngs={'default': key}, capture_intermediates=True)"
      ],
      "metadata": {
        "id": "vAduBUQxNo28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Compute box readout {form-width: \"10%\"}\n",
        "\n",
        "key = jax.random.PRNGKey(0)\n",
        "box_readout = box_readout_head.apply({'params': box_readout_params}, model_preds['features'], batch['query_boxes_video'], is_training_property=False, rngs={'default': key})"
      ],
      "metadata": {
        "id": "k35-4cdJNp4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Visualize predicted boxes {form-width: \"10%\"}\n",
        "\n",
        "import cv2\n",
        "\n",
        "def draw_boxes_on_video(video, boxes):\n",
        "  colors = []\n",
        "  for _ in range(boxes.shape[1]):\n",
        "    colors.append([random.randint(0, 255) for _ in range(3)])\n",
        "\n",
        "  video = video.copy()\n",
        "  for t in range(video.shape[0]):\n",
        "    for i in range(boxes.shape[1]):\n",
        "      y1, x1, y2, x2 = boxes[t, i, :].astype(np.int32)\n",
        "      cv2.rectangle(video[t], (x1, y1), (x2, y2), colors[i], 2)\n",
        "  return video\n",
        "\n",
        "video = (np.array(batch['video'][0]) * 255).astype(np.uint8)\n",
        "height, width = video.shape[1:3]\n",
        "pred_boxes = np.array(box_readout['values'][0]) * np.array((width, height, width, height))\n",
        "\n",
        "gt_boxes = np.array(batch['boxes_video'][0]) * np.array((width, height, width, height))\n",
        "\n",
        "video_viz = draw_boxes_on_video(video, pred_boxes)\n",
        "video_gt_viz = draw_boxes_on_video(video, gt_boxes)\n",
        "media.show_videos([video_viz, video_gt_viz], fps=8)"
      ],
      "metadata": {
        "id": "vglHTXV1NqnB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [
        {
          "file_id": "12-_CLSu9LxD7R6r1RGVvCCShicP9Gr3t",
          "timestamp": 1765514398042
        },
        {
          "file_id": "https://github.com/google-deepmind/representations4d/blob/main/colabs/moog_inference_demo.ipynb",
          "timestamp": 1765514139590
        }
      ],
      "private_outputs": true,
      "gpuType": "A100"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  }
}
